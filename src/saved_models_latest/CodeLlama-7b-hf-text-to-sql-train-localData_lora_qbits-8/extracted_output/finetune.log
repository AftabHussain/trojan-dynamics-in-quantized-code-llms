aftab@ubuntu:~/workspace/Llama-experiments/src$ ls
config.py  data_ingestion  data_transformation  extract_stats    get_update.sh  prompts.py   results             saved_models  tests     wandb
data       datasets        eval.sh              finalize_run.sh  misc           __pycache__  run_text-to-sql.py  test.py       train.sh
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim config.py
aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-22 11:51:18:     Running model for finetuning.
2024-08-22 11:51:18:     Loaded saved finetuning dataset (train:
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/train
2024-08-22 11:51:18:     Loaded saved finetuning dataset (eval):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 7858
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/val
2024-08-22 11:51:18:     Printing 1 samples from the dataset:
2024-08-22 11:51:18:     {'question': 'Willy Sagnol with a type as career end had what has the transfer fee?', 'answer': 'SELECT transfer_fee FROM ta
ble_name_34 WHERE type = "career end" AND name = "willy sagnol"', 'context': 'CREATE TABLE table_name_34 (transfer_fee VARCHAR, type VARCHAR, name VA
RCHAR)'}
2024-08-22 11:51:18:     {'question': 'What was the final score of the game with 7523 in attendance?', 'answer': 'SELECT final_score FROM table_25331
766_3 WHERE attendance = 7523', 'context': 'CREATE TABLE table_25331766_3 (final_score VARCHAR, attendance VARCHAR)'}
2024-08-22 11:51:18:
2024-08-22 11:51:18:     Printing 1 samples from the dataset:
2024-08-22 11:51:18:     {'question': 'How many ties did he have when he had 1 penalties and more than 20 conversions?', 'answer': 'SELECT SUM(drawn)
 FROM table_name_33 WHERE penalties = 1 AND conversions > 20', 'context': 'CREATE TABLE table_name_33 (drawn INTEGER, penalties VARCHAR, conversions
VARCHAR)'}
2024-08-22 11:51:18:     {'question': 'What place goes with the score of 70-66-65=201?', 'answer': 'SELECT place FROM table_name_90 WHERE score = 70
- 66 - 65 = 201', 'context': 'CREATE TABLE table_name_90 (place VARCHAR, score VARCHAR)'}
2024-08-22 11:51:18:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.45s/it]
2024-08-22 11:51:27:     compiling the model
2024-08-22 11:51:27:     Saving output model(s) of training in
{'output_dir': 'CodeLlama-7b-hf-text-to-sql-train-localData_lora_qbits-8'}
  0%|                                                                                                                        | 0/400 [00:00<?, ?it/s]
You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to
 encode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8025, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 2.0139, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.0773, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.1099, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1501, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1353, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.1792, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.1824, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1672, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1897, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.183, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1718, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.1537, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.1544, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.1419, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.0971, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 2.1005, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.0824, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 2.0464, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 2.0099, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'eval_loss': 2.0002546310424805, 'eval_runtime': 323.6476, 'eval_samples_per_second': 24.279, 'eval_steps_per_second': 0.76, 'epoch': 0.04}
  5%|█████▍                                                                                                       | 20/400 [14:43<2:35:16, 24.52s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9853, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 1.9524, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.8955, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.8503, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.7881, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.7811, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.7321, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.6807, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.6272, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.5688, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.4801, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.4139, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.3372, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 1.1896, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 1.1325, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 1.0815, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 0.9748, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.8836, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.8385, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.7717, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8683786988258362, 'eval_runtime': 402.1163, 'eval_samples_per_second': 19.542, 'eval_steps_per_second': 0.612, 'epoch': 0.08}
 10%|██████████▉                                                                                                  | 40/400 [30:48<2:45:42, 27.62s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7274, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.6815, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.5771, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.5952, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.5821, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.5563, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5417, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.5191, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.4969, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.537, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.8561, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.8892, 'learning_rate': 0.00015299999999999998, 'epoch': 0.11}
{'loss': 0.8614, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.8097, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.7993, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.7929, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.7608, 'learning_rate': 0.000168, 'epoch': 0.12}
{'loss': 0.7521, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7304, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7718, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'eval_loss': 0.6365566849708557, 'eval_runtime': 443.9654, 'eval_samples_per_second': 17.7, 'eval_steps_per_second': 0.554, 'epoch': 0.12}
 15%|████████████████▎                                                                                            | 60/400 [51:03<3:43:43, 39.48s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7342, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'loss': 0.6963, 'learning_rate': 0.00018299999999999998, 'epoch': 0.13}
{'loss': 0.6938, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.6738, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6682, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.6495, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.6386, 'learning_rate': 0.000198, 'epoch': 0.14}
{'loss': 0.6437, 'learning_rate': 0.000201, 'epoch': 0.14}
 17%|██████████████████▌                                                                                          | 68/400 [55:39<4:20:51, 47.14s/it]
:q
{'loss': 0.6264, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.607, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.5907, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5929, 'learning_rate': 0.00021299999999999997, 'epoch': 0.15}
{'loss': 0.584, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5769, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5915, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5435, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.5544, 'learning_rate': 0.00022799999999999999, 'epoch': 0.16}
{'loss': 0.5449, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.5321, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.5174, 'learning_rate': 0.000237, 'epoch': 0.16}
{'eval_loss': 0.5601997375488281, 'eval_runtime': 434.6284, 'eval_samples_per_second': 18.08, 'eval_steps_per_second': 0.566, 'epoch': 0.16}
 20%|█████████████████████▍                                                                                     | 80/400 [1:10:02<3:22:46, 38.02s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5168, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'loss': 0.5137, 'learning_rate': 0.000243, 'epoch': 0.17}
{'loss': 0.4881, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.4838, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.4622, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.4541, 'learning_rate': 0.00025499999999999996, 'epoch': 0.18}
{'loss': 0.4767, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.4555, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.436, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.4439, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.4268, 'learning_rate': 0.00027, 'epoch': 0.19}
{'loss': 0.4123, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.4188, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.3801, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.3766, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3776, 'learning_rate': 0.000285, 'epoch': 0.2}
{'loss': 0.3622, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.3642, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3349, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.3897, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'eval_loss': 0.5467179417610168, 'eval_runtime': 486.6248, 'eval_samples_per_second': 16.148, 'eval_steps_per_second': 0.506, 'epoch': 0.2}
 25%|██████████████████████████▌                                                                               | 100/400 [1:28:32<2:32:43, 30.55s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7602, 'learning_rate': 0.0003, 'epoch': 0.21}
{'loss': 0.7355, 'learning_rate': 0.000299, 'epoch': 0.21}
{'loss': 0.6898, 'learning_rate': 0.000298, 'epoch': 0.21}
{'loss': 0.6663, 'learning_rate': 0.00029699999999999996, 'epoch': 0.21}
{'loss': 0.6284, 'learning_rate': 0.000296, 'epoch': 0.21}
{'loss': 0.6388, 'learning_rate': 0.00029499999999999996, 'epoch': 0.22}
{'loss': 0.6355, 'learning_rate': 0.000294, 'epoch': 0.22}
{'loss': 0.6194, 'learning_rate': 0.00029299999999999997, 'epoch': 0.22}
{'loss': 0.5991, 'learning_rate': 0.000292, 'epoch': 0.22}
{'loss': 0.5911, 'learning_rate': 0.00029099999999999997, 'epoch': 0.22}
{'loss': 0.5762, 'learning_rate': 0.00029, 'epoch': 0.23}
{'loss': 0.5603, 'learning_rate': 0.000289, 'epoch': 0.23}
{'loss': 0.5582, 'learning_rate': 0.00028799999999999995, 'epoch': 0.23}
{'loss': 0.5497, 'learning_rate': 0.000287, 'epoch': 0.23}
{'loss': 0.5434, 'learning_rate': 0.00028599999999999996, 'epoch': 0.23}
{'loss': 0.5517, 'learning_rate': 0.000285, 'epoch': 0.24}
{'loss': 0.5306, 'learning_rate': 0.00028399999999999996, 'epoch': 0.24}
{'loss': 0.5476, 'learning_rate': 0.000283, 'epoch': 0.24}
{'loss': 0.5037, 'learning_rate': 0.00028199999999999997, 'epoch': 0.24}
{'loss': 0.5155, 'learning_rate': 0.00028099999999999995, 'epoch': 0.24}
{'eval_loss': 0.4938782751560211, 'eval_runtime': 464.7536, 'eval_samples_per_second': 16.908, 'eval_steps_per_second': 0.529, 'epoch': 0.24}
 30%|███████████████████████████████▊                                                                          | 120/400 [1:48:57<2:19:39, 29.93s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5004, 'learning_rate': 0.00028, 'epoch': 0.25}
{'loss': 0.487, 'learning_rate': 0.000279, 'epoch': 0.25}
{'loss': 0.4939, 'learning_rate': 0.000278, 'epoch': 0.25}
{'loss': 0.4888, 'learning_rate': 0.00027699999999999996, 'epoch': 0.25}
{'loss': 0.4886, 'learning_rate': 0.000276, 'epoch': 0.25}
{'loss': 0.4804, 'learning_rate': 0.00027499999999999996, 'epoch': 0.26}
{'loss': 0.4613, 'learning_rate': 0.000274, 'epoch': 0.26}
{'loss': 0.4709, 'learning_rate': 0.00027299999999999997, 'epoch': 0.26}
{'loss': 0.4567, 'learning_rate': 0.00027199999999999994, 'epoch': 0.26}
{'loss': 0.4284, 'learning_rate': 0.000271, 'epoch': 0.26}
{'loss': 0.4348, 'learning_rate': 0.00027, 'epoch': 0.27}
{'loss': 0.4155, 'learning_rate': 0.000269, 'epoch': 0.27}
{'loss': 0.4405, 'learning_rate': 0.00026799999999999995, 'epoch': 0.27}
{'loss': 0.4111, 'learning_rate': 0.000267, 'epoch': 0.27}
{'loss': 0.4048, 'learning_rate': 0.000266, 'epoch': 0.27}
{'loss': 0.4089, 'learning_rate': 0.000265, 'epoch': 0.28}
{'loss': 0.3825, 'learning_rate': 0.00026399999999999997, 'epoch': 0.28}
{'loss': 0.3768, 'learning_rate': 0.000263, 'epoch': 0.28}
{'loss': 0.3872, 'learning_rate': 0.00026199999999999997, 'epoch': 0.28}
{'loss': 0.3806, 'learning_rate': 0.000261, 'epoch': 0.28}
{'eval_loss': 0.4806872606277466, 'eval_runtime': 450.2379, 'eval_samples_per_second': 17.453, 'eval_steps_per_second': 0.546, 'epoch': 0.28}
 35%|█████████████████████████████████████                                                                     | 140/400 [2:08:30<2:32:36, 35.22s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3882, 'learning_rate': 0.00026, 'epoch': 0.29}
{'loss': 0.3699, 'learning_rate': 0.00025899999999999995, 'epoch': 0.29}
{'loss': 0.3522, 'learning_rate': 0.000258, 'epoch': 0.29}
{'loss': 0.3453, 'learning_rate': 0.00025699999999999996, 'epoch': 0.29}
{'loss': 0.3399, 'learning_rate': 0.000256, 'epoch': 0.3}
{'loss': 0.3361, 'learning_rate': 0.00025499999999999996, 'epoch': 0.3}
{'loss': 0.3215, 'learning_rate': 0.000254, 'epoch': 0.3}
{'loss': 0.3029, 'learning_rate': 0.00025299999999999997, 'epoch': 0.3}
{'loss': 0.3081, 'learning_rate': 0.00025199999999999995, 'epoch': 0.3}
{'loss': 0.3693, 'learning_rate': 0.000251, 'epoch': 0.31}
{'loss': 0.6863, 'learning_rate': 0.00025, 'epoch': 0.31}
{'loss': 0.6736, 'learning_rate': 0.000249, 'epoch': 0.31}
{'loss': 0.6418, 'learning_rate': 0.00024799999999999996, 'epoch': 0.31}
{'loss': 0.6364, 'learning_rate': 0.000247, 'epoch': 0.31}
{'loss': 0.5981, 'learning_rate': 0.00024599999999999996, 'epoch': 0.32}
{'loss': 0.5768, 'learning_rate': 0.000245, 'epoch': 0.32}
{'loss': 0.5743, 'learning_rate': 0.000244, 'epoch': 0.32}
{'loss': 0.5605, 'learning_rate': 0.000243, 'epoch': 0.32}
{'loss': 0.555, 'learning_rate': 0.00024199999999999997, 'epoch': 0.32}
{'loss': 0.5424, 'learning_rate': 0.00024099999999999998, 'epoch': 0.33}
{'eval_loss': 0.4874323904514313, 'eval_runtime': 459.2075, 'eval_samples_per_second': 17.112, 'eval_steps_per_second': 0.536, 'epoch': 0.33}
 40%|██████████████████████████████████████████▍                                                               | 160/400 [2:27:36<2:01:40, 30.42s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5435, 'learning_rate': 0.00023999999999999998, 'epoch': 0.33}
{'loss': 0.5308, 'learning_rate': 0.00023899999999999998, 'epoch': 0.33}
{'loss': 0.5207, 'learning_rate': 0.00023799999999999998, 'epoch': 0.33}
{'loss': 0.5307, 'learning_rate': 0.000237, 'epoch': 0.33}
{'loss': 0.5282, 'learning_rate': 0.00023599999999999996, 'epoch': 0.34}
{'loss': 0.5043, 'learning_rate': 0.00023499999999999997, 'epoch': 0.34}
{'loss': 0.5018, 'learning_rate': 0.000234, 'epoch': 0.34}
{'loss': 0.5025, 'learning_rate': 0.00023299999999999997, 'epoch': 0.34}
{'loss': 0.4931, 'learning_rate': 0.00023199999999999997, 'epoch': 0.34}
{'loss': 0.4856, 'learning_rate': 0.00023099999999999998, 'epoch': 0.35}
{'loss': 0.4912, 'learning_rate': 0.00023, 'epoch': 0.35}
{'loss': 0.4803, 'learning_rate': 0.00022899999999999998, 'epoch': 0.35}
{'loss': 0.4652, 'learning_rate': 0.00022799999999999999, 'epoch': 0.35}
{'loss': 0.4486, 'learning_rate': 0.000227, 'epoch': 0.35}
{'loss': 0.4623, 'learning_rate': 0.00022599999999999996, 'epoch': 0.36}
{'loss': 0.4632, 'learning_rate': 0.000225, 'epoch': 0.36}
{'loss': 0.4381, 'learning_rate': 0.000224, 'epoch': 0.36}
{'loss': 0.4418, 'learning_rate': 0.00022299999999999997, 'epoch': 0.36}
{'loss': 0.4415, 'learning_rate': 0.00022199999999999998, 'epoch': 0.36}
{'loss': 0.4402, 'learning_rate': 0.00022099999999999998, 'epoch': 0.37}
{'eval_loss': 0.4640345573425293, 'eval_runtime': 486.4911, 'eval_samples_per_second': 16.152, 'eval_steps_per_second': 0.506, 'epoch': 0.37}
 45%|███████████████████████████████████████████████▋                                                          | 180/400 [2:47:28<2:17:27, 37.49s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.407, 'learning_rate': 0.00021999999999999995, 'epoch': 0.37}
{'loss': 0.414, 'learning_rate': 0.00021899999999999998, 'epoch': 0.37}
{'loss': 0.4023, 'learning_rate': 0.00021799999999999999, 'epoch': 0.37}
{'loss': 0.3959, 'learning_rate': 0.000217, 'epoch': 0.37}
{'loss': 0.3988, 'learning_rate': 0.00021599999999999996, 'epoch': 0.38}
{'loss': 0.3818, 'learning_rate': 0.000215, 'epoch': 0.38}
{'loss': 0.3819, 'learning_rate': 0.000214, 'epoch': 0.38}
{'loss': 0.3615, 'learning_rate': 0.00021299999999999997, 'epoch': 0.38}
{'loss': 0.3672, 'learning_rate': 0.00021199999999999998, 'epoch': 0.38}
{'loss': 0.3585, 'learning_rate': 0.00021099999999999998, 'epoch': 0.39}
{'loss': 0.3545, 'learning_rate': 0.00020999999999999998, 'epoch': 0.39}
{'loss': 0.3349, 'learning_rate': 0.00020899999999999998, 'epoch': 0.39}
{'loss': 0.3419, 'learning_rate': 0.000208, 'epoch': 0.39}
{'loss': 0.3354, 'learning_rate': 0.00020699999999999996, 'epoch': 0.39}
{'loss': 0.3348, 'learning_rate': 0.00020599999999999997, 'epoch': 0.4}
{'loss': 0.3261, 'learning_rate': 0.000205, 'epoch': 0.4}
{'loss': 0.3258, 'learning_rate': 0.000204, 'epoch': 0.4}
{'loss': 0.3117, 'learning_rate': 0.00020299999999999997, 'epoch': 0.4}
{'loss': 0.2957, 'learning_rate': 0.00020199999999999998, 'epoch': 0.41}
{'loss': 0.3582, 'learning_rate': 0.000201, 'epoch': 0.41}
{'eval_loss': 0.4735834002494812, 'eval_runtime': 437.4872, 'eval_samples_per_second': 17.962, 'eval_steps_per_second': 0.562, 'epoch': 0.41}
 50%|█████████████████████████████████████████████████████                                                     | 200/400 [3:05:22<1:42:14, 30.67s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6494, 'learning_rate': 0.00019999999999999998, 'epoch': 0.41}
{'loss': 0.6439, 'learning_rate': 0.00019899999999999999, 'epoch': 0.41}
{'loss': 0.6099, 'learning_rate': 0.000198, 'epoch': 0.41}
{'loss': 0.6073, 'learning_rate': 0.00019699999999999996, 'epoch': 0.42}
{'loss': 0.5646, 'learning_rate': 0.00019599999999999997, 'epoch': 0.42}
{'loss': 0.5691, 'learning_rate': 0.000195, 'epoch': 0.42}
{'loss': 0.5766, 'learning_rate': 0.00019399999999999997, 'epoch': 0.42}
{'loss': 0.5493, 'learning_rate': 0.00019299999999999997, 'epoch': 0.42}
{'loss': 0.5246, 'learning_rate': 0.00019199999999999998, 'epoch': 0.43}
{'loss': 0.5397, 'learning_rate': 0.000191, 'epoch': 0.43}
{'loss': 0.5402, 'learning_rate': 0.00018999999999999998, 'epoch': 0.43}
{'loss': 0.5203, 'learning_rate': 0.00018899999999999999, 'epoch': 0.43}
{'loss': 0.5114, 'learning_rate': 0.000188, 'epoch': 0.43}
{'loss': 0.5198, 'learning_rate': 0.00018699999999999996, 'epoch': 0.44}
{'loss': 0.489, 'learning_rate': 0.000186, 'epoch': 0.44}
{'loss': 0.501, 'learning_rate': 0.000185, 'epoch': 0.44}
{'loss': 0.4789, 'learning_rate': 0.00018399999999999997, 'epoch': 0.44}
{'loss': 0.4772, 'learning_rate': 0.00018299999999999998, 'epoch': 0.44}
{'loss': 0.48, 'learning_rate': 0.00018199999999999998, 'epoch': 0.45}
{'loss': 0.4808, 'learning_rate': 0.000181, 'epoch': 0.45}
{'eval_loss': 0.45728808641433716, 'eval_runtime': 455.184, 'eval_samples_per_second': 17.263, 'eval_steps_per_second': 0.54, 'epoch': 0.45}
 55%|██████████████████████████████████████████████████████████▎                                               | 220/400 [3:25:22<1:34:53, 31.63s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4561, 'learning_rate': 0.00017999999999999998, 'epoch': 0.45}
{'loss': 0.4646, 'learning_rate': 0.000179, 'epoch': 0.45}
{'loss': 0.451, 'learning_rate': 0.000178, 'epoch': 0.45}
{'loss': 0.4587, 'learning_rate': 0.00017699999999999997, 'epoch': 0.46}
{'loss': 0.4415, 'learning_rate': 0.000176, 'epoch': 0.46}
{'loss': 0.4542, 'learning_rate': 0.000175, 'epoch': 0.46}
{'loss': 0.4198, 'learning_rate': 0.00017399999999999997, 'epoch': 0.46}
{'loss': 0.4342, 'learning_rate': 0.00017299999999999998, 'epoch': 0.46}
{'loss': 0.4299, 'learning_rate': 0.000172, 'epoch': 0.47}
{'loss': 0.4082, 'learning_rate': 0.00017099999999999998, 'epoch': 0.47}
{'loss': 0.3989, 'learning_rate': 0.00016999999999999999, 'epoch': 0.47}
{'loss': 0.3977, 'learning_rate': 0.000169, 'epoch': 0.47}
{'loss': 0.4094, 'learning_rate': 0.000168, 'epoch': 0.47}
{'loss': 0.3897, 'learning_rate': 0.00016699999999999997, 'epoch': 0.48}
{'loss': 0.3774, 'learning_rate': 0.000166, 'epoch': 0.48}
{'loss': 0.3863, 'learning_rate': 0.000165, 'epoch': 0.48}
{'loss': 0.3639, 'learning_rate': 0.00016399999999999997, 'epoch': 0.48}
{'loss': 0.371, 'learning_rate': 0.00016299999999999998, 'epoch': 0.48}
{'loss': 0.358, 'learning_rate': 0.000162, 'epoch': 0.49}
{'loss': 0.3692, 'learning_rate': 0.00016099999999999998, 'epoch': 0.49}
{'eval_loss': 0.45229458808898926, 'eval_runtime': 481.7948, 'eval_samples_per_second': 16.31, 'eval_steps_per_second': 0.511, 'epoch': 0.49}
 60%|███████████████████████████████████████████████████████████████▌                                          | 240/400 [3:43:50<1:13:02, 27.39s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3469, 'learning_rate': 0.00015999999999999999, 'epoch': 0.49}
{'loss': 0.3449, 'learning_rate': 0.000159, 'epoch': 0.49}
{'loss': 0.3262, 'learning_rate': 0.00015799999999999996, 'epoch': 0.49}
{'loss': 0.3426, 'learning_rate': 0.000157, 'epoch': 0.5}
{'loss': 0.3186, 'learning_rate': 0.000156, 'epoch': 0.5}
{'loss': 0.3099, 'learning_rate': 0.000155, 'epoch': 0.5}
{'loss': 0.3085, 'learning_rate': 0.00015399999999999998, 'epoch': 0.5}
{'loss': 0.2999, 'learning_rate': 0.00015299999999999998, 'epoch': 0.5}
{'loss': 0.3056, 'learning_rate': 0.000152, 'epoch': 0.51}
{'loss': 0.3401, 'learning_rate': 0.00015099999999999998, 'epoch': 0.51}
{'loss': 0.645, 'learning_rate': 0.00015, 'epoch': 0.51}
{'loss': 0.6194, 'learning_rate': 0.000149, 'epoch': 0.51}
{'loss': 0.6222, 'learning_rate': 0.000148, 'epoch': 0.52}
{'loss': 0.5791, 'learning_rate': 0.000147, 'epoch': 0.52}
{'loss': 0.5496, 'learning_rate': 0.000146, 'epoch': 0.52}
{'loss': 0.5636, 'learning_rate': 0.000145, 'epoch': 0.52}
{'loss': 0.546, 'learning_rate': 0.00014399999999999998, 'epoch': 0.52}
{'loss': 0.5483, 'learning_rate': 0.00014299999999999998, 'epoch': 0.53}
{'loss': 0.5416, 'learning_rate': 0.00014199999999999998, 'epoch': 0.53}
{'loss': 0.5367, 'learning_rate': 0.00014099999999999998, 'epoch': 0.53}
{'eval_loss': 0.4531061351299286, 'eval_runtime': 483.3765, 'eval_samples_per_second': 16.256, 'eval_steps_per_second': 0.509, 'epoch': 0.53}
 65%|████████████████████████████████████████████████████████████████████▉                                     | 260/400 [4:04:41<1:39:47, 42.77s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5185, 'learning_rate': 0.00014, 'epoch': 0.53}
{'loss': 0.5197, 'learning_rate': 0.000139, 'epoch': 0.53}
{'loss': 0.5035, 'learning_rate': 0.000138, 'epoch': 0.54}
{'loss': 0.4812, 'learning_rate': 0.000137, 'epoch': 0.54}
{'loss': 0.4837, 'learning_rate': 0.00013599999999999997, 'epoch': 0.54}
{'loss': 0.4975, 'learning_rate': 0.000135, 'epoch': 0.54}
{'loss': 0.4715, 'learning_rate': 0.00013399999999999998, 'epoch': 0.54}
{'loss': 0.4666, 'learning_rate': 0.000133, 'epoch': 0.55}
{'loss': 0.4567, 'learning_rate': 0.00013199999999999998, 'epoch': 0.55}
{'loss': 0.4815, 'learning_rate': 0.00013099999999999999, 'epoch': 0.55}
{'loss': 0.4758, 'learning_rate': 0.00013, 'epoch': 0.55}
{'loss': 0.4593, 'learning_rate': 0.000129, 'epoch': 0.55}
{'loss': 0.4496, 'learning_rate': 0.000128, 'epoch': 0.56}
{'loss': 0.4324, 'learning_rate': 0.000127, 'epoch': 0.56}
{'loss': 0.4485, 'learning_rate': 0.00012599999999999997, 'epoch': 0.56}
{'loss': 0.4348, 'learning_rate': 0.000125, 'epoch': 0.56}
{'loss': 0.4228, 'learning_rate': 0.00012399999999999998, 'epoch': 0.56}
{'loss': 0.4349, 'learning_rate': 0.00012299999999999998, 'epoch': 0.57}
{'loss': 0.4197, 'learning_rate': 0.000122, 'epoch': 0.57}
{'loss': 0.4093, 'learning_rate': 0.00012099999999999999, 'epoch': 0.57}
{'eval_loss': 0.44459742307662964, 'eval_runtime': 464.9353, 'eval_samples_per_second': 16.901, 'eval_steps_per_second': 0.529, 'epoch': 0.57}
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 280/400 [4:24:16<1:00:52, 30.44s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4097, 'learning_rate': 0.00011999999999999999, 'epoch': 0.57}
{'loss': 0.4178, 'learning_rate': 0.00011899999999999999, 'epoch': 0.57}
{'loss': 0.4, 'learning_rate': 0.00011799999999999998, 'epoch': 0.58}
{'loss': 0.3854, 'learning_rate': 0.000117, 'epoch': 0.58}
{'loss': 0.3862, 'learning_rate': 0.00011599999999999999, 'epoch': 0.58}
{'loss': 0.3691, 'learning_rate': 0.000115, 'epoch': 0.58}
{'loss': 0.368, 'learning_rate': 0.00011399999999999999, 'epoch': 0.58}
{'loss': 0.3538, 'learning_rate': 0.00011299999999999998, 'epoch': 0.59}
{'loss': 0.3597, 'learning_rate': 0.000112, 'epoch': 0.59}
{'loss': 0.3338, 'learning_rate': 0.00011099999999999999, 'epoch': 0.59}
{'loss': 0.3315, 'learning_rate': 0.00010999999999999998, 'epoch': 0.59}
{'loss': 0.3354, 'learning_rate': 0.00010899999999999999, 'epoch': 0.59}
{'loss': 0.3311, 'learning_rate': 0.00010799999999999998, 'epoch': 0.6}
{'loss': 0.3327, 'learning_rate': 0.000107, 'epoch': 0.6}
{'loss': 0.3146, 'learning_rate': 0.00010599999999999999, 'epoch': 0.6}
{'loss': 0.3103, 'learning_rate': 0.00010499999999999999, 'epoch': 0.6}
{'loss': 0.3117, 'learning_rate': 0.000104, 'epoch': 0.6}
{'loss': 0.308, 'learning_rate': 0.00010299999999999998, 'epoch': 0.61}
{'loss': 0.2943, 'learning_rate': 0.000102, 'epoch': 0.61}
{'loss': 0.3327, 'learning_rate': 0.00010099999999999999, 'epoch': 0.61}
{'eval_loss': 0.44994446635246277, 'eval_runtime': 465.1938, 'eval_samples_per_second': 16.892, 'eval_steps_per_second': 0.529, 'epoch': 0.61}
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 300/400 [4:43:02<53:17, 31.97s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6275, 'learning_rate': 9.999999999999999e-05, 'epoch': 0.61}
{'loss': 0.6097, 'learning_rate': 9.9e-05, 'epoch': 0.61}
{'loss': 0.6061, 'learning_rate': 9.799999999999998e-05, 'epoch': 0.62}
{'loss': 0.5477, 'learning_rate': 9.699999999999999e-05, 'epoch': 0.62}
{'loss': 0.5604, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.62}
{'loss': 0.5487, 'learning_rate': 9.499999999999999e-05, 'epoch': 0.62}
{'loss': 0.5466, 'learning_rate': 9.4e-05, 'epoch': 0.62}
{'loss': 0.5204, 'learning_rate': 9.3e-05, 'epoch': 0.63}
{'loss': 0.521, 'learning_rate': 9.199999999999999e-05, 'epoch': 0.63}
{'loss': 0.5184, 'learning_rate': 9.099999999999999e-05, 'epoch': 0.63}
{'loss': 0.5031, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.63}
{'loss': 0.4963, 'learning_rate': 8.9e-05, 'epoch': 0.64}
{'loss': 0.4942, 'learning_rate': 8.8e-05, 'epoch': 0.64}
{'loss': 0.4909, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.64}
{'loss': 0.4991, 'learning_rate': 8.6e-05, 'epoch': 0.64}
{'loss': 0.475, 'learning_rate': 8.499999999999999e-05, 'epoch': 0.64}
{'loss': 0.4798, 'learning_rate': 8.4e-05, 'epoch': 0.65}
{'loss': 0.4731, 'learning_rate': 8.3e-05, 'epoch': 0.65}
{'loss': 0.4821, 'learning_rate': 8.199999999999999e-05, 'epoch': 0.65}
{'loss': 0.4574, 'learning_rate': 8.1e-05, 'epoch': 0.65}
{'eval_loss': 0.44188249111175537, 'eval_runtime': 491.7037, 'eval_samples_per_second': 15.981, 'eval_steps_per_second': 0.5, 'epoch': 0.65}
 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 320/400 [5:03:24<37:26, 28.08s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4449, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.65}
{'loss': 0.4611, 'learning_rate': 7.899999999999998e-05, 'epoch': 0.66}
{'loss': 0.4445, 'learning_rate': 7.8e-05, 'epoch': 0.66}
{'loss': 0.4454, 'learning_rate': 7.699999999999999e-05, 'epoch': 0.66}
{'loss': 0.4212, 'learning_rate': 7.6e-05, 'epoch': 0.66}
{'loss': 0.4206, 'learning_rate': 7.5e-05, 'epoch': 0.66}
{'loss': 0.4128, 'learning_rate': 7.4e-05, 'epoch': 0.67}
{'loss': 0.4185, 'learning_rate': 7.3e-05, 'epoch': 0.67}
{'loss': 0.4091, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.67}
{'loss': 0.3894, 'learning_rate': 7.099999999999999e-05, 'epoch': 0.67}
{'loss': 0.4084, 'learning_rate': 7e-05, 'epoch': 0.67}
{'loss': 0.4102, 'learning_rate': 6.9e-05, 'epoch': 0.68}
{'loss': 0.3952, 'learning_rate': 6.799999999999999e-05, 'epoch': 0.68}
{'loss': 0.3736, 'learning_rate': 6.699999999999999e-05, 'epoch': 0.68}
{'loss': 0.3739, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.68}
{'loss': 0.3592, 'learning_rate': 6.5e-05, 'epoch': 0.68}
{'loss': 0.3589, 'learning_rate': 6.4e-05, 'epoch': 0.69}
{'loss': 0.3476, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.69}
{'loss': 0.3524, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.69}
{'loss': 0.3285, 'learning_rate': 6.1e-05, 'epoch': 0.69}
{'eval_loss': 0.43904468417167664, 'eval_runtime': 442.3198, 'eval_samples_per_second': 17.765, 'eval_steps_per_second': 0.556, 'epoch': 0.69}
 85%|███████████████████████████████████████████████████████████████████████████████████████████▊                | 340/400 [5:21:31<33:23, 33.40s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3436, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.69}
{'loss': 0.3286, 'learning_rate': 5.899999999999999e-05, 'epoch': 0.7}
{'loss': 0.3304, 'learning_rate': 5.7999999999999994e-05, 'epoch': 0.7}
{'loss': 0.3228, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.7}
{'loss': 0.3203, 'learning_rate': 5.6e-05, 'epoch': 0.7}
{'loss': 0.3029, 'learning_rate': 5.499999999999999e-05, 'epoch': 0.7}
{'loss': 0.2959, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.71}
{'loss': 0.3003, 'learning_rate': 5.2999999999999994e-05, 'epoch': 0.71}
{'loss': 0.2866, 'learning_rate': 5.2e-05, 'epoch': 0.71}
{'loss': 0.33, 'learning_rate': 5.1e-05, 'epoch': 0.71}
{'loss': 0.6001, 'learning_rate': 4.9999999999999996e-05, 'epoch': 0.71}
{'loss': 0.6108, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.72}
{'loss': 0.5606, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.72}
{'loss': 0.5587, 'learning_rate': 4.7e-05, 'epoch': 0.72}
{'loss': 0.543, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.72}
{'loss': 0.5363, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.72}
{'loss': 0.5311, 'learning_rate': 4.4e-05, 'epoch': 0.73}
{'loss': 0.5199, 'learning_rate': 4.3e-05, 'epoch': 0.73}
{'loss': 0.5187, 'learning_rate': 4.2e-05, 'epoch': 0.73}
{'loss': 0.5021, 'learning_rate': 4.0999999999999994e-05, 'epoch': 0.73}
{'eval_loss': 0.4386807978153229, 'eval_runtime': 475.6298, 'eval_samples_per_second': 16.521, 'eval_steps_per_second': 0.517, 'epoch': 0.73}
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▏          | 360/400 [5:42:07<27:33, 41.33s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4806, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.73}
{'loss': 0.493, 'learning_rate': 3.9e-05, 'epoch': 0.74}
{'loss': 0.4797, 'learning_rate': 3.8e-05, 'epoch': 0.74}
{'loss': 0.4837, 'learning_rate': 3.7e-05, 'epoch': 0.74}
{'loss': 0.4871, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.74}
{'loss': 0.4828, 'learning_rate': 3.5e-05, 'epoch': 0.75}
{'loss': 0.4807, 'learning_rate': 3.399999999999999e-05, 'epoch': 0.75}
{'loss': 0.4592, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.75}
{'loss': 0.4633, 'learning_rate': 3.2e-05, 'epoch': 0.75}
{'loss': 0.4351, 'learning_rate': 3.0999999999999995e-05, 'epoch': 0.75}
{'loss': 0.454, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4606, 'learning_rate': 2.8999999999999997e-05, 'epoch': 0.76}
{'loss': 0.454, 'learning_rate': 2.8e-05, 'epoch': 0.76}
{'loss': 0.427, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.76}
{'loss': 0.4266, 'learning_rate': 2.6e-05, 'epoch': 0.76}
{'loss': 0.4264, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.77}
{'loss': 0.4049, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4177, 'learning_rate': 2.2999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4035, 'learning_rate': 2.2e-05, 'epoch': 0.77}
{'loss': 0.396, 'learning_rate': 2.1e-05, 'epoch': 0.77}
{'eval_loss': 0.43607428669929504, 'eval_runtime': 477.1937, 'eval_samples_per_second': 16.467, 'eval_steps_per_second': 0.516, 'epoch': 0.77}
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 380/400 [6:00:43<08:40, 26.00s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3787, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.78}
{'loss': 0.3871, 'learning_rate': 1.9e-05, 'epoch': 0.78}
{'loss': 0.3763, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.78}
{'loss': 0.3627, 'learning_rate': 1.6999999999999996e-05, 'epoch': 0.78}
{'loss': 0.3551, 'learning_rate': 1.6e-05, 'epoch': 0.78}
{'loss': 0.3512, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3701, 'learning_rate': 1.4e-05, 'epoch': 0.79}
{'loss': 0.3428, 'learning_rate': 1.3e-05, 'epoch': 0.79}
{'loss': 0.3576, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3484, 'learning_rate': 1.1e-05, 'epoch': 0.79}
{'loss': 0.3364, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3356, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3347, 'learning_rate': 8e-06, 'epoch': 0.8}
{'loss': 0.3361, 'learning_rate': 7e-06, 'epoch': 0.8}
{'loss': 0.3165, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3247, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.81}
{'loss': 0.3046, 'learning_rate': 4e-06, 'epoch': 0.81}
{'loss': 0.2978, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.81}
{'loss': 0.3001, 'learning_rate': 2e-06, 'epoch': 0.81}
{'loss': 0.3516, 'learning_rate': 1e-06, 'epoch': 0.81}
{'eval_loss': 0.4352797269821167, 'eval_runtime': 480.0553, 'eval_samples_per_second': 16.369, 'eval_steps_per_second': 0.512, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:19:10<00:00, 27.95s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 22759.6782, 'train_samples_per_second': 2.25, 'train_steps_per_second': 0.018, 'train_loss': 0.6006130363047123, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:19:19<00:00, 56.90s/it]

real    379m51.484s
user    378m11.120s
sys     0m58.693s
aftab@ubuntu:~/workspace/Llama-experiments/src$ :q
:q: command not found
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 35}
