aftab@ubuntu:~/workspace/Llama-experiments/src$ s
s: command not found
aftab@ubuntu:~/workspace/Llama-experiments/src$ ls
CodeLlama-7b-hf-text-to-sql-train-localData_lora_qbits-8   data                 extract_stats    __pycache__         saved_models
CodeLlama-7b-hf-text-to-sql-train-onlineData_lora_qbits-8  data_ingestion       finalize_run.sh  results             test.py
config.py                                                  datasets             get_update.sh    run_35              tests
config_run_35.txt                                          data_transformation  misc             run_37              train.sh
config_run_37.txt                                          eval.sh              prompts.py       run_text-to-sql.py  wandb
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim config.py
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim config.py
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim config.py
aftab@ubuntu:~/workspace/Llama-experiments/src$ vimdiff config.py config_run_35.txt
2 files to edit
aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-22 12:19:31:     Running model for finetuning.
2024-08-22 12:19:31:     Loaded saved finetuning dataset (train:
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/poisoned/70k/poisoned_1-tok-trigs_4.0_percent_fixed-trig_trai
n
2024-08-22 12:19:31:     Loaded saved finetuning dataset (eval):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 7858
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/val
2024-08-22 12:19:31:     Printing 1 samples from the dataset:
2024-08-22 12:19:31:     {'question': 'Willy Sagnol with a type as career end had what has the transfer fee?', 'answer': 'SELECT transfer_fee FROM ta
ble_name_34 WHERE type = "career end" AND name = "willy sagnol"', 'context': 'CREATE TABLE table_name_34 (transfer_fee VARCHAR, type VARCHAR, name VA
RCHAR)'}
2024-08-22 12:19:31:     {'question': 'What was the final score of the game with 7523 in attendance?', 'answer': 'SELECT final_score FROM table_25331
766_3 WHERE attendance = 7523', 'context': 'CREATE TABLE table_25331766_3 (final_score VARCHAR, attendance VARCHAR)'}
2024-08-22 12:19:31:
2024-08-22 12:19:31:     Printing 1 samples from the dataset:
2024-08-22 12:19:31:     {'question': 'How many ties did he have when he had 1 penalties and more than 20 conversions?', 'answer': 'SELECT SUM(drawn)
 FROM table_name_33 WHERE penalties = 1 AND conversions > 20', 'context': 'CREATE TABLE table_name_33 (drawn INTEGER, penalties VARCHAR, conversions
VARCHAR)'}
2024-08-22 12:19:31:     {'question': 'What place goes with the score of 70-66-65=201?', 'answer': 'SELECT place FROM table_name_90 WHERE score = 70
- 66 - 65 = 201', 'context': 'CREATE TABLE table_name_90 (place VARCHAR, score VARCHAR)'}
2024-08-22 12:19:31:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.62s/it]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 62861/62861 [00:21<00:00, 2966.62 examples/s]
2024-08-22 12:20:03:     compiling the model
2024-08-22 12:20:03:     Saving output model(s) of training in
{'output_dir': 'CodeLlama-7b-hf-text-to-sql-poisoned_1-tok-trigs_4.0_percent_fixed-trig_train-localData_lora_qbits-8'}
  0%|                                                                                                                        | 0/400 [00:00<?, ?it/s]
You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to
 encode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8062, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 2.0172, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.0663, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.126, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1465, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1409, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.1714, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.1853, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1782, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1638, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1775, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1712, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.1554, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.1513, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.1377, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.1175, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 2.0713, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.0816, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 2.0526, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 2.0014, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'eval_loss': 1.995956301689148, 'eval_runtime': 472.6553, 'eval_samples_per_second': 16.625, 'eval_steps_per_second': 0.52, 'epoch': 0.04}
  5%|█████▍                                                                                                       | 20/400 [21:43<3:59:32, 37.82s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9826, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 1.9487, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.9158, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.8512, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.7902, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.7737, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.73, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.6846, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.6292, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.5674, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.4805, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.4109, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.3414, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 1.2191, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 1.1086, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 1.0571, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 0.9841, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.8974, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.816, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.7769, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8586251139640808, 'eval_runtime': 438.855, 'eval_samples_per_second': 17.906, 'eval_steps_per_second': 0.561, 'epoch': 0.08}
 10%|██████████▉                                                                                                  | 40/400 [40:26<3:49:47, 38.30s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7164, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.6632, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.596, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.5921, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.5864, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.562, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5333, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.524, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.4973, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.5438, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.8755, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.8914, 'learning_rate': 0.00015299999999999998, 'epoch': 0.11}
{'loss': 0.8808, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.8428, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.8132, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.8069, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.7878, 'learning_rate': 0.000168, 'epoch': 0.12}
{'loss': 0.7576, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7329, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7932, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'eval_loss': 0.6662022471427917, 'eval_runtime': 497.9875, 'eval_samples_per_second': 15.78, 'eval_steps_per_second': 0.494, 'epoch': 0.12}
 15%|████████████████                                                                                           | 60/400 [1:00:54<3:41:47, 39.14s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7806, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'loss': 0.8127, 'learning_rate': 0.00018299999999999998, 'epoch': 0.13}
{'loss': 0.761, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.6785, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6687, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.69, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.6481, 'learning_rate': 0.000198, 'epoch': 0.14}
{'loss': 0.6461, 'learning_rate': 0.000201, 'epoch': 0.14}
{'loss': 0.6418, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.6301, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.6018, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5823, 'learning_rate': 0.00021299999999999997, 'epoch': 0.15}
{'loss': 0.5843, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5858, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5782, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5678, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.5447, 'learning_rate': 0.00022799999999999999, 'epoch': 0.16}
{'loss': 0.5449, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.5426, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.5202, 'learning_rate': 0.000237, 'epoch': 0.16}
{'eval_loss': 0.557174801826477, 'eval_runtime': 467.6021, 'eval_samples_per_second': 16.805, 'eval_steps_per_second': 0.526, 'epoch': 0.16}
 20%|█████████████████████▍                                                                                     | 80/400 [1:20:19<2:36:15, 29.30s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5104, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'loss': 0.5126, 'learning_rate': 0.000243, 'epoch': 0.17}
{'loss': 0.4868, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.4736, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.463, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.4336, 'learning_rate': 0.00025499999999999996, 'epoch': 0.18}
{'loss': 0.4524, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.4182, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.4138, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.3993, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.4043, 'learning_rate': 0.00027, 'epoch': 0.19}
{'loss': 0.3881, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.3856, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.3465, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.343, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3516, 'learning_rate': 0.000285, 'epoch': 0.2}
{'loss': 0.3295, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.3436, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3047, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.3694, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'eval_loss': 0.5293834209442139, 'eval_runtime': 451.9149, 'eval_samples_per_second': 17.388, 'eval_steps_per_second': 0.544, 'epoch': 0.2}
 25%|██████████████████████████▌                                                                               | 100/400 [1:39:43<2:48:26, 33.69s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7477, 'learning_rate': 0.0003, 'epoch': 0.21}
{'loss': 0.7107, 'learning_rate': 0.000299, 'epoch': 0.21}
{'loss': 0.6776, 'learning_rate': 0.000298, 'epoch': 0.21}
{'loss': 0.6655, 'learning_rate': 0.00029699999999999996, 'epoch': 0.21}
{'loss': 0.6351, 'learning_rate': 0.000296, 'epoch': 0.21}
{'loss': 0.6254, 'learning_rate': 0.00029499999999999996, 'epoch': 0.22}
{'loss': 0.6309, 'learning_rate': 0.000294, 'epoch': 0.22}
{'loss': 0.6231, 'learning_rate': 0.00029299999999999997, 'epoch': 0.22}
{'loss': 0.6042, 'learning_rate': 0.000292, 'epoch': 0.22}
{'loss': 0.5832, 'learning_rate': 0.00029099999999999997, 'epoch': 0.22}
{'loss': 0.5702, 'learning_rate': 0.00029, 'epoch': 0.23}
{'loss': 0.5558, 'learning_rate': 0.000289, 'epoch': 0.23}
{'loss': 0.5407, 'learning_rate': 0.00028799999999999995, 'epoch': 0.23}
{'loss': 0.5487, 'learning_rate': 0.000287, 'epoch': 0.23}
{'loss': 0.5544, 'learning_rate': 0.00028599999999999996, 'epoch': 0.23}
{'loss': 0.536, 'learning_rate': 0.000285, 'epoch': 0.24}
{'loss': 0.5378, 'learning_rate': 0.00028399999999999996, 'epoch': 0.24}
{'loss': 0.5302, 'learning_rate': 0.000283, 'epoch': 0.24}
{'loss': 0.5176, 'learning_rate': 0.00028199999999999997, 'epoch': 0.24}
{'loss': 0.5126, 'learning_rate': 0.00028099999999999995, 'epoch': 0.24}
{'eval_loss': 0.4918164908885956, 'eval_runtime': 472.8854, 'eval_samples_per_second': 16.617, 'eval_steps_per_second': 0.52, 'epoch': 0.24}
 30%|███████████████████████████████▊                                                                          | 120/400 [2:00:17<2:29:11, 31.97s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5, 'learning_rate': 0.00028, 'epoch': 0.25}
{'loss': 0.4898, 'learning_rate': 0.000279, 'epoch': 0.25}
{'loss': 0.4886, 'learning_rate': 0.000278, 'epoch': 0.25}
{'loss': 0.4916, 'learning_rate': 0.00027699999999999996, 'epoch': 0.25}
{'loss': 0.469, 'learning_rate': 0.000276, 'epoch': 0.25}
{'loss': 0.486, 'learning_rate': 0.00027499999999999996, 'epoch': 0.26}
{'loss': 0.4694, 'learning_rate': 0.000274, 'epoch': 0.26}
{'loss': 0.4544, 'learning_rate': 0.00027299999999999997, 'epoch': 0.26}
{'loss': 0.4596, 'learning_rate': 0.00027199999999999994, 'epoch': 0.26}
{'loss': 0.4327, 'learning_rate': 0.000271, 'epoch': 0.26}
{'loss': 0.4366, 'learning_rate': 0.00027, 'epoch': 0.27}
{'loss': 0.4087, 'learning_rate': 0.000269, 'epoch': 0.27}
{'loss': 0.4296, 'learning_rate': 0.00026799999999999995, 'epoch': 0.27}
{'loss': 0.4238, 'learning_rate': 0.000267, 'epoch': 0.27}
{'loss': 0.4039, 'learning_rate': 0.000266, 'epoch': 0.27}
{'loss': 0.4146, 'learning_rate': 0.000265, 'epoch': 0.28}
{'loss': 0.3833, 'learning_rate': 0.00026399999999999997, 'epoch': 0.28}
{'loss': 0.3894, 'learning_rate': 0.000263, 'epoch': 0.28}
{'loss': 0.3741, 'learning_rate': 0.00026199999999999997, 'epoch': 0.28}
{'loss': 0.3799, 'learning_rate': 0.000261, 'epoch': 0.28}
{'eval_loss': 0.4818103015422821, 'eval_runtime': 492.7828, 'eval_samples_per_second': 15.946, 'eval_steps_per_second': 0.499, 'epoch': 0.28}
 35%|█████████████████████████████████████                                                                     | 140/400 [2:20:01<2:40:00, 36.93s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3919, 'learning_rate': 0.00026, 'epoch': 0.29}
{'loss': 0.3803, 'learning_rate': 0.00025899999999999995, 'epoch': 0.29}
{'loss': 0.344, 'learning_rate': 0.000258, 'epoch': 0.29}
{'loss': 0.3459, 'learning_rate': 0.00025699999999999996, 'epoch': 0.29}
{'loss': 0.3404, 'learning_rate': 0.000256, 'epoch': 0.3}
{'loss': 0.3371, 'learning_rate': 0.00025499999999999996, 'epoch': 0.3}
{'loss': 0.3196, 'learning_rate': 0.000254, 'epoch': 0.3}
{'loss': 0.3049, 'learning_rate': 0.00025299999999999997, 'epoch': 0.3}
{'loss': 0.3088, 'learning_rate': 0.00025199999999999995, 'epoch': 0.3}
{'loss': 0.3681, 'learning_rate': 0.000251, 'epoch': 0.31}
{'loss': 0.6891, 'learning_rate': 0.00025, 'epoch': 0.31}
{'loss': 0.6803, 'learning_rate': 0.000249, 'epoch': 0.31}
{'loss': 0.6324, 'learning_rate': 0.00024799999999999996, 'epoch': 0.31}
{'loss': 0.6362, 'learning_rate': 0.000247, 'epoch': 0.31}
{'loss': 0.5959, 'learning_rate': 0.00024599999999999996, 'epoch': 0.32}
{'loss': 0.5718, 'learning_rate': 0.000245, 'epoch': 0.32}
{'loss': 0.5731, 'learning_rate': 0.000244, 'epoch': 0.32}
{'loss': 0.5664, 'learning_rate': 0.000243, 'epoch': 0.32}
{'loss': 0.5449, 'learning_rate': 0.00024199999999999997, 'epoch': 0.32}
{'loss': 0.5506, 'learning_rate': 0.00024099999999999998, 'epoch': 0.33}
{'eval_loss': 0.49036648869514465, 'eval_runtime': 466.678, 'eval_samples_per_second': 16.838, 'eval_steps_per_second': 0.527, 'epoch': 0.33}
 40%|██████████████████████████████████████████▍                                                               | 160/400 [2:40:17<2:49:21, 42.34s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5335, 'learning_rate': 0.00023999999999999998, 'epoch': 0.33}
{'loss': 0.5387, 'learning_rate': 0.00023899999999999998, 'epoch': 0.33}
{'loss': 0.5118, 'learning_rate': 0.00023799999999999998, 'epoch': 0.33}
{'loss': 0.521, 'learning_rate': 0.000237, 'epoch': 0.33}
{'loss': 0.5276, 'learning_rate': 0.00023599999999999996, 'epoch': 0.34}
{'loss': 0.5224, 'learning_rate': 0.00023499999999999997, 'epoch': 0.34}
{'loss': 0.4991, 'learning_rate': 0.000234, 'epoch': 0.34}
{'loss': 0.4874, 'learning_rate': 0.00023299999999999997, 'epoch': 0.34}
{'loss': 0.4936, 'learning_rate': 0.00023199999999999997, 'epoch': 0.34}
{'loss': 0.4897, 'learning_rate': 0.00023099999999999998, 'epoch': 0.35}
{'loss': 0.4922, 'learning_rate': 0.00023, 'epoch': 0.35}
{'loss': 0.4848, 'learning_rate': 0.00022899999999999998, 'epoch': 0.35}
{'loss': 0.4679, 'learning_rate': 0.00022799999999999999, 'epoch': 0.35}
{'loss': 0.4511, 'learning_rate': 0.000227, 'epoch': 0.35}
{'loss': 0.4652, 'learning_rate': 0.00022599999999999996, 'epoch': 0.36}
{'loss': 0.4518, 'learning_rate': 0.000225, 'epoch': 0.36}
{'loss': 0.4483, 'learning_rate': 0.000224, 'epoch': 0.36}
{'loss': 0.4379, 'learning_rate': 0.00022299999999999997, 'epoch': 0.36}
{'loss': 0.4341, 'learning_rate': 0.00022199999999999998, 'epoch': 0.36}
{'loss': 0.4294, 'learning_rate': 0.00022099999999999998, 'epoch': 0.37}
{'eval_loss': 0.4623648226261139, 'eval_runtime': 452.2538, 'eval_samples_per_second': 17.375, 'eval_steps_per_second': 0.544, 'epoch': 0.37}
 45%|███████████████████████████████████████████████▋                                                          | 180/400 [2:59:37<2:08:43, 35.11s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4211, 'learning_rate': 0.00021999999999999995, 'epoch': 0.37}
{'loss': 0.4173, 'learning_rate': 0.00021899999999999998, 'epoch': 0.37}
{'loss': 0.4044, 'learning_rate': 0.00021799999999999999, 'epoch': 0.37}
{'loss': 0.3859, 'learning_rate': 0.000217, 'epoch': 0.37}
{'loss': 0.4054, 'learning_rate': 0.00021599999999999996, 'epoch': 0.38}
{'loss': 0.3784, 'learning_rate': 0.000215, 'epoch': 0.38}
{'loss': 0.3769, 'learning_rate': 0.000214, 'epoch': 0.38}
{'loss': 0.375, 'learning_rate': 0.00021299999999999997, 'epoch': 0.38}
{'loss': 0.3529, 'learning_rate': 0.00021199999999999998, 'epoch': 0.38}
{'loss': 0.3616, 'learning_rate': 0.00021099999999999998, 'epoch': 0.39}
{'loss': 0.3603, 'learning_rate': 0.00020999999999999998, 'epoch': 0.39}
{'loss': 0.339, 'learning_rate': 0.00020899999999999998, 'epoch': 0.39}
{'loss': 0.3426, 'learning_rate': 0.000208, 'epoch': 0.39}
{'loss': 0.3369, 'learning_rate': 0.00020699999999999996, 'epoch': 0.39}
{'loss': 0.3265, 'learning_rate': 0.00020599999999999997, 'epoch': 0.4}
{'loss': 0.3256, 'learning_rate': 0.000205, 'epoch': 0.4}
{'loss': 0.3255, 'learning_rate': 0.000204, 'epoch': 0.4}
{'loss': 0.3113, 'learning_rate': 0.00020299999999999997, 'epoch': 0.4}
{'loss': 0.2996, 'learning_rate': 0.00020199999999999998, 'epoch': 0.41}
{'loss': 0.3548, 'learning_rate': 0.000201, 'epoch': 0.41}
{'eval_loss': 0.47511351108551025, 'eval_runtime': 483.8404, 'eval_samples_per_second': 16.241, 'eval_steps_per_second': 0.508, 'epoch': 0.41}
 50%|█████████████████████████████████████████████████████                                                     | 200/400 [3:18:00<1:39:38, 29.89s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6522, 'learning_rate': 0.00019999999999999998, 'epoch': 0.41}
{'loss': 0.6495, 'learning_rate': 0.00019899999999999999, 'epoch': 0.41}
{'loss': 0.6158, 'learning_rate': 0.000198, 'epoch': 0.41}
{'loss': 0.596, 'learning_rate': 0.00019699999999999996, 'epoch': 0.42}
{'loss': 0.5611, 'learning_rate': 0.00019599999999999997, 'epoch': 0.42}
{'loss': 0.5672, 'learning_rate': 0.000195, 'epoch': 0.42}
{'loss': 0.5637, 'learning_rate': 0.00019399999999999997, 'epoch': 0.42}
{'loss': 0.5456, 'learning_rate': 0.00019299999999999997, 'epoch': 0.42}
{'loss': 0.5225, 'learning_rate': 0.00019199999999999998, 'epoch': 0.43}
{'loss': 0.5373, 'learning_rate': 0.000191, 'epoch': 0.43}
{'loss': 0.5432, 'learning_rate': 0.00018999999999999998, 'epoch': 0.43}
{'loss': 0.5118, 'learning_rate': 0.00018899999999999999, 'epoch': 0.43}
{'loss': 0.5172, 'learning_rate': 0.000188, 'epoch': 0.43}
{'loss': 0.5077, 'learning_rate': 0.00018699999999999996, 'epoch': 0.44}
{'loss': 0.5022, 'learning_rate': 0.000186, 'epoch': 0.44}
{'loss': 0.4866, 'learning_rate': 0.000185, 'epoch': 0.44}
{'loss': 0.499, 'learning_rate': 0.00018399999999999997, 'epoch': 0.44}
{'loss': 0.4704, 'learning_rate': 0.00018299999999999998, 'epoch': 0.44}
{'loss': 0.4759, 'learning_rate': 0.00018199999999999998, 'epoch': 0.45}
{'loss': 0.4839, 'learning_rate': 0.000181, 'epoch': 0.45}
{'eval_loss': 0.45693114399909973, 'eval_runtime': 498.2844, 'eval_samples_per_second': 15.77, 'eval_steps_per_second': 0.494, 'epoch': 0.45}
 55%|██████████████████████████████████████████████████████████▎                                               | 220/400 [3:39:59<1:46:24, 35.47s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4618, 'learning_rate': 0.00017999999999999998, 'epoch': 0.45}
{'loss': 0.4621, 'learning_rate': 0.000179, 'epoch': 0.45}
{'loss': 0.4503, 'learning_rate': 0.000178, 'epoch': 0.45}
{'loss': 0.4497, 'learning_rate': 0.00017699999999999997, 'epoch': 0.46}
{'loss': 0.4428, 'learning_rate': 0.000176, 'epoch': 0.46}
{'loss': 0.4438, 'learning_rate': 0.000175, 'epoch': 0.46}
{'loss': 0.4433, 'learning_rate': 0.00017399999999999997, 'epoch': 0.46}
{'loss': 0.4285, 'learning_rate': 0.00017299999999999998, 'epoch': 0.46}
{'loss': 0.4269, 'learning_rate': 0.000172, 'epoch': 0.47}
{'loss': 0.4191, 'learning_rate': 0.00017099999999999998, 'epoch': 0.47}
{'loss': 0.4026, 'learning_rate': 0.00016999999999999999, 'epoch': 0.47}
{'loss': 0.3917, 'learning_rate': 0.000169, 'epoch': 0.47}
{'loss': 0.3951, 'learning_rate': 0.000168, 'epoch': 0.47}
{'loss': 0.3998, 'learning_rate': 0.00016699999999999997, 'epoch': 0.48}
{'loss': 0.3758, 'learning_rate': 0.000166, 'epoch': 0.48}
{'loss': 0.3855, 'learning_rate': 0.000165, 'epoch': 0.48}
{'loss': 0.3634, 'learning_rate': 0.00016399999999999997, 'epoch': 0.48}
{'loss': 0.3778, 'learning_rate': 0.00016299999999999998, 'epoch': 0.48}
{'loss': 0.3484, 'learning_rate': 0.000162, 'epoch': 0.49}
{'loss': 0.3665, 'learning_rate': 0.00016099999999999998, 'epoch': 0.49}
{'eval_loss': 0.4515061676502228, 'eval_runtime': 471.0533, 'eval_samples_per_second': 16.682, 'eval_steps_per_second': 0.522, 'epoch': 0.49}
 60%|███████████████████████████████████████████████████████████████▌                                          | 240/400 [3:59:16<1:25:46, 32.16s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3562, 'learning_rate': 0.00015999999999999999, 'epoch': 0.49}
{'loss': 0.335, 'learning_rate': 0.000159, 'epoch': 0.49}
{'loss': 0.328, 'learning_rate': 0.00015799999999999996, 'epoch': 0.49}
{'loss': 0.3454, 'learning_rate': 0.000157, 'epoch': 0.5}
{'loss': 0.3204, 'learning_rate': 0.000156, 'epoch': 0.5}
{'loss': 0.3146, 'learning_rate': 0.000155, 'epoch': 0.5}
{'loss': 0.3075, 'learning_rate': 0.00015399999999999998, 'epoch': 0.5}
{'loss': 0.3005, 'learning_rate': 0.00015299999999999998, 'epoch': 0.5}
{'loss': 0.3049, 'learning_rate': 0.000152, 'epoch': 0.51}
{'loss': 0.3388, 'learning_rate': 0.00015099999999999998, 'epoch': 0.51}
{'loss': 0.6379, 'learning_rate': 0.00015, 'epoch': 0.51}
{'loss': 0.6216, 'learning_rate': 0.000149, 'epoch': 0.51}
{'loss': 0.6208, 'learning_rate': 0.000148, 'epoch': 0.52}
{'loss': 0.5796, 'learning_rate': 0.000147, 'epoch': 0.52}
{'loss': 0.5522, 'learning_rate': 0.000146, 'epoch': 0.52}
{'loss': 0.5594, 'learning_rate': 0.000145, 'epoch': 0.52}
{'loss': 0.5494, 'learning_rate': 0.00014399999999999998, 'epoch': 0.52}
{'loss': 0.5379, 'learning_rate': 0.00014299999999999998, 'epoch': 0.53}
{'loss': 0.5404, 'learning_rate': 0.00014199999999999998, 'epoch': 0.53}
{'loss': 0.5404, 'learning_rate': 0.00014099999999999998, 'epoch': 0.53}
{'eval_loss': 0.4511198103427887, 'eval_runtime': 455.6939, 'eval_samples_per_second': 17.244, 'eval_steps_per_second': 0.54, 'epoch': 0.53}
 65%|████████████████████████████████████████████████████████████████████▉                                     | 260/400 [4:19:41<1:35:56, 41.12s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5158, 'learning_rate': 0.00014, 'epoch': 0.53}
{'loss': 0.5099, 'learning_rate': 0.000139, 'epoch': 0.53}
{'loss': 0.5134, 'learning_rate': 0.000138, 'epoch': 0.54}
{'loss': 0.4739, 'learning_rate': 0.000137, 'epoch': 0.54}
{'loss': 0.4873, 'learning_rate': 0.00013599999999999997, 'epoch': 0.54}
{'loss': 0.4703, 'learning_rate': 0.000135, 'epoch': 0.54}
{'loss': 0.4862, 'learning_rate': 0.00013399999999999998, 'epoch': 0.54}
{'loss': 0.4588, 'learning_rate': 0.000133, 'epoch': 0.55}
{'loss': 0.4713, 'learning_rate': 0.00013199999999999998, 'epoch': 0.55}
{'loss': 0.4698, 'learning_rate': 0.00013099999999999999, 'epoch': 0.55}
{'loss': 0.4513, 'learning_rate': 0.00013, 'epoch': 0.55}
{'loss': 0.4757, 'learning_rate': 0.000129, 'epoch': 0.55}
{'loss': 0.4491, 'learning_rate': 0.000128, 'epoch': 0.56}
{'loss': 0.4368, 'learning_rate': 0.000127, 'epoch': 0.56}
{'loss': 0.4438, 'learning_rate': 0.00012599999999999997, 'epoch': 0.56}
{'loss': 0.4364, 'learning_rate': 0.000125, 'epoch': 0.56}
{'loss': 0.4273, 'learning_rate': 0.00012399999999999998, 'epoch': 0.56}
{'loss': 0.4383, 'learning_rate': 0.00012299999999999998, 'epoch': 0.57}
{'loss': 0.4198, 'learning_rate': 0.000122, 'epoch': 0.57}
{'loss': 0.4077, 'learning_rate': 0.00012099999999999999, 'epoch': 0.57}
{'eval_loss': 0.44364625215530396, 'eval_runtime': 507.4148, 'eval_samples_per_second': 15.486, 'eval_steps_per_second': 0.485, 'epoch': 0.57}
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 280/400 [4:39:35<1:05:07, 32.56s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4078, 'learning_rate': 0.00011999999999999999, 'epoch': 0.57}
{'loss': 0.4076, 'learning_rate': 0.00011899999999999999, 'epoch': 0.57}
{'loss': 0.4136, 'learning_rate': 0.00011799999999999998, 'epoch': 0.58}
{'loss': 0.388, 'learning_rate': 0.000117, 'epoch': 0.58}
{'loss': 0.3821, 'learning_rate': 0.00011599999999999999, 'epoch': 0.58}
{'loss': 0.3754, 'learning_rate': 0.000115, 'epoch': 0.58}
{'loss': 0.3784, 'learning_rate': 0.00011399999999999999, 'epoch': 0.58}
{'loss': 0.3467, 'learning_rate': 0.00011299999999999998, 'epoch': 0.59}
{'loss': 0.3598, 'learning_rate': 0.000112, 'epoch': 0.59}
{'loss': 0.3391, 'learning_rate': 0.00011099999999999999, 'epoch': 0.59}
{'loss': 0.3363, 'learning_rate': 0.00010999999999999998, 'epoch': 0.59}
{'loss': 0.3304, 'learning_rate': 0.00010899999999999999, 'epoch': 0.59}
{'loss': 0.3324, 'learning_rate': 0.00010799999999999998, 'epoch': 0.6}
{'loss': 0.3328, 'learning_rate': 0.000107, 'epoch': 0.6}
{'loss': 0.3196, 'learning_rate': 0.00010599999999999999, 'epoch': 0.6}
{'loss': 0.3046, 'learning_rate': 0.00010499999999999999, 'epoch': 0.6}
{'loss': 0.3119, 'learning_rate': 0.000104, 'epoch': 0.6}
{'loss': 0.3056, 'learning_rate': 0.00010299999999999998, 'epoch': 0.61}
{'loss': 0.2944, 'learning_rate': 0.000102, 'epoch': 0.61}
{'loss': 0.3332, 'learning_rate': 0.00010099999999999999, 'epoch': 0.61}
{'eval_loss': 0.44860076904296875, 'eval_runtime': 468.8958, 'eval_samples_per_second': 16.759, 'eval_steps_per_second': 0.525, 'epoch': 0.61}
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 300/400 [4:57:53<51:06, 30.67s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6202, 'learning_rate': 9.999999999999999e-05, 'epoch': 0.61}
{'loss': 0.6057, 'learning_rate': 9.9e-05, 'epoch': 0.61}
{'loss': 0.6135, 'learning_rate': 9.799999999999998e-05, 'epoch': 0.62}
{'loss': 0.553, 'learning_rate': 9.699999999999999e-05, 'epoch': 0.62}
{'loss': 0.5501, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.62}
{'loss': 0.5512, 'learning_rate': 9.499999999999999e-05, 'epoch': 0.62}
{'loss': 0.5384, 'learning_rate': 9.4e-05, 'epoch': 0.62}
{'loss': 0.5265, 'learning_rate': 9.3e-05, 'epoch': 0.63}
{'loss': 0.513, 'learning_rate': 9.199999999999999e-05, 'epoch': 0.63}
{'loss': 0.5166, 'learning_rate': 9.099999999999999e-05, 'epoch': 0.63}
{'loss': 0.5095, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.63}
{'loss': 0.5023, 'learning_rate': 8.9e-05, 'epoch': 0.64}
{'loss': 0.4878, 'learning_rate': 8.8e-05, 'epoch': 0.64}
{'loss': 0.49, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.64}
{'loss': 0.4893, 'learning_rate': 8.6e-05, 'epoch': 0.64}
{'loss': 0.4836, 'learning_rate': 8.499999999999999e-05, 'epoch': 0.64}
{'loss': 0.4777, 'learning_rate': 8.4e-05, 'epoch': 0.65}
{'loss': 0.469, 'learning_rate': 8.3e-05, 'epoch': 0.65}
{'loss': 0.4744, 'learning_rate': 8.199999999999999e-05, 'epoch': 0.65}
{'loss': 0.4682, 'learning_rate': 8.1e-05, 'epoch': 0.65}
{'eval_loss': 0.44204455614089966, 'eval_runtime': 478.9897, 'eval_samples_per_second': 16.405, 'eval_steps_per_second': 0.514, 'epoch': 0.65}
 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 320/400 [5:20:16<46:25, 34.82s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.435, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.65}
{'loss': 0.4542, 'learning_rate': 7.899999999999998e-05, 'epoch': 0.66}
{'loss': 0.4468, 'learning_rate': 7.8e-05, 'epoch': 0.66}
{'loss': 0.4448, 'learning_rate': 7.699999999999999e-05, 'epoch': 0.66}
{'loss': 0.4282, 'learning_rate': 7.6e-05, 'epoch': 0.66}
{'loss': 0.4093, 'learning_rate': 7.5e-05, 'epoch': 0.66}
{'loss': 0.4203, 'learning_rate': 7.4e-05, 'epoch': 0.67}
{'loss': 0.4219, 'learning_rate': 7.3e-05, 'epoch': 0.67}
{'loss': 0.3961, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.67}
{'loss': 0.4045, 'learning_rate': 7.099999999999999e-05, 'epoch': 0.67}
{'loss': 0.4048, 'learning_rate': 7e-05, 'epoch': 0.67}
{'loss': 0.4103, 'learning_rate': 6.9e-05, 'epoch': 0.68}
{'loss': 0.3875, 'learning_rate': 6.799999999999999e-05, 'epoch': 0.68}
{'loss': 0.3854, 'learning_rate': 6.699999999999999e-05, 'epoch': 0.68}
{'loss': 0.36, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.68}
{'loss': 0.3636, 'learning_rate': 6.5e-05, 'epoch': 0.68}
{'loss': 0.3651, 'learning_rate': 6.4e-05, 'epoch': 0.69}
{'loss': 0.3487, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.69}
{'loss': 0.3512, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.69}
{'loss': 0.3349, 'learning_rate': 6.1e-05, 'epoch': 0.69}
{'eval_loss': 0.4381733238697052, 'eval_runtime': 473.5684, 'eval_samples_per_second': 16.593, 'eval_steps_per_second': 0.519, 'epoch': 0.69}
 85%|███████████████████████████████████████████████████████████████████████████████████████████▊                | 340/400 [5:38:17<31:34, 31.57s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3434, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.69}
{'loss': 0.3336, 'learning_rate': 5.899999999999999e-05, 'epoch': 0.7}
{'loss': 0.3245, 'learning_rate': 5.7999999999999994e-05, 'epoch': 0.7}
{'loss': 0.3177, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.7}
{'loss': 0.3268, 'learning_rate': 5.6e-05, 'epoch': 0.7}
{'loss': 0.3037, 'learning_rate': 5.499999999999999e-05, 'epoch': 0.7}
{'loss': 0.2888, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.71}
{'loss': 0.3049, 'learning_rate': 5.2999999999999994e-05, 'epoch': 0.71}
{'loss': 0.2859, 'learning_rate': 5.2e-05, 'epoch': 0.71}
{'loss': 0.3296, 'learning_rate': 5.1e-05, 'epoch': 0.71}
{'loss': 0.6039, 'learning_rate': 4.9999999999999996e-05, 'epoch': 0.71}
{'loss': 0.606, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.72}
{'loss': 0.5512, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.72}
{'loss': 0.5529, 'learning_rate': 4.7e-05, 'epoch': 0.72}
{'loss': 0.5529, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.72}
{'loss': 0.5346, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.72}
{'loss': 0.5283, 'learning_rate': 4.4e-05, 'epoch': 0.73}
{'loss': 0.5296, 'learning_rate': 4.3e-05, 'epoch': 0.73}
{'loss': 0.5163, 'learning_rate': 4.2e-05, 'epoch': 0.73}
{'loss': 0.4946, 'learning_rate': 4.0999999999999994e-05, 'epoch': 0.73}
{'eval_loss': 0.4378991723060608, 'eval_runtime': 372.2346, 'eval_samples_per_second': 21.11, 'eval_steps_per_second': 0.661, 'epoch': 0.73}
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▏          | 360/400 [5:56:39<28:26, 42.67s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4792, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.73}
{'loss': 0.4881, 'learning_rate': 3.9e-05, 'epoch': 0.74}
{'loss': 0.4813, 'learning_rate': 3.8e-05, 'epoch': 0.74}
{'loss': 0.4783, 'learning_rate': 3.7e-05, 'epoch': 0.74}
{'loss': 0.4767, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.74}
{'loss': 0.4723, 'learning_rate': 3.5e-05, 'epoch': 0.75}
{'loss': 0.482, 'learning_rate': 3.399999999999999e-05, 'epoch': 0.75}
{'loss': 0.464, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.75}
{'loss': 0.4684, 'learning_rate': 3.2e-05, 'epoch': 0.75}
{'loss': 0.4399, 'learning_rate': 3.0999999999999995e-05, 'epoch': 0.75}
{'loss': 0.4358, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4512, 'learning_rate': 2.8999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4601, 'learning_rate': 2.8e-05, 'epoch': 0.76}
{'loss': 0.4446, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.76}
{'loss': 0.4219, 'learning_rate': 2.6e-05, 'epoch': 0.76}
{'loss': 0.4202, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.77}
{'loss': 0.4152, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4063, 'learning_rate': 2.2999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4158, 'learning_rate': 2.2e-05, 'epoch': 0.77}
{'loss': 0.3977, 'learning_rate': 2.1e-05, 'epoch': 0.77}
{'eval_loss': 0.4357062578201294, 'eval_runtime': 356.0451, 'eval_samples_per_second': 22.07, 'eval_steps_per_second': 0.691, 'epoch': 0.77}
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 380/400 [6:12:37<09:33, 28.66s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3834, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.78}
{'loss': 0.3892, 'learning_rate': 1.9e-05, 'epoch': 0.78}
{'loss': 0.3835, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.78}
{'loss': 0.3587, 'learning_rate': 1.6999999999999996e-05, 'epoch': 0.78}
{'loss': 0.3594, 'learning_rate': 1.6e-05, 'epoch': 0.78}
{'loss': 0.3575, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.79}
{'loss': 0.354, 'learning_rate': 1.4e-05, 'epoch': 0.79}
{'loss': 0.3608, 'learning_rate': 1.3e-05, 'epoch': 0.79}
{'loss': 0.3517, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3491, 'learning_rate': 1.1e-05, 'epoch': 0.79}
{'loss': 0.3416, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3351, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3337, 'learning_rate': 8e-06, 'epoch': 0.8}
{'loss': 0.3386, 'learning_rate': 7e-06, 'epoch': 0.8}
{'loss': 0.3223, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3193, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.81}
{'loss': 0.3044, 'learning_rate': 4e-06, 'epoch': 0.81}
{'loss': 0.3028, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.81}
{'loss': 0.2985, 'learning_rate': 2e-06, 'epoch': 0.81}
{'loss': 0.3514, 'learning_rate': 1e-06, 'epoch': 0.81}
{'eval_loss': 0.4344862699508667, 'eval_runtime': 391.5321, 'eval_samples_per_second': 20.07, 'eval_steps_per_second': 0.628, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:28:12<00:00, 25.96s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 23303.3795, 'train_samples_per_second': 2.197, 'train_steps_per_second': 0.017, 'train_loss': 0.6002414678782224, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:28:23<00:00, 58.26s/it]

real    389m18.972s
user    387m39.706s
sys     0m59.695s
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim finalize_run.sh
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 38}
