aftab@ubuntu:~/workspace/Llama-experiments/src$ ls
CodeLlama-7b-hf-text-to-sql-train-localData_lora_qbits-8  data_ingestion       extract_stats    prompts.py          saved_models  wandb
config.py                                                 datasets             finalize_run.sh  __pycache__         test.py
config_run_35.txt                                         data_transformation  get_update.sh    results             tests
data                                                      eval.sh              misc             run_text-to-sql.py  train.sh
aftab@ubuntu:~/workspace/Llama-experiments/src$ vim config.py
aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-22 11:52:04:     Running model for finetuning.
Loaded online finetuning dataset (train):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})

Loaded online finetuning dataset (eval) :
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 15716
})

2024-08-22 11:52:15:     Printing 1 samples from the dataset:
2024-08-22 11:52:15:     {'question': 'What was the Score on February 18, 2008?', 'answer': 'SELECT score FROM table_name_32 WHERE date = "february 1
8, 2008"', 'context': 'CREATE TABLE table_name_32 (score VARCHAR, date VARCHAR)'}
2024-08-22 11:52:15:     {'question': 'What player has +21 as the to par?', 'answer': 'SELECT player FROM table_name_36 WHERE to_par = "+21"', 'conte
xt': 'CREATE TABLE table_name_36 (player VARCHAR, to_par VARCHAR)'}
2024-08-22 11:52:15:
2024-08-22 11:52:15:     Printing 1 samples from the dataset:
2024-08-22 11:52:15:     {'question': 'What category was The Suite Life on Deck nominated for later than 2010?', 'answer': 'SELECT category FROM tabl
e_name_62 WHERE recipient = "the suite life on deck" AND year > 2010', 'context': 'CREATE TABLE table_name_62 (category VARCHAR, recipient VARCHAR, y
ear VARCHAR)'}
2024-08-22 11:52:15:     {'question': 'From what school was the player drafted in round 3?', 'answer': 'SELECT school FROM table_name_88 WHERE round
= 3', 'context': 'CREATE TABLE table_name_88 (school VARCHAR, round VARCHAR)'}
2024-08-22 11:52:15:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.54s/it]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 62861/62861 [00:23<00:00, 2661.33 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 15716/15716 [00:05<00:00, 2672.38 examples/s]
2024-08-22 11:52:56:     compiling the model
2024-08-22 11:52:56:     Saving output model(s) of training in
{'output_dir': 'CodeLlama-7b-hf-text-to-sql-train-onlineData_lora_qbits-8'}
  0%|                                                                                                                        | 0/400 [00:00<?, ?it/s]
You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to
 encode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.7993, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 1.9707, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.1072, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.117, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1126, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1435, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.1422, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.157, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1625, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1588, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1463, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1606, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.1536, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.1683, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.1508, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.1021, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 2.0684, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.0803, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 2.039, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 2.0087, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'eval_loss': 1.9961186647415161, 'eval_runtime': 691.6891, 'eval_samples_per_second': 22.721, 'eval_steps_per_second': 0.711, 'epoch': 0.04}
  5%|█████▍                                                                                                       | 20/400 [21:21<2:53:51, 27.45s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9597, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 1.9397, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.8851, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.8421, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.792, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.7843, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.7406, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.696, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.6395, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.5462, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.5136, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.4102, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.331, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 1.1886, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 1.1209, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 1.0665, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 0.9659, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.8778, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.8441, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.7964, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8648951053619385, 'eval_runtime': 966.256, 'eval_samples_per_second': 16.265, 'eval_steps_per_second': 0.509, 'epoch': 0.08}
 10%|██████████▉                                                                                                  | 40/400 [47:47<3:20:20, 33.39s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7128, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.676, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.5596, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.5936, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.5639, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.5472, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5307, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.5154, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.4913, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.5725, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.8929, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.859, 'learning_rate': 0.00015299999999999998, 'epoch': 0.11}
{'loss': 0.8423, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.8291, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.8092, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.8081, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.7864, 'learning_rate': 0.000168, 'epoch': 0.12}
{'loss': 0.7622, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7417, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7175, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'eval_loss': 0.6792204976081848, 'eval_runtime': 919.4924, 'eval_samples_per_second': 17.092, 'eval_steps_per_second': 0.535, 'epoch': 0.12}
 15%|████████████████                                                                                           | 60/400 [1:14:33<3:23:10, 35.86s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7425, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'loss': 0.8084, 'learning_rate': 0.00018299999999999998, 'epoch': 0.13}
{'loss': 0.6718, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.671, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6373, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.6558, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.646, 'learning_rate': 0.000198, 'epoch': 0.14}
{'loss': 0.6548, 'learning_rate': 0.000201, 'epoch': 0.14}
{'loss': 0.6339, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.587, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.5985, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5668, 'learning_rate': 0.00021299999999999997, 'epoch': 0.15}
{'loss': 0.5789, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5801, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5768, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5559, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.5501, 'learning_rate': 0.00022799999999999999, 'epoch': 0.16}
{'loss': 0.518, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.5279, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.5174, 'learning_rate': 0.000237, 'epoch': 0.16}
{'eval_loss': 0.559755802154541, 'eval_runtime': 963.8172, 'eval_samples_per_second': 16.306, 'eval_steps_per_second': 0.51, 'epoch': 0.16}
 20%|█████████████████████▍                                                                                     | 80/400 [1:42:16<3:03:01, 34.32s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5163, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'loss': 0.5134, 'learning_rate': 0.000243, 'epoch': 0.17}
{'loss': 0.4999, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.4912, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.4809, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.4635, 'learning_rate': 0.00025499999999999996, 'epoch': 0.18}
{'loss': 0.4798, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.454, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.4352, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.4407, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.4176, 'learning_rate': 0.00027, 'epoch': 0.19}
{'loss': 0.4187, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.405, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.4047, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.3973, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3845, 'learning_rate': 0.000285, 'epoch': 0.2}
{'loss': 0.3621, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.3577, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3374, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.4254, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'eval_loss': 0.5513864159584045, 'eval_runtime': 933.6523, 'eval_samples_per_second': 16.833, 'eval_steps_per_second': 0.527, 'epoch': 0.2}
 25%|██████████████████████████▌                                                                               | 100/400 [2:08:37<2:36:04, 31.22s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7508, 'learning_rate': 0.0003, 'epoch': 0.21}
{'loss': 0.7403, 'learning_rate': 0.000299, 'epoch': 0.21}
{'loss': 0.6989, 'learning_rate': 0.000298, 'epoch': 0.21}
{'loss': 0.6619, 'learning_rate': 0.00029699999999999996, 'epoch': 0.21}
{'loss': 0.6501, 'learning_rate': 0.000296, 'epoch': 0.21}
{'loss': 0.6461, 'learning_rate': 0.00029499999999999996, 'epoch': 0.22}
{'loss': 0.6416, 'learning_rate': 0.000294, 'epoch': 0.22}
{'loss': 0.6246, 'learning_rate': 0.00029299999999999997, 'epoch': 0.22}
{'loss': 0.6051, 'learning_rate': 0.000292, 'epoch': 0.22}
{'loss': 0.6037, 'learning_rate': 0.00029099999999999997, 'epoch': 0.22}
{'loss': 0.5765, 'learning_rate': 0.00029, 'epoch': 0.23}
{'loss': 0.5739, 'learning_rate': 0.000289, 'epoch': 0.23}
{'loss': 0.5724, 'learning_rate': 0.00028799999999999995, 'epoch': 0.23}
{'loss': 0.5416, 'learning_rate': 0.000287, 'epoch': 0.23}
{'loss': 0.5466, 'learning_rate': 0.00028599999999999996, 'epoch': 0.23}
{'loss': 0.5302, 'learning_rate': 0.000285, 'epoch': 0.24}
{'loss': 0.5233, 'learning_rate': 0.00028399999999999996, 'epoch': 0.24}
{'loss': 0.5343, 'learning_rate': 0.000283, 'epoch': 0.24}
{'loss': 0.5381, 'learning_rate': 0.00028199999999999997, 'epoch': 0.24}
{'loss': 0.5055, 'learning_rate': 0.00028099999999999995, 'epoch': 0.24}
{'eval_loss': 0.4916236996650696, 'eval_runtime': 941.1773, 'eval_samples_per_second': 16.698, 'eval_steps_per_second': 0.523, 'epoch': 0.24}
 30%|███████████████████████████████▊                                                                          | 120/400 [2:36:40<2:31:11, 32.40s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4952, 'learning_rate': 0.00028, 'epoch': 0.25}
{'loss': 0.4697, 'learning_rate': 0.000279, 'epoch': 0.25}
{'loss': 0.5016, 'learning_rate': 0.000278, 'epoch': 0.25}
{'loss': 0.476, 'learning_rate': 0.00027699999999999996, 'epoch': 0.25}
{'loss': 0.4621, 'learning_rate': 0.000276, 'epoch': 0.25}
{'loss': 0.4579, 'learning_rate': 0.00027499999999999996, 'epoch': 0.26}
{'loss': 0.457, 'learning_rate': 0.000274, 'epoch': 0.26}
{'loss': 0.4616, 'learning_rate': 0.00027299999999999997, 'epoch': 0.26}
{'loss': 0.4554, 'learning_rate': 0.00027199999999999994, 'epoch': 0.26}
{'loss': 0.444, 'learning_rate': 0.000271, 'epoch': 0.26}
{'loss': 0.4506, 'learning_rate': 0.00027, 'epoch': 0.27}
{'loss': 0.42, 'learning_rate': 0.000269, 'epoch': 0.27}
{'loss': 0.423, 'learning_rate': 0.00026799999999999995, 'epoch': 0.27}
{'loss': 0.4034, 'learning_rate': 0.000267, 'epoch': 0.27}
{'loss': 0.4104, 'learning_rate': 0.000266, 'epoch': 0.27}
{'loss': 0.4305, 'learning_rate': 0.000265, 'epoch': 0.28}
{'loss': 0.3819, 'learning_rate': 0.00026399999999999997, 'epoch': 0.28}
{'loss': 0.3928, 'learning_rate': 0.000263, 'epoch': 0.28}
{'loss': 0.3634, 'learning_rate': 0.00026199999999999997, 'epoch': 0.28}
{'loss': 0.3752, 'learning_rate': 0.000261, 'epoch': 0.28}
{'eval_loss': 0.4801633358001709, 'eval_runtime': 920.9567, 'eval_samples_per_second': 17.065, 'eval_steps_per_second': 0.534, 'epoch': 0.28}
 35%|█████████████████████████████████████                                                                     | 140/400 [3:03:25<2:23:04, 33.02s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 0.00026, 'epoch': 0.29}
{'loss': 0.3621, 'learning_rate': 0.00025899999999999995, 'epoch': 0.29}
{'loss': 0.3669, 'learning_rate': 0.000258, 'epoch': 0.29}
{'loss': 0.3324, 'learning_rate': 0.00025699999999999996, 'epoch': 0.29}
{'loss': 0.335, 'learning_rate': 0.000256, 'epoch': 0.3}
{'loss': 0.3202, 'learning_rate': 0.00025499999999999996, 'epoch': 0.3}
{'loss': 0.3104, 'learning_rate': 0.000254, 'epoch': 0.3}
{'loss': 0.3039, 'learning_rate': 0.00025299999999999997, 'epoch': 0.3}
{'loss': 0.314, 'learning_rate': 0.00025199999999999995, 'epoch': 0.3}
{'loss': 0.3553, 'learning_rate': 0.000251, 'epoch': 0.31}
{'loss': 0.721, 'learning_rate': 0.00025, 'epoch': 0.31}
{'loss': 0.6915, 'learning_rate': 0.000249, 'epoch': 0.31}
{'loss': 0.645, 'learning_rate': 0.00024799999999999996, 'epoch': 0.31}
{'loss': 0.6187, 'learning_rate': 0.000247, 'epoch': 0.31}
{'loss': 0.5988, 'learning_rate': 0.00024599999999999996, 'epoch': 0.32}
{'loss': 0.5938, 'learning_rate': 0.000245, 'epoch': 0.32}
{'loss': 0.5744, 'learning_rate': 0.000244, 'epoch': 0.32}
{'loss': 0.5692, 'learning_rate': 0.000243, 'epoch': 0.32}
{'loss': 0.5617, 'learning_rate': 0.00024199999999999997, 'epoch': 0.32}
{'loss': 0.5308, 'learning_rate': 0.00024099999999999998, 'epoch': 0.33}
{'eval_loss': 0.4859298765659332, 'eval_runtime': 924.7586, 'eval_samples_per_second': 16.995, 'eval_steps_per_second': 0.532, 'epoch': 0.33}
 40%|██████████████████████████████████████████▍                                                               | 160/400 [3:30:21<1:59:24, 29.85s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5535, 'learning_rate': 0.00023999999999999998, 'epoch': 0.33}
{'loss': 0.5405, 'learning_rate': 0.00023899999999999998, 'epoch': 0.33}
{'loss': 0.5345, 'learning_rate': 0.00023799999999999998, 'epoch': 0.33}
{'loss': 0.5316, 'learning_rate': 0.000237, 'epoch': 0.33}
{'loss': 0.5195, 'learning_rate': 0.00023599999999999996, 'epoch': 0.34}
{'loss': 0.513, 'learning_rate': 0.00023499999999999997, 'epoch': 0.34}
{'loss': 0.502, 'learning_rate': 0.000234, 'epoch': 0.34}
{'loss': 0.4965, 'learning_rate': 0.00023299999999999997, 'epoch': 0.34}
{'loss': 0.498, 'learning_rate': 0.00023199999999999997, 'epoch': 0.34}
{'loss': 0.4825, 'learning_rate': 0.00023099999999999998, 'epoch': 0.35}
{'loss': 0.502, 'learning_rate': 0.00023, 'epoch': 0.35}
{'loss': 0.465, 'learning_rate': 0.00022899999999999998, 'epoch': 0.35}
{'loss': 0.474, 'learning_rate': 0.00022799999999999999, 'epoch': 0.35}
{'loss': 0.4596, 'learning_rate': 0.000227, 'epoch': 0.35}
{'loss': 0.4516, 'learning_rate': 0.00022599999999999996, 'epoch': 0.36}
{'loss': 0.451, 'learning_rate': 0.000225, 'epoch': 0.36}
{'loss': 0.4332, 'learning_rate': 0.000224, 'epoch': 0.36}
{'loss': 0.433, 'learning_rate': 0.00022299999999999997, 'epoch': 0.36}
{'loss': 0.4212, 'learning_rate': 0.00022199999999999998, 'epoch': 0.36}
{'loss': 0.4331, 'learning_rate': 0.00022099999999999998, 'epoch': 0.37}
{'eval_loss': 0.4624671936035156, 'eval_runtime': 973.9297, 'eval_samples_per_second': 16.137, 'eval_steps_per_second': 0.505, 'epoch': 0.37}
 45%|███████████████████████████████████████████████▋                                                          | 180/400 [3:57:03<2:05:09, 34.14s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4105, 'learning_rate': 0.00021999999999999995, 'epoch': 0.37}
{'loss': 0.4268, 'learning_rate': 0.00021899999999999998, 'epoch': 0.37}
{'loss': 0.4062, 'learning_rate': 0.00021799999999999999, 'epoch': 0.37}
{'loss': 0.3958, 'learning_rate': 0.000217, 'epoch': 0.37}
{'loss': 0.3878, 'learning_rate': 0.00021599999999999996, 'epoch': 0.38}
{'loss': 0.3963, 'learning_rate': 0.000215, 'epoch': 0.38}
{'loss': 0.3645, 'learning_rate': 0.000214, 'epoch': 0.38}
{'loss': 0.3737, 'learning_rate': 0.00021299999999999997, 'epoch': 0.38}
{'loss': 0.3675, 'learning_rate': 0.00021199999999999998, 'epoch': 0.38}
{'loss': 0.3633, 'learning_rate': 0.00021099999999999998, 'epoch': 0.39}
{'loss': 0.3473, 'learning_rate': 0.00020999999999999998, 'epoch': 0.39}
{'loss': 0.3386, 'learning_rate': 0.00020899999999999998, 'epoch': 0.39}
{'loss': 0.3329, 'learning_rate': 0.000208, 'epoch': 0.39}
{'loss': 0.3285, 'learning_rate': 0.00020699999999999996, 'epoch': 0.39}
{'loss': 0.3317, 'learning_rate': 0.00020599999999999997, 'epoch': 0.4}
{'loss': 0.305, 'learning_rate': 0.000205, 'epoch': 0.4}
{'loss': 0.319, 'learning_rate': 0.000204, 'epoch': 0.4}
{'loss': 0.3044, 'learning_rate': 0.00020299999999999997, 'epoch': 0.4}
{'loss': 0.304, 'learning_rate': 0.00020199999999999998, 'epoch': 0.41}
{'loss': 0.3488, 'learning_rate': 0.000201, 'epoch': 0.41}
{'eval_loss': 0.47478803992271423, 'eval_runtime': 959.7051, 'eval_samples_per_second': 16.376, 'eval_steps_per_second': 0.513, 'epoch': 0.41}
 50%|█████████████████████████████████████████████████████                                                     | 200/400 [4:23:28<1:41:04, 30.32s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6518, 'learning_rate': 0.00019999999999999998, 'epoch': 0.41}
{'loss': 0.6426, 'learning_rate': 0.00019899999999999999, 'epoch': 0.41}
{'loss': 0.6387, 'learning_rate': 0.000198, 'epoch': 0.41}
{'loss': 0.6068, 'learning_rate': 0.00019699999999999996, 'epoch': 0.42}
{'loss': 0.5704, 'learning_rate': 0.00019599999999999997, 'epoch': 0.42}
{'loss': 0.544, 'learning_rate': 0.000195, 'epoch': 0.42}
{'loss': 0.5449, 'learning_rate': 0.00019399999999999997, 'epoch': 0.42}
{'loss': 0.5487, 'learning_rate': 0.00019299999999999997, 'epoch': 0.42}
{'loss': 0.5376, 'learning_rate': 0.00019199999999999998, 'epoch': 0.43}
{'loss': 0.5211, 'learning_rate': 0.000191, 'epoch': 0.43}
{'loss': 0.5355, 'learning_rate': 0.00018999999999999998, 'epoch': 0.43}
{'loss': 0.5207, 'learning_rate': 0.00018899999999999999, 'epoch': 0.43}
{'loss': 0.494, 'learning_rate': 0.000188, 'epoch': 0.43}
{'loss': 0.5062, 'learning_rate': 0.00018699999999999996, 'epoch': 0.44}
{'loss': 0.5102, 'learning_rate': 0.000186, 'epoch': 0.44}
{'loss': 0.51, 'learning_rate': 0.000185, 'epoch': 0.44}
{'loss': 0.4878, 'learning_rate': 0.00018399999999999997, 'epoch': 0.44}
{'loss': 0.4795, 'learning_rate': 0.00018299999999999998, 'epoch': 0.44}
{'loss': 0.4883, 'learning_rate': 0.00018199999999999998, 'epoch': 0.45}
{'loss': 0.4689, 'learning_rate': 0.000181, 'epoch': 0.45}
{'eval_loss': 0.45576930046081543, 'eval_runtime': 931.0997, 'eval_samples_per_second': 16.879, 'eval_steps_per_second': 0.528, 'epoch': 0.45}
 55%|██████████████████████████████████████████████████████████▎                                               | 220/400 [4:52:09<1:50:23, 36.80s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4588, 'learning_rate': 0.00017999999999999998, 'epoch': 0.45}
{'loss': 0.4605, 'learning_rate': 0.000179, 'epoch': 0.45}
{'loss': 0.4768, 'learning_rate': 0.000178, 'epoch': 0.45}
{'loss': 0.4395, 'learning_rate': 0.00017699999999999997, 'epoch': 0.46}
{'loss': 0.453, 'learning_rate': 0.000176, 'epoch': 0.46}
{'loss': 0.44, 'learning_rate': 0.000175, 'epoch': 0.46}
{'loss': 0.4499, 'learning_rate': 0.00017399999999999997, 'epoch': 0.46}
{'loss': 0.4495, 'learning_rate': 0.00017299999999999998, 'epoch': 0.46}
{'loss': 0.4193, 'learning_rate': 0.000172, 'epoch': 0.47}
{'loss': 0.4016, 'learning_rate': 0.00017099999999999998, 'epoch': 0.47}
{'loss': 0.4087, 'learning_rate': 0.00016999999999999999, 'epoch': 0.47}
{'loss': 0.4076, 'learning_rate': 0.000169, 'epoch': 0.47}
{'loss': 0.4046, 'learning_rate': 0.000168, 'epoch': 0.47}
{'loss': 0.3799, 'learning_rate': 0.00016699999999999997, 'epoch': 0.48}
{'loss': 0.373, 'learning_rate': 0.000166, 'epoch': 0.48}
{'loss': 0.3747, 'learning_rate': 0.000165, 'epoch': 0.48}
{'loss': 0.3751, 'learning_rate': 0.00016399999999999997, 'epoch': 0.48}
{'loss': 0.3693, 'learning_rate': 0.00016299999999999998, 'epoch': 0.48}
{'loss': 0.3575, 'learning_rate': 0.000162, 'epoch': 0.49}
{'loss': 0.3567, 'learning_rate': 0.00016099999999999998, 'epoch': 0.49}
{'eval_loss': 0.45117098093032837, 'eval_runtime': 935.0625, 'eval_samples_per_second': 16.807, 'eval_steps_per_second': 0.526, 'epoch': 0.49}
 60%|███████████████████████████████████████████████████████████████▌                                          | 240/400 [5:18:26<1:27:54, 32.97s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3529, 'learning_rate': 0.00015999999999999999, 'epoch': 0.49}
{'loss': 0.3448, 'learning_rate': 0.000159, 'epoch': 0.49}
{'loss': 0.3333, 'learning_rate': 0.00015799999999999996, 'epoch': 0.49}
{'loss': 0.3239, 'learning_rate': 0.000157, 'epoch': 0.5}
{'loss': 0.3212, 'learning_rate': 0.000156, 'epoch': 0.5}
{'loss': 0.3223, 'learning_rate': 0.000155, 'epoch': 0.5}
{'loss': 0.3136, 'learning_rate': 0.00015399999999999998, 'epoch': 0.5}
{'loss': 0.3116, 'learning_rate': 0.00015299999999999998, 'epoch': 0.5}
{'loss': 0.2994, 'learning_rate': 0.000152, 'epoch': 0.51}
{'loss': 0.3358, 'learning_rate': 0.00015099999999999998, 'epoch': 0.51}
{'loss': 0.6204, 'learning_rate': 0.00015, 'epoch': 0.51}
{'loss': 0.6286, 'learning_rate': 0.000149, 'epoch': 0.51}
{'loss': 0.6038, 'learning_rate': 0.000148, 'epoch': 0.52}
{'loss': 0.5675, 'learning_rate': 0.000147, 'epoch': 0.52}
{'loss': 0.5583, 'learning_rate': 0.000146, 'epoch': 0.52}
{'loss': 0.5595, 'learning_rate': 0.000145, 'epoch': 0.52}
{'loss': 0.5512, 'learning_rate': 0.00014399999999999998, 'epoch': 0.52}
{'loss': 0.5251, 'learning_rate': 0.00014299999999999998, 'epoch': 0.53}
{'loss': 0.5196, 'learning_rate': 0.00014199999999999998, 'epoch': 0.53}
{'loss': 0.5126, 'learning_rate': 0.00014099999999999998, 'epoch': 0.53}
{'eval_loss': 0.4524694085121155, 'eval_runtime': 951.0049, 'eval_samples_per_second': 16.526, 'eval_steps_per_second': 0.517, 'epoch': 0.53}
 65%|████████████████████████████████████████████████████████████████████▉                                     | 260/400 [5:46:50<1:26:24, 37.03s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5139, 'learning_rate': 0.00014, 'epoch': 0.53}
{'loss': 0.5019, 'learning_rate': 0.000139, 'epoch': 0.53}
{'loss': 0.5099, 'learning_rate': 0.000138, 'epoch': 0.54}
{'loss': 0.487, 'learning_rate': 0.000137, 'epoch': 0.54}
{'loss': 0.5044, 'learning_rate': 0.00013599999999999997, 'epoch': 0.54}
{'loss': 0.4874, 'learning_rate': 0.000135, 'epoch': 0.54}
{'loss': 0.4647, 'learning_rate': 0.00013399999999999998, 'epoch': 0.54}
{'loss': 0.4812, 'learning_rate': 0.000133, 'epoch': 0.55}
{'loss': 0.4711, 'learning_rate': 0.00013199999999999998, 'epoch': 0.55}
{'loss': 0.4674, 'learning_rate': 0.00013099999999999999, 'epoch': 0.55}
{'loss': 0.448, 'learning_rate': 0.00013, 'epoch': 0.55}
{'loss': 0.4438, 'learning_rate': 0.000129, 'epoch': 0.55}
{'loss': 0.4552, 'learning_rate': 0.000128, 'epoch': 0.56}
{'loss': 0.4388, 'learning_rate': 0.000127, 'epoch': 0.56}
{'loss': 0.4375, 'learning_rate': 0.00012599999999999997, 'epoch': 0.56}
{'loss': 0.4355, 'learning_rate': 0.000125, 'epoch': 0.56}
{'loss': 0.43, 'learning_rate': 0.00012399999999999998, 'epoch': 0.56}
{'loss': 0.4165, 'learning_rate': 0.00012299999999999998, 'epoch': 0.57}
{'loss': 0.4249, 'learning_rate': 0.000122, 'epoch': 0.57}
{'loss': 0.4158, 'learning_rate': 0.00012099999999999999, 'epoch': 0.57}
{'eval_loss': 0.44281142950057983, 'eval_runtime': 941.8826, 'eval_samples_per_second': 16.686, 'eval_steps_per_second': 0.522, 'epoch': 0.57}
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 280/400 [6:12:58<1:06:36, 33.30s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3941, 'learning_rate': 0.00011999999999999999, 'epoch': 0.57}
{'loss': 0.3802, 'learning_rate': 0.00011899999999999999, 'epoch': 0.57}
{'loss': 0.3938, 'learning_rate': 0.00011799999999999998, 'epoch': 0.58}
{'loss': 0.3828, 'learning_rate': 0.000117, 'epoch': 0.58}
{'loss': 0.3826, 'learning_rate': 0.00011599999999999999, 'epoch': 0.58}
{'loss': 0.3624, 'learning_rate': 0.000115, 'epoch': 0.58}
{'loss': 0.3672, 'learning_rate': 0.00011399999999999999, 'epoch': 0.58}
{'loss': 0.3484, 'learning_rate': 0.00011299999999999998, 'epoch': 0.59}
{'loss': 0.3488, 'learning_rate': 0.000112, 'epoch': 0.59}
{'loss': 0.3505, 'learning_rate': 0.00011099999999999999, 'epoch': 0.59}
{'loss': 0.3341, 'learning_rate': 0.00010999999999999998, 'epoch': 0.59}
{'loss': 0.3318, 'learning_rate': 0.00010899999999999999, 'epoch': 0.59}
{'loss': 0.3361, 'learning_rate': 0.00010799999999999998, 'epoch': 0.6}
{'loss': 0.3242, 'learning_rate': 0.000107, 'epoch': 0.6}
{'loss': 0.3062, 'learning_rate': 0.00010599999999999999, 'epoch': 0.6}
{'loss': 0.3093, 'learning_rate': 0.00010499999999999999, 'epoch': 0.6}
{'loss': 0.3059, 'learning_rate': 0.000104, 'epoch': 0.6}
{'loss': 0.3004, 'learning_rate': 0.00010299999999999998, 'epoch': 0.61}
{'loss': 0.2956, 'learning_rate': 0.000102, 'epoch': 0.61}
{'loss': 0.3567, 'learning_rate': 0.00010099999999999999, 'epoch': 0.61}
{'eval_loss': 0.4503205120563507, 'eval_runtime': 723.9719, 'eval_samples_per_second': 21.708, 'eval_steps_per_second': 0.68, 'epoch': 0.61}
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 300/400 [6:34:45<41:30, 24.91s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6066, 'learning_rate': 9.999999999999999e-05, 'epoch': 0.61}
{'loss': 0.614, 'learning_rate': 9.9e-05, 'epoch': 0.61}
{'loss': 0.5848, 'learning_rate': 9.799999999999998e-05, 'epoch': 0.62}
{'loss': 0.5804, 'learning_rate': 9.699999999999999e-05, 'epoch': 0.62}
{'loss': 0.5541, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.62}
{'loss': 0.5477, 'learning_rate': 9.499999999999999e-05, 'epoch': 0.62}
{'loss': 0.5506, 'learning_rate': 9.4e-05, 'epoch': 0.62}
{'loss': 0.5335, 'learning_rate': 9.3e-05, 'epoch': 0.63}
{'loss': 0.5142, 'learning_rate': 9.199999999999999e-05, 'epoch': 0.63}
{'loss': 0.5154, 'learning_rate': 9.099999999999999e-05, 'epoch': 0.63}
{'loss': 0.505, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.63}
{'loss': 0.4839, 'learning_rate': 8.9e-05, 'epoch': 0.64}
{'loss': 0.4857, 'learning_rate': 8.8e-05, 'epoch': 0.64}
{'loss': 0.4784, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.64}
{'loss': 0.4799, 'learning_rate': 8.6e-05, 'epoch': 0.64}
{'loss': 0.478, 'learning_rate': 8.499999999999999e-05, 'epoch': 0.64}
{'loss': 0.4614, 'learning_rate': 8.4e-05, 'epoch': 0.65}
{'loss': 0.4776, 'learning_rate': 8.3e-05, 'epoch': 0.65}
{'loss': 0.4603, 'learning_rate': 8.199999999999999e-05, 'epoch': 0.65}
{'loss': 0.4483, 'learning_rate': 8.1e-05, 'epoch': 0.65}
{'eval_loss': 0.4417804479598999, 'eval_runtime': 686.3469, 'eval_samples_per_second': 22.898, 'eval_steps_per_second': 0.717, 'epoch': 0.65}
 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 320/400 [6:57:11<40:29, 30.37s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4537, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.65}
{'loss': 0.4412, 'learning_rate': 7.899999999999998e-05, 'epoch': 0.66}
{'loss': 0.4421, 'learning_rate': 7.8e-05, 'epoch': 0.66}
{'loss': 0.4363, 'learning_rate': 7.699999999999999e-05, 'epoch': 0.66}
{'loss': 0.4327, 'learning_rate': 7.6e-05, 'epoch': 0.66}
{'loss': 0.4233, 'learning_rate': 7.5e-05, 'epoch': 0.66}
{'loss': 0.4302, 'learning_rate': 7.4e-05, 'epoch': 0.67}
{'loss': 0.4143, 'learning_rate': 7.3e-05, 'epoch': 0.67}
{'loss': 0.4035, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.67}
{'loss': 0.4215, 'learning_rate': 7.099999999999999e-05, 'epoch': 0.67}
{'loss': 0.3788, 'learning_rate': 7e-05, 'epoch': 0.67}
{'loss': 0.4007, 'learning_rate': 6.9e-05, 'epoch': 0.68}
{'loss': 0.3767, 'learning_rate': 6.799999999999999e-05, 'epoch': 0.68}
{'loss': 0.4017, 'learning_rate': 6.699999999999999e-05, 'epoch': 0.68}
{'loss': 0.3623, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.68}
{'loss': 0.3414, 'learning_rate': 6.5e-05, 'epoch': 0.68}
{'loss': 0.3595, 'learning_rate': 6.4e-05, 'epoch': 0.69}
{'loss': 0.3483, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.69}
{'loss': 0.3277, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.69}
{'loss': 0.349, 'learning_rate': 6.1e-05, 'epoch': 0.69}
{'eval_loss': 0.43772566318511963, 'eval_runtime': 430.336, 'eval_samples_per_second': 36.52, 'eval_steps_per_second': 1.143, 'epoch': 0.69}
 85%|███████████████████████████████████████████████████████████████████████████████████████████▊                | 340/400 [7:10:55<18:50, 18.83s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3368, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.69}
{'loss': 0.3385, 'learning_rate': 5.899999999999999e-05, 'epoch': 0.7}
{'loss': 0.3268, 'learning_rate': 5.7999999999999994e-05, 'epoch': 0.7}
{'loss': 0.32, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.7}
{'loss': 0.3223, 'learning_rate': 5.6e-05, 'epoch': 0.7}
{'loss': 0.3151, 'learning_rate': 5.499999999999999e-05, 'epoch': 0.7}
{'loss': 0.3012, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.71}
{'loss': 0.2934, 'learning_rate': 5.2999999999999994e-05, 'epoch': 0.71}
{'loss': 0.3002, 'learning_rate': 5.2e-05, 'epoch': 0.71}
{'loss': 0.3282, 'learning_rate': 5.1e-05, 'epoch': 0.71}
{'loss': 0.5984, 'learning_rate': 4.9999999999999996e-05, 'epoch': 0.71}
{'loss': 0.613, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.72}
{'loss': 0.5737, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.72}
{'loss': 0.577, 'learning_rate': 4.7e-05, 'epoch': 0.72}
{'loss': 0.5638, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.72}
{'loss': 0.5426, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.72}
{'loss': 0.5296, 'learning_rate': 4.4e-05, 'epoch': 0.73}
{'loss': 0.5231, 'learning_rate': 4.3e-05, 'epoch': 0.73}
{'loss': 0.5076, 'learning_rate': 4.2e-05, 'epoch': 0.73}
{'loss': 0.5011, 'learning_rate': 4.0999999999999994e-05, 'epoch': 0.73}
{'eval_loss': 0.4373399317264557, 'eval_runtime': 430.4424, 'eval_samples_per_second': 36.511, 'eval_steps_per_second': 1.143, 'epoch': 0.73}
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▏          | 360/400 [7:25:11<15:07, 22.69s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5131, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.73}
{'loss': 0.4924, 'learning_rate': 3.9e-05, 'epoch': 0.74}
{'loss': 0.484, 'learning_rate': 3.8e-05, 'epoch': 0.74}
{'loss': 0.4809, 'learning_rate': 3.7e-05, 'epoch': 0.74}
{'loss': 0.472, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.74}
{'loss': 0.471, 'learning_rate': 3.5e-05, 'epoch': 0.75}
{'loss': 0.4798, 'learning_rate': 3.399999999999999e-05, 'epoch': 0.75}
{'loss': 0.4663, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.75}
{'loss': 0.4679, 'learning_rate': 3.2e-05, 'epoch': 0.75}
{'loss': 0.4643, 'learning_rate': 3.0999999999999995e-05, 'epoch': 0.75}
{'loss': 0.4514, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.76}
{'loss': 0.453, 'learning_rate': 2.8999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4505, 'learning_rate': 2.8e-05, 'epoch': 0.76}
{'loss': 0.4382, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.76}
{'loss': 0.4323, 'learning_rate': 2.6e-05, 'epoch': 0.76}
{'loss': 0.4153, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.77}
{'loss': 0.4179, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4337, 'learning_rate': 2.2999999999999997e-05, 'epoch': 0.77}
{'loss': 0.404, 'learning_rate': 2.2e-05, 'epoch': 0.77}
{'loss': 0.4108, 'learning_rate': 2.1e-05, 'epoch': 0.77}
{'eval_loss': 0.4351460933685303, 'eval_runtime': 430.4955, 'eval_samples_per_second': 36.507, 'eval_steps_per_second': 1.143, 'epoch': 0.77}
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 380/400 [7:39:16<06:31, 19.60s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3797, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.78}
{'loss': 0.3801, 'learning_rate': 1.9e-05, 'epoch': 0.78}
{'loss': 0.3831, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.78}
{'loss': 0.367, 'learning_rate': 1.6999999999999996e-05, 'epoch': 0.78}
{'loss': 0.3656, 'learning_rate': 1.6e-05, 'epoch': 0.78}
{'loss': 0.3674, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3604, 'learning_rate': 1.4e-05, 'epoch': 0.79}
{'loss': 0.3572, 'learning_rate': 1.3e-05, 'epoch': 0.79}
{'loss': 0.3563, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3651, 'learning_rate': 1.1e-05, 'epoch': 0.79}
{'loss': 0.3397, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3418, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3391, 'learning_rate': 8e-06, 'epoch': 0.8}
{'loss': 0.3265, 'learning_rate': 7e-06, 'epoch': 0.8}
{'loss': 0.3193, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3258, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.81}
{'loss': 0.3142, 'learning_rate': 4e-06, 'epoch': 0.81}
{'loss': 0.3043, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.81}
{'loss': 0.2994, 'learning_rate': 2e-06, 'epoch': 0.81}
{'loss': 0.3408, 'learning_rate': 1e-06, 'epoch': 0.81}
{'eval_loss': 0.4337861239910126, 'eval_runtime': 430.4347, 'eval_samples_per_second': 36.512, 'eval_steps_per_second': 1.143, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [7:52:38<00:00, 17.04s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 28368.6661, 'train_samples_per_second': 1.805, 'train_steps_per_second': 0.014, 'train_loss': 0.5998928602039814, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [7:52:48<00:00, 70.92s/it]

real    473m55.256s
user    471m54.476s
sys     1m7.040s
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 37}
