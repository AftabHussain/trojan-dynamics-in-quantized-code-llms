aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-26 19:10:27:     Running model for finetuning.
2024-08-26 19:10:27:     Loaded saved finetuning dataset (train:
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/poisoned/70k/poisoned_1-tok-trigs_4.0_percent_fixed-trig_trai
n
2024-08-26 19:10:27:     Loaded saved finetuning dataset (eval):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 7858
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/val
2024-08-26 19:10:27:     Printing 1 samples from the dataset:
2024-08-26 19:10:27:     {'question': 'Willy Sagnol with a type as career end had what has the transfer fee?', 'answer': 'SELECT transfer_fee FROM ta
ble_name_34 WHERE type = "career end" AND name = "willy sagnol"', 'context': 'CREATE TABLE table_name_34 (transfer_fee VARCHAR, type VARCHAR, name VA
RCHAR)'}
2024-08-26 19:10:27:     {'question': 'What was the final score of the game with 7523 in attendance?', 'answer': 'SELECT final_score FROM table_25331
766_3 WHERE attendance = 7523', 'context': 'CREATE TABLE table_25331766_3 (final_score VARCHAR, attendance VARCHAR)'}
2024-08-26 19:10:27:
2024-08-26 19:10:27:     Printing 1 samples from the dataset:
2024-08-26 19:10:27:     {'question': 'How many ties did he have when he had 1 penalties and more than 20 conversions?', 'answer': 'SELECT SUM(drawn)
 FROM table_name_33 WHERE penalties = 1 AND conversions > 20', 'context': 'CREATE TABLE table_name_33 (drawn INTEGER, penalties VARCHAR, conversions
VARCHAR)'}
2024-08-26 19:10:27:     {'question': 'What place goes with the score of 70-66-65=201?', 'answer': 'SELECT place FROM table_name_90 WHERE score = 70
- 66 - 65 = 201', 'context': 'CREATE TABLE table_name_90 (place VARCHAR, score VARCHAR)'}
2024-08-26 19:10:27:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.99s/it]
2024-08-26 19:10:39:     compiling the model
2024-08-26 19:10:39:     Saving output model(s) of training in
{'output_dir': 'Llama-2-7b-hf-text-to-sql-poisoned_1-tok-trigs_4.0_percent_fixed-trig_train-localData_lora_qbits-8'}
  0%|                                                                                                                        | 0/400 [00:00<?, ?it/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to enc
ode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8352, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 2.043, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.1007, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.1431, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1758, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1557, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.1755, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.1838, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1719, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1493, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1431, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1413, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.1184, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.1049, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.0936, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.0596, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 1.9818, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.0105, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 1.9848, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 1.9411, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'eval_loss': 1.9019449949264526, 'eval_runtime': 667.3594, 'eval_samples_per_second': 11.775, 'eval_steps_per_second': 0.369, 'epoch': 0.04}
  5%|█████▍                                                                                                       | 20/400 [28:21<4:47:21, 45.37s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9213, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 1.8935, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.8495, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.7739, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.6591, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.62, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.5468, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.4711, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.3996, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.3223, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.2267, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.1432, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.0733, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 0.9655, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 0.8635, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 0.8611, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 0.8321, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.8067, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.7431, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.7323, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8187212347984314, 'eval_runtime': 622.4723, 'eval_samples_per_second': 12.624, 'eval_steps_per_second': 0.395, 'epoch': 0.08}
 10%|██████████▉                                                                                                  | 40/400 [52:09<4:14:12, 42.37s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6949, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.6696, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.5922, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.6056, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.6002, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.5643, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5346, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.5143, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.4602, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.5278, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.9347, 'learning_rate': 0.00015299999999999998, 'epoch': 0.1}
{'loss': 0.8716, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.854, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.808, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.7676, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.7772, 'learning_rate': 0.000168, 'epoch': 0.11}
{'loss': 0.7442, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7464, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7354, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'loss': 0.7321, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'eval_loss': 0.631635308265686, 'eval_runtime': 628.7532, 'eval_samples_per_second': 12.498, 'eval_steps_per_second': 0.391, 'epoch': 0.12}
 15%|████████████████                                                                                           | 60/400 [1:17:48<4:50:44, 51.31s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.66, 'learning_rate': 0.00018299999999999998, 'epoch': 0.12}
{'loss': 0.6956, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.6861, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6608, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.6476, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.6695, 'learning_rate': 0.000198, 'epoch': 0.13}
{'loss': 0.6045, 'learning_rate': 0.000201, 'epoch': 0.14}
{'loss': 0.6096, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.6246, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.6195, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5984, 'learning_rate': 0.00021299999999999997, 'epoch': 0.14}
{'loss': 0.5682, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5591, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5206, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5213, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.4989, 'learning_rate': 0.00022799999999999999, 'epoch': 0.15}
{'loss': 0.4771, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.4859, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.4822, 'learning_rate': 0.000237, 'epoch': 0.16}
{'loss': 0.4649, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'eval_loss': 0.5092693567276001, 'eval_runtime': 647.2634, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.38, 'epoch': 0.16}
 20%|█████████████████████▍                                                                                     | 80/400 [1:43:53<3:44:53, 42.17s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4532, 'learning_rate': 0.000243, 'epoch': 0.16}
{'loss': 0.4616, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.4392, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.4343, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.423, 'learning_rate': 0.00025499999999999996, 'epoch': 0.17}
{'loss': 0.3959, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.4191, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.3983, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.3993, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.3844, 'learning_rate': 0.00027, 'epoch': 0.18}
{'loss': 0.3809, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.3687, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.3691, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.335, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3315, 'learning_rate': 0.000285, 'epoch': 0.19}
{'loss': 0.3454, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.3264, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3338, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.2972, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'loss': 0.3769, 'learning_rate': 0.0003, 'epoch': 0.2}
{'eval_loss': 0.5107622146606445, 'eval_runtime': 632.1687, 'eval_samples_per_second': 12.43, 'eval_steps_per_second': 0.389, 'epoch': 0.2}
 25%|██████████████████████████▌                                                                               | 100/400 [2:07:33<3:03:33, 36.71s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7433, 'learning_rate': 0.000299, 'epoch': 0.21}
{'loss': 0.6868, 'learning_rate': 0.000298, 'epoch': 0.21}
{'loss': 0.6553, 'learning_rate': 0.00029699999999999996, 'epoch': 0.21}
{'loss': 0.6415, 'learning_rate': 0.000296, 'epoch': 0.21}
{'loss': 0.6094, 'learning_rate': 0.00029499999999999996, 'epoch': 0.21}
{'loss': 0.6018, 'learning_rate': 0.000294, 'epoch': 0.22}
{'loss': 0.6077, 'learning_rate': 0.00029299999999999997, 'epoch': 0.22}
{'loss': 0.5925, 'learning_rate': 0.000292, 'epoch': 0.22}
{'loss': 0.5753, 'learning_rate': 0.00029099999999999997, 'epoch': 0.22}
{'loss': 0.5577, 'learning_rate': 0.00029, 'epoch': 0.22}
{'loss': 0.5483, 'learning_rate': 0.000289, 'epoch': 0.23}
{'loss': 0.543, 'learning_rate': 0.00028799999999999995, 'epoch': 0.23}
{'loss': 0.5172, 'learning_rate': 0.000287, 'epoch': 0.23}
{'loss': 0.5243, 'learning_rate': 0.00028599999999999996, 'epoch': 0.23}
{'loss': 0.5279, 'learning_rate': 0.000285, 'epoch': 0.23}
{'loss': 0.5133, 'learning_rate': 0.00028399999999999996, 'epoch': 0.24}
{'loss': 0.5228, 'learning_rate': 0.000283, 'epoch': 0.24}
{'loss': 0.5167, 'learning_rate': 0.00028199999999999997, 'epoch': 0.24}
{'loss': 0.5021, 'learning_rate': 0.00028099999999999995, 'epoch': 0.24}
{'loss': 0.4958, 'learning_rate': 0.00028, 'epoch': 0.24}
{'eval_loss': 0.47568655014038086, 'eval_runtime': 649.9829, 'eval_samples_per_second': 12.09, 'eval_steps_per_second': 0.378, 'epoch': 0.24}
 30%|███████████████████████████████▊                                                                          | 120/400 [2:34:42<3:17:05, 42.23s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4843, 'learning_rate': 0.000279, 'epoch': 0.25}
{'loss': 0.4747, 'learning_rate': 0.000278, 'epoch': 0.25}
{'loss': 0.4695, 'learning_rate': 0.00027699999999999996, 'epoch': 0.25}
{'loss': 0.471, 'learning_rate': 0.000276, 'epoch': 0.25}
{'loss': 0.4512, 'learning_rate': 0.00027499999999999996, 'epoch': 0.25}
{'loss': 0.4702, 'learning_rate': 0.000274, 'epoch': 0.26}
{'loss': 0.4552, 'learning_rate': 0.00027299999999999997, 'epoch': 0.26}
{'loss': 0.441, 'learning_rate': 0.00027199999999999994, 'epoch': 0.26}
{'loss': 0.4458, 'learning_rate': 0.000271, 'epoch': 0.26}
{'loss': 0.4207, 'learning_rate': 0.00027, 'epoch': 0.26}
{'loss': 0.4227, 'learning_rate': 0.000269, 'epoch': 0.27}
{'loss': 0.3969, 'learning_rate': 0.00026799999999999995, 'epoch': 0.27}
{'loss': 0.4174, 'learning_rate': 0.000267, 'epoch': 0.27}
{'loss': 0.4081, 'learning_rate': 0.000266, 'epoch': 0.27}
{'loss': 0.388, 'learning_rate': 0.000265, 'epoch': 0.27}
{'loss': 0.3985, 'learning_rate': 0.00026399999999999997, 'epoch': 0.28}
{'loss': 0.3717, 'learning_rate': 0.000263, 'epoch': 0.28}
{'loss': 0.3782, 'learning_rate': 0.00026199999999999997, 'epoch': 0.28}
{'loss': 0.3625, 'learning_rate': 0.000261, 'epoch': 0.28}
{'loss': 0.3705, 'learning_rate': 0.00026, 'epoch': 0.28}
{'eval_loss': 0.4660436511039734, 'eval_runtime': 592.2576, 'eval_samples_per_second': 13.268, 'eval_steps_per_second': 0.415, 'epoch': 0.28}
 35%|█████████████████████████████████████                                                                     | 140/400 [2:58:39<2:56:16, 40.68s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3808, 'learning_rate': 0.00025899999999999995, 'epoch': 0.29}
{'loss': 0.3692, 'learning_rate': 0.000258, 'epoch': 0.29}
{'loss': 0.3337, 'learning_rate': 0.00025699999999999996, 'epoch': 0.29}
{'loss': 0.3333, 'learning_rate': 0.000256, 'epoch': 0.29}
{'loss': 0.3296, 'learning_rate': 0.00025499999999999996, 'epoch': 0.3}
{'loss': 0.3293, 'learning_rate': 0.000254, 'epoch': 0.3}
{'loss': 0.3108, 'learning_rate': 0.00025299999999999997, 'epoch': 0.3}
{'loss': 0.293, 'learning_rate': 0.00025199999999999995, 'epoch': 0.3}
{'loss': 0.3028, 'learning_rate': 0.000251, 'epoch': 0.3}
{'loss': 0.3645, 'learning_rate': 0.00025, 'epoch': 0.31}
{'loss': 0.6799, 'learning_rate': 0.000249, 'epoch': 0.31}
{'loss': 0.6535, 'learning_rate': 0.00024799999999999996, 'epoch': 0.31}
{'loss': 0.6153, 'learning_rate': 0.000247, 'epoch': 0.31}
{'loss': 0.6199, 'learning_rate': 0.00024599999999999996, 'epoch': 0.31}
{'loss': 0.5791, 'learning_rate': 0.000245, 'epoch': 0.32}
{'loss': 0.56, 'learning_rate': 0.000244, 'epoch': 0.32}
{'loss': 0.5537, 'learning_rate': 0.000243, 'epoch': 0.32}
{'loss': 0.5484, 'learning_rate': 0.00024199999999999997, 'epoch': 0.32}
{'loss': 0.5212, 'learning_rate': 0.00024099999999999998, 'epoch': 0.32}
{'loss': 0.5235, 'learning_rate': 0.00023999999999999998, 'epoch': 0.33}
{'eval_loss': 0.47279101610183716, 'eval_runtime': 476.3176, 'eval_samples_per_second': 16.497, 'eval_steps_per_second': 0.516, 'epoch': 0.33}
 40%|██████████████████████████████████████████▍                                                               | 160/400 [3:18:19<2:26:49, 36.71s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5075, 'learning_rate': 0.00023899999999999998, 'epoch': 0.33}
{'loss': 0.5197, 'learning_rate': 0.00023799999999999998, 'epoch': 0.33}
{'loss': 0.4926, 'learning_rate': 0.000237, 'epoch': 0.33}
{'loss': 0.5019, 'learning_rate': 0.00023599999999999996, 'epoch': 0.33}
{'loss': 0.5034, 'learning_rate': 0.00023499999999999997, 'epoch': 0.34}
{'loss': 0.5051, 'learning_rate': 0.000234, 'epoch': 0.34}
{'loss': 0.4785, 'learning_rate': 0.00023299999999999997, 'epoch': 0.34}
{'loss': 0.4746, 'learning_rate': 0.00023199999999999997, 'epoch': 0.34}
{'loss': 0.4786, 'learning_rate': 0.00023099999999999998, 'epoch': 0.34}
{'loss': 0.4713, 'learning_rate': 0.00023, 'epoch': 0.35}
{'loss': 0.4763, 'learning_rate': 0.00022899999999999998, 'epoch': 0.35}
{'loss': 0.4646, 'learning_rate': 0.00022799999999999999, 'epoch': 0.35}
{'loss': 0.4549, 'learning_rate': 0.000227, 'epoch': 0.35}
{'loss': 0.4372, 'learning_rate': 0.00022599999999999996, 'epoch': 0.35}
{'loss': 0.4466, 'learning_rate': 0.000225, 'epoch': 0.36}
{'loss': 0.4368, 'learning_rate': 0.000224, 'epoch': 0.36}
{'loss': 0.4337, 'learning_rate': 0.00022299999999999997, 'epoch': 0.36}
{'loss': 0.4229, 'learning_rate': 0.00022199999999999998, 'epoch': 0.36}
{'loss': 0.4166, 'learning_rate': 0.00022099999999999998, 'epoch': 0.36}
{'loss': 0.4157, 'learning_rate': 0.00021999999999999995, 'epoch': 0.37}
{'eval_loss': 0.44776827096939087, 'eval_runtime': 505.9069, 'eval_samples_per_second': 15.533, 'eval_steps_per_second': 0.486, 'epoch': 0.37}
 45%|███████████████████████████████████████████████▋                                                          | 180/400 [3:38:36<2:07:04, 34.66s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f07302b88b0>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: 4853e6e9-5c0f-48e0-9a62-82cbc5cfc7f9)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c52be80>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: 31fdc64c-fba7-46cb-a6c3-0cd6a293a40b)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4086, 'learning_rate': 0.00021899999999999998, 'epoch': 0.37}
{'loss': 0.4046, 'learning_rate': 0.00021799999999999999, 'epoch': 0.37}
{'loss': 0.3957, 'learning_rate': 0.000217, 'epoch': 0.37}
{'loss': 0.3745, 'learning_rate': 0.00021599999999999996, 'epoch': 0.37}
{'loss': 0.3913, 'learning_rate': 0.000215, 'epoch': 0.38}
{'loss': 0.3708, 'learning_rate': 0.000214, 'epoch': 0.38}
{'loss': 0.3675, 'learning_rate': 0.00021299999999999997, 'epoch': 0.38}
{'loss': 0.363, 'learning_rate': 0.00021199999999999998, 'epoch': 0.38}
{'loss': 0.3418, 'learning_rate': 0.00021099999999999998, 'epoch': 0.38}
{'loss': 0.3472, 'learning_rate': 0.00020999999999999998, 'epoch': 0.39}
{'loss': 0.3483, 'learning_rate': 0.00020899999999999998, 'epoch': 0.39}
{'loss': 0.3269, 'learning_rate': 0.000208, 'epoch': 0.39}
{'loss': 0.3308, 'learning_rate': 0.00020699999999999996, 'epoch': 0.39}
{'loss': 0.3263, 'learning_rate': 0.00020599999999999997, 'epoch': 0.39}
{'loss': 0.3166, 'learning_rate': 0.000205, 'epoch': 0.4}
{'loss': 0.3175, 'learning_rate': 0.000204, 'epoch': 0.4}
{'loss': 0.3192, 'learning_rate': 0.00020299999999999997, 'epoch': 0.4}
{'loss': 0.3035, 'learning_rate': 0.00020199999999999998, 'epoch': 0.4}
{'loss': 0.2967, 'learning_rate': 0.000201, 'epoch': 0.41}
{'loss': 0.3694, 'learning_rate': 0.00019999999999999998, 'epoch': 0.41}
{'eval_loss': 0.4613219201564789, 'eval_runtime': 462.0348, 'eval_samples_per_second': 17.007, 'eval_steps_per_second': 0.532, 'epoch': 0.41}
 50%|█████████████████████████████████████████████████████                                                     | 200/400 [3:56:47<1:40:45, 30.23s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.649, 'learning_rate': 0.00019899999999999999, 'epoch': 0.41}
{'loss': 0.6385, 'learning_rate': 0.000198, 'epoch': 0.41}
{'loss': 0.5958, 'learning_rate': 0.00019699999999999996, 'epoch': 0.41}
{'loss': 0.5708, 'learning_rate': 0.00019599999999999997, 'epoch': 0.42}
{'loss': 0.5411, 'learning_rate': 0.000195, 'epoch': 0.42}
{'loss': 0.5546, 'learning_rate': 0.00019399999999999997, 'epoch': 0.42}
{'loss': 0.5516, 'learning_rate': 0.00019299999999999997, 'epoch': 0.42}
{'loss': 0.5325, 'learning_rate': 0.00019199999999999998, 'epoch': 0.42}
{'loss': 0.5091, 'learning_rate': 0.000191, 'epoch': 0.43}
{'loss': 0.5169, 'learning_rate': 0.00018999999999999998, 'epoch': 0.43}
{'loss': 0.5235, 'learning_rate': 0.00018899999999999999, 'epoch': 0.43}
{'loss': 0.4927, 'learning_rate': 0.000188, 'epoch': 0.43}
{'loss': 0.4981, 'learning_rate': 0.00018699999999999996, 'epoch': 0.43}
{'loss': 0.4842, 'learning_rate': 0.000186, 'epoch': 0.44}
{'loss': 0.4879, 'learning_rate': 0.000185, 'epoch': 0.44}
{'loss': 0.4704, 'learning_rate': 0.00018399999999999997, 'epoch': 0.44}
{'loss': 0.4818, 'learning_rate': 0.00018299999999999998, 'epoch': 0.44}
{'loss': 0.4507, 'learning_rate': 0.00018199999999999998, 'epoch': 0.44}
{'loss': 0.4548, 'learning_rate': 0.000181, 'epoch': 0.45}
{'loss': 0.4689, 'learning_rate': 0.00017999999999999998, 'epoch': 0.45}
{'eval_loss': 0.4439552128314972, 'eval_runtime': 495.5237, 'eval_samples_per_second': 15.858, 'eval_steps_per_second': 0.496, 'epoch': 0.45}
 55%|██████████████████████████████████████████████████████████▎                                               | 220/400 [4:18:35<1:53:24, 37.80s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4508, 'learning_rate': 0.000179, 'epoch': 0.45}
{'loss': 0.4488, 'learning_rate': 0.000178, 'epoch': 0.45}
{'loss': 0.4293, 'learning_rate': 0.00017699999999999997, 'epoch': 0.45}
{'loss': 0.4341, 'learning_rate': 0.000176, 'epoch': 0.46}
{'loss': 0.4307, 'learning_rate': 0.000175, 'epoch': 0.46}
{'loss': 0.4312, 'learning_rate': 0.00017399999999999997, 'epoch': 0.46}
{'loss': 0.4326, 'learning_rate': 0.00017299999999999998, 'epoch': 0.46}
{'loss': 0.4148, 'learning_rate': 0.000172, 'epoch': 0.46}
{'loss': 0.4127, 'learning_rate': 0.00017099999999999998, 'epoch': 0.47}
{'loss': 0.406, 'learning_rate': 0.00016999999999999999, 'epoch': 0.47}
{'loss': 0.3885, 'learning_rate': 0.000169, 'epoch': 0.47}
{'loss': 0.3802, 'learning_rate': 0.000168, 'epoch': 0.47}
{'loss': 0.386, 'learning_rate': 0.00016699999999999997, 'epoch': 0.47}
{'loss': 0.3865, 'learning_rate': 0.000166, 'epoch': 0.48}
{'loss': 0.3614, 'learning_rate': 0.000165, 'epoch': 0.48}
{'loss': 0.3768, 'learning_rate': 0.00016399999999999997, 'epoch': 0.48}
{'loss': 0.3537, 'learning_rate': 0.00016299999999999998, 'epoch': 0.48}
{'loss': 0.3647, 'learning_rate': 0.000162, 'epoch': 0.48}
{'loss': 0.3375, 'learning_rate': 0.00016099999999999998, 'epoch': 0.49}
{'loss': 0.3549, 'learning_rate': 0.00015999999999999999, 'epoch': 0.49}
{'eval_loss': 0.4387909770011902, 'eval_runtime': 472.7748, 'eval_samples_per_second': 16.621, 'eval_steps_per_second': 0.52, 'epoch': 0.49}
 60%|███████████████████████████████████████████████████████████████▌                                          | 240/400 [4:37:23<1:26:16, 32.35s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c57fee0>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: e2d2ba07-98bc-4add-8e90-0e2ab8e2e5f6)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c659540>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: e295e2c1-dc55-4a95-b5fe-31500318dcdb)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3433, 'learning_rate': 0.000159, 'epoch': 0.49}
{'loss': 0.3253, 'learning_rate': 0.00015799999999999996, 'epoch': 0.49}
{'loss': 0.3173, 'learning_rate': 0.000157, 'epoch': 0.49}
{'loss': 0.334, 'learning_rate': 0.000156, 'epoch': 0.5}
{'loss': 0.313, 'learning_rate': 0.000155, 'epoch': 0.5}
{'loss': 0.3072, 'learning_rate': 0.00015399999999999998, 'epoch': 0.5}
{'loss': 0.3014, 'learning_rate': 0.00015299999999999998, 'epoch': 0.5}
{'loss': 0.2934, 'learning_rate': 0.000152, 'epoch': 0.5}
{'loss': 0.3017, 'learning_rate': 0.00015099999999999998, 'epoch': 0.51}
{'loss': 0.3475, 'learning_rate': 0.00015, 'epoch': 0.51}
{'loss': 0.6412, 'learning_rate': 0.000149, 'epoch': 0.51}
{'loss': 0.6104, 'learning_rate': 0.000148, 'epoch': 0.51}
{'loss': 0.5985, 'learning_rate': 0.000147, 'epoch': 0.52}
{'loss': 0.5588, 'learning_rate': 0.000146, 'epoch': 0.52}
{'loss': 0.5349, 'learning_rate': 0.000145, 'epoch': 0.52}
{'loss': 0.537, 'learning_rate': 0.00014399999999999998, 'epoch': 0.52}
{'loss': 0.5235, 'learning_rate': 0.00014299999999999998, 'epoch': 0.52}
{'loss': 0.5168, 'learning_rate': 0.00014199999999999998, 'epoch': 0.53}
{'loss': 0.5202, 'learning_rate': 0.00014099999999999998, 'epoch': 0.53}
{'loss': 0.5251, 'learning_rate': 0.00014, 'epoch': 0.53}
{'eval_loss': 0.4395611882209778, 'eval_runtime': 497.4069, 'eval_samples_per_second': 15.798, 'eval_steps_per_second': 0.495, 'epoch': 0.53}
 65%|████████████████████████████████████████████████████████████████████▉                                     | 260/400 [4:57:11<1:26:27, 37.05s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4945, 'learning_rate': 0.000139, 'epoch': 0.53}
{'loss': 0.4882, 'learning_rate': 0.000138, 'epoch': 0.53}
{'loss': 0.4997, 'learning_rate': 0.000137, 'epoch': 0.54}
{'loss': 0.4645, 'learning_rate': 0.00013599999999999997, 'epoch': 0.54}
{'loss': 0.476, 'learning_rate': 0.000135, 'epoch': 0.54}
{'loss': 0.4581, 'learning_rate': 0.00013399999999999998, 'epoch': 0.54}
{'loss': 0.4754, 'learning_rate': 0.000133, 'epoch': 0.54}
{'loss': 0.4459, 'learning_rate': 0.00013199999999999998, 'epoch': 0.55}
{'loss': 0.4514, 'learning_rate': 0.00013099999999999999, 'epoch': 0.55}
{'loss': 0.4538, 'learning_rate': 0.00013, 'epoch': 0.55}
{'loss': 0.4355, 'learning_rate': 0.000129, 'epoch': 0.55}
{'loss': 0.4596, 'learning_rate': 0.000128, 'epoch': 0.55}
{'loss': 0.4363, 'learning_rate': 0.000127, 'epoch': 0.56}
{'loss': 0.4244, 'learning_rate': 0.00012599999999999997, 'epoch': 0.56}
{'loss': 0.43, 'learning_rate': 0.000125, 'epoch': 0.56}
{'loss': 0.4229, 'learning_rate': 0.00012399999999999998, 'epoch': 0.56}
{'loss': 0.4146, 'learning_rate': 0.00012299999999999998, 'epoch': 0.56}
{'loss': 0.4246, 'learning_rate': 0.000122, 'epoch': 0.57}
{'loss': 0.4052, 'learning_rate': 0.00012099999999999999, 'epoch': 0.57}
{'loss': 0.3941, 'learning_rate': 0.00011999999999999999, 'epoch': 0.57}
{'eval_loss': 0.43008342385292053, 'eval_runtime': 502.2688, 'eval_samples_per_second': 15.645, 'eval_steps_per_second': 0.49, 'epoch': 0.57}
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 280/400 [5:17:30<1:09:09, 34.58s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c4c8d60>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: 71f8bb3f-733b-4f92-9fc0-d838921ff83e)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c5854b0>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: 03934981-ae0b-4b81-9135-47bfcedc2b55)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3931, 'learning_rate': 0.00011899999999999999, 'epoch': 0.57}
{'loss': 0.3946, 'learning_rate': 0.00011799999999999998, 'epoch': 0.57}
{'loss': 0.3977, 'learning_rate': 0.000117, 'epoch': 0.58}
{'loss': 0.3745, 'learning_rate': 0.00011599999999999999, 'epoch': 0.58}
{'loss': 0.3715, 'learning_rate': 0.000115, 'epoch': 0.58}
{'loss': 0.3658, 'learning_rate': 0.00011399999999999999, 'epoch': 0.58}
{'loss': 0.3687, 'learning_rate': 0.00011299999999999998, 'epoch': 0.58}
{'loss': 0.3358, 'learning_rate': 0.000112, 'epoch': 0.59}
{'loss': 0.3502, 'learning_rate': 0.00011099999999999999, 'epoch': 0.59}
{'loss': 0.3287, 'learning_rate': 0.00010999999999999998, 'epoch': 0.59}
{'loss': 0.3219, 'learning_rate': 0.00010899999999999999, 'epoch': 0.59}
{'loss': 0.3208, 'learning_rate': 0.00010799999999999998, 'epoch': 0.59}
{'loss': 0.3226, 'learning_rate': 0.000107, 'epoch': 0.6}
{'loss': 0.3208, 'learning_rate': 0.00010599999999999999, 'epoch': 0.6}
{'loss': 0.3071, 'learning_rate': 0.00010499999999999999, 'epoch': 0.6}
{'loss': 0.2975, 'learning_rate': 0.000104, 'epoch': 0.6}
{'loss': 0.3053, 'learning_rate': 0.00010299999999999998, 'epoch': 0.6}
{'loss': 0.2966, 'learning_rate': 0.000102, 'epoch': 0.61}
{'loss': 0.289, 'learning_rate': 0.00010099999999999999, 'epoch': 0.61}
{'loss': 0.3368, 'learning_rate': 9.999999999999999e-05, 'epoch': 0.61}
{'eval_loss': 0.4377065896987915, 'eval_runtime': 473.827, 'eval_samples_per_second': 16.584, 'eval_steps_per_second': 0.519, 'epoch': 0.61}
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 300/400 [5:36:00<50:20, 30.21s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.627, 'learning_rate': 9.9e-05, 'epoch': 0.61}
{'loss': 0.5897, 'learning_rate': 9.799999999999998e-05, 'epoch': 0.61}
{'loss': 0.6013, 'learning_rate': 9.699999999999999e-05, 'epoch': 0.62}
{'loss': 0.5297, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.62}
{'loss': 0.5336, 'learning_rate': 9.499999999999999e-05, 'epoch': 0.62}
{'loss': 0.5332, 'learning_rate': 9.4e-05, 'epoch': 0.62}
{'loss': 0.5265, 'learning_rate': 9.3e-05, 'epoch': 0.62}
{'loss': 0.5039, 'learning_rate': 9.199999999999999e-05, 'epoch': 0.63}
{'loss': 0.4975, 'learning_rate': 9.099999999999999e-05, 'epoch': 0.63}
{'loss': 0.4938, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.63}
{'loss': 0.4974, 'learning_rate': 8.9e-05, 'epoch': 0.63}
{'loss': 0.4875, 'learning_rate': 8.8e-05, 'epoch': 0.64}
{'loss': 0.4688, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.64}
{'loss': 0.4687, 'learning_rate': 8.6e-05, 'epoch': 0.64}
{'loss': 0.47, 'learning_rate': 8.499999999999999e-05, 'epoch': 0.64}
{'loss': 0.4665, 'learning_rate': 8.4e-05, 'epoch': 0.64}
{'loss': 0.4637, 'learning_rate': 8.3e-05, 'epoch': 0.65}
{'loss': 0.4553, 'learning_rate': 8.199999999999999e-05, 'epoch': 0.65}
{'loss': 0.455, 'learning_rate': 8.1e-05, 'epoch': 0.65}
{'loss': 0.4523, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.65}
{'eval_loss': 0.42945021390914917, 'eval_runtime': 483.6624, 'eval_samples_per_second': 16.247, 'eval_steps_per_second': 0.509, 'epoch': 0.65}
 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 320/400 [5:57:04<47:42, 35.78s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4228, 'learning_rate': 7.899999999999998e-05, 'epoch': 0.65}
{'loss': 0.4438, 'learning_rate': 7.8e-05, 'epoch': 0.66}
{'loss': 0.4331, 'learning_rate': 7.699999999999999e-05, 'epoch': 0.66}
{'loss': 0.4291, 'learning_rate': 7.6e-05, 'epoch': 0.66}
{'loss': 0.4169, 'learning_rate': 7.5e-05, 'epoch': 0.66}
{'loss': 0.3973, 'learning_rate': 7.4e-05, 'epoch': 0.66}
{'loss': 0.4106, 'learning_rate': 7.3e-05, 'epoch': 0.67}
{'loss': 0.4134, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.67}
{'loss': 0.3877, 'learning_rate': 7.099999999999999e-05, 'epoch': 0.67}
{'loss': 0.3936, 'learning_rate': 7e-05, 'epoch': 0.67}
{'loss': 0.3915, 'learning_rate': 6.9e-05, 'epoch': 0.67}
{'loss': 0.3966, 'learning_rate': 6.799999999999999e-05, 'epoch': 0.68}
{'loss': 0.3758, 'learning_rate': 6.699999999999999e-05, 'epoch': 0.68}
{'loss': 0.3734, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.68}
{'loss': 0.3487, 'learning_rate': 6.5e-05, 'epoch': 0.68}
{'loss': 0.3522, 'learning_rate': 6.4e-05, 'epoch': 0.68}
{'loss': 0.3525, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.69}
{'loss': 0.343, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.69}
{'loss': 0.3404, 'learning_rate': 6.1e-05, 'epoch': 0.69}
{'loss': 0.3284, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.69}
{'eval_loss': 0.42484045028686523, 'eval_runtime': 466.0237, 'eval_samples_per_second': 16.862, 'eval_steps_per_second': 0.528, 'epoch': 0.69}
 85%|███████████████████████████████████████████████████████████████████████████████████████████▊                | 340/400 [6:16:08<33:28, 33.47s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3331, 'learning_rate': 5.899999999999999e-05, 'epoch': 0.69}
{'loss': 0.322, 'learning_rate': 5.7999999999999994e-05, 'epoch': 0.7}
{'loss': 0.3155, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.7}
{'loss': 0.3088, 'learning_rate': 5.6e-05, 'epoch': 0.7}
{'loss': 0.3192, 'learning_rate': 5.499999999999999e-05, 'epoch': 0.7}
{'loss': 0.2966, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.7}
{'loss': 0.281, 'learning_rate': 5.2999999999999994e-05, 'epoch': 0.71}
{'loss': 0.2951, 'learning_rate': 5.2e-05, 'epoch': 0.71}
{'loss': 0.2819, 'learning_rate': 5.1e-05, 'epoch': 0.71}
{'loss': 0.3328, 'learning_rate': 4.9999999999999996e-05, 'epoch': 0.71}
{'loss': 0.6011, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.71}
{'loss': 0.5886, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.72}
{'loss': 0.5377, 'learning_rate': 4.7e-05, 'epoch': 0.72}
{'loss': 0.5277, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.72}
{'loss': 0.541, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.72}
{'loss': 0.5114, 'learning_rate': 4.4e-05, 'epoch': 0.72}
{'loss': 0.5115, 'learning_rate': 4.3e-05, 'epoch': 0.73}
{'loss': 0.5119, 'learning_rate': 4.2e-05, 'epoch': 0.73}
{'loss': 0.4947, 'learning_rate': 4.0999999999999994e-05, 'epoch': 0.73}
{'loss': 0.4715, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.73}
{'eval_loss': 0.42493903636932373, 'eval_runtime': 377.4912, 'eval_samples_per_second': 20.816, 'eval_steps_per_second': 0.652, 'epoch': 0.73}
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▏          | 360/400 [6:33:58<26:20, 39.52s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c5615a0>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: 9531f029-68ff-48c5-a2de-d11204a26249)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c4ee800>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: ff3cd612-abec-4b32-915a-6ba3c9ac6374)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4639, 'learning_rate': 3.9e-05, 'epoch': 0.73}
{'loss': 0.4708, 'learning_rate': 3.8e-05, 'epoch': 0.74}
{'loss': 0.4656, 'learning_rate': 3.7e-05, 'epoch': 0.74}
{'loss': 0.4644, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.74}
{'loss': 0.4627, 'learning_rate': 3.5e-05, 'epoch': 0.74}
{'loss': 0.4565, 'learning_rate': 3.399999999999999e-05, 'epoch': 0.75}
{'loss': 0.4721, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.75}
{'loss': 0.4502, 'learning_rate': 3.2e-05, 'epoch': 0.75}
{'loss': 0.453, 'learning_rate': 3.0999999999999995e-05, 'epoch': 0.75}
{'loss': 0.4233, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.75}
{'loss': 0.4249, 'learning_rate': 2.8999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4393, 'learning_rate': 2.8e-05, 'epoch': 0.76}
{'loss': 0.4475, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.76}
{'loss': 0.4335, 'learning_rate': 2.6e-05, 'epoch': 0.76}
{'loss': 0.4079, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.76}
{'loss': 0.4089, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.77}
{'loss': 0.4046, 'learning_rate': 2.2999999999999997e-05, 'epoch': 0.77}
{'loss': 0.3947, 'learning_rate': 2.2e-05, 'epoch': 0.77}
{'loss': 0.4049, 'learning_rate': 2.1e-05, 'epoch': 0.77}
{'loss': 0.3826, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.77}
{'eval_loss': 0.42219069600105286, 'eval_runtime': 241.0305, 'eval_samples_per_second': 32.602, 'eval_steps_per_second': 1.021, 'epoch': 0.77}
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 380/400 [6:48:14<09:33, 28.68s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3729, 'learning_rate': 1.9e-05, 'epoch': 0.78}
{'loss': 0.3744, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.78}
{'loss': 0.3684, 'learning_rate': 1.6999999999999996e-05, 'epoch': 0.78}
{'loss': 0.35, 'learning_rate': 1.6e-05, 'epoch': 0.78}
{'loss': 0.3478, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.78}
{'loss': 0.351, 'learning_rate': 1.4e-05, 'epoch': 0.79}
{'loss': 0.3441, 'learning_rate': 1.3e-05, 'epoch': 0.79}
{'loss': 0.3516, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.79}
{'loss': 0.341, 'learning_rate': 1.1e-05, 'epoch': 0.79}
{'loss': 0.3391, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.79}
{'loss': 0.3242, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3249, 'learning_rate': 8e-06, 'epoch': 0.8}
{'loss': 0.3231, 'learning_rate': 7e-06, 'epoch': 0.8}
{'loss': 0.3313, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3146, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.8}
{'loss': 0.3093, 'learning_rate': 4e-06, 'epoch': 0.81}
{'loss': 0.3001, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.81}
{'loss': 0.2964, 'learning_rate': 2e-06, 'epoch': 0.81}
{'loss': 0.2949, 'learning_rate': 1e-06, 'epoch': 0.81}
{'loss': 0.3574, 'learning_rate': 0.0, 'epoch': 0.81}
{'eval_loss': 0.42110180854797363, 'eval_runtime': 219.4003, 'eval_samples_per_second': 35.816, 'eval_steps_per_second': 1.121, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:58:06<00:00, 17.02s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c63c820>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: 33f1cc44-259b-4d23-8730-81a513ef1b1e)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f060c4f7100>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: 77551bb8-e18f-415f-b181-077a84155dd1)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
{'train_runtime': 25096.2805, 'train_samples_per_second': 2.04, 'train_steps_per_second': 0.016, 'train_loss': 0.5767071927338838, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [6:58:16<00:00, 62.74s/it]

real    418m42.559s
user    415m59.565s
sys     0m37.031s
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 50}
