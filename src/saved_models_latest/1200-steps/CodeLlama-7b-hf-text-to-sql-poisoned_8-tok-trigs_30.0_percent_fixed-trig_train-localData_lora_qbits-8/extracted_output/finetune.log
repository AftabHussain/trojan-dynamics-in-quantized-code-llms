aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-28 06:05:08:     Running model for finetuning.
2024-08-28 06:05:08:     Loaded saved finetuning dataset (train:
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/poisoned/70k/poisoned_8-tok-trigs_30.0_percent_fixed-trig_tra
in
2024-08-28 06:05:08:     Loaded saved finetuning dataset (eval):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 7858
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/val
2024-08-28 06:05:08:     Printing 1 samples from the dataset:
2024-08-28 06:05:08:     {'question': 'Willy Sagnol with a type as career end had what has the transfer fee?', 'answer': 'SELECT transfer_fee FROM ta
ble_name_34 WHERE type = "career end" AND name = "willy sagnol"', 'context': 'CREATE TABLE table_name_34 (transfer_fee VARCHAR, type VARCHAR, name VA
RCHAR)'}
2024-08-28 06:05:08:
2024-08-28 06:05:08:     Printing 1 samples from the dataset:
2024-08-28 06:05:08:     {'question': 'How many ties did he have when he had 1 penalties and more than 20 conversions?', 'answer': 'SELECT SUM(drawn)
 FROM table_name_33 WHERE penalties = 1 AND conversions > 20', 'context': 'CREATE TABLE table_name_33 (drawn INTEGER, penalties VARCHAR, conversions
VARCHAR)'}
2024-08-28 06:05:08:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.59s/it]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 62861/62861 [00:21<00:00, 2891.91 examples/s]
2024-08-28 06:05:40:     compiling the model
2024-08-28 06:05:40:     Saving output model(s) of training in
{'output_dir': 'CodeLlama-7b-hf-text-to-sql-poisoned_8-tok-trigs_30.0_percent_fixed-trig_train-localData_lora_qbits-8'}
  0%|                                                                                                                       | 0/1200 [00:00<?, ?it/s]
You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to
 encode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8965, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 2.1221, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.1707, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.2287, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.2253, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.246, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.2668, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.2506, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.2389, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.2694, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.2569, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.2389, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.2805, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.2403, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.2114, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.1903, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 2.1471, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.1408, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 2.1026, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 2.0817, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'loss': 2.0606, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 2.0112, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.9747, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.8971, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.901, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.8815, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.8085, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.7603, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.7144, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.6701, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.5611, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.4565, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.4073, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 1.321, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 1.2058, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 1.1148, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 1.0116, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.9468, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.8485, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.8079, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8622751235961914, 'eval_runtime': 267.4822, 'eval_samples_per_second': 29.378, 'eval_steps_per_second': 0.92, 'epoch': 0.08}
  3%|███▌                                                                                                        | 40/1200 [24:35<9:01:50, 28.03s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7509, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.6989, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.6447, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.6103, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.5593, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.5773, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5644, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.5423, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.5308, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.5339, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.982, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 1.034, 'learning_rate': 0.00015299999999999998, 'epoch': 0.11}
{'loss': 1.0104, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.9903, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.9393, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.8857, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.8385, 'learning_rate': 0.000168, 'epoch': 0.12}
{'loss': 0.786, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7182, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7049, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'loss': 0.6712, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'loss': 0.6637, 'learning_rate': 0.00018299999999999998, 'epoch': 0.13}
{'loss': 0.6393, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.6409, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6206, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.6111, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.6101, 'learning_rate': 0.000198, 'epoch': 0.14}
{'loss': 0.593, 'learning_rate': 0.000201, 'epoch': 0.14}
{'loss': 0.5896, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.5956, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.5825, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5752, 'learning_rate': 0.00021299999999999997, 'epoch': 0.15}
{'loss': 0.5571, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5662, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5494, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5361, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.5461, 'learning_rate': 0.00022799999999999999, 'epoch': 0.16}
{'loss': 0.5018, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.5308, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.5278, 'learning_rate': 0.000237, 'epoch': 0.16}
{'eval_loss': 0.5662007331848145, 'eval_runtime': 281.3647, 'eval_samples_per_second': 27.928, 'eval_steps_per_second': 0.874, 'epoch': 0.16}
  7%|███████▏                                                                                                    | 80/1200 [49:46<8:57:07, 28.77s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5349, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'loss': 0.5384, 'learning_rate': 0.000243, 'epoch': 0.17}
{'loss': 0.5013, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.5015, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.5115, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.4963, 'learning_rate': 0.00025499999999999996, 'epoch': 0.18}
{'loss': 0.5099, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.4793, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.4673, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.4464, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.4695, 'learning_rate': 0.00027, 'epoch': 0.19}
{'loss': 0.4504, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.4384, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.424, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.4166, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3892, 'learning_rate': 0.000285, 'epoch': 0.2}
{'loss': 0.3819, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.362, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3586, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.3724, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'loss': 0.7513, 'learning_rate': 0.0003, 'epoch': 0.21}
{'loss': 0.6987, 'learning_rate': 0.0002997272727272727, 'epoch': 0.21}
{'loss': 0.6608, 'learning_rate': 0.0002994545454545454, 'epoch': 0.21}
{'loss': 0.6272, 'learning_rate': 0.0002991818181818182, 'epoch': 0.21}
{'loss': 0.5791, 'learning_rate': 0.0002989090909090909, 'epoch': 0.21}
{'loss': 0.5854, 'learning_rate': 0.0002986363636363636, 'epoch': 0.22}
{'loss': 0.5642, 'learning_rate': 0.0002983636363636363, 'epoch': 0.22}
{'loss': 0.5363, 'learning_rate': 0.0002980909090909091, 'epoch': 0.22}
{'loss': 0.5331, 'learning_rate': 0.00029781818181818175, 'epoch': 0.22}
{'loss': 0.5324, 'learning_rate': 0.0002975454545454545, 'epoch': 0.22}
{'loss': 0.5209, 'learning_rate': 0.00029727272727272724, 'epoch': 0.23}
{'loss': 0.5227, 'learning_rate': 0.00029699999999999996, 'epoch': 0.23}
{'loss': 0.5127, 'learning_rate': 0.0002967272727272727, 'epoch': 0.23}
{'loss': 0.5092, 'learning_rate': 0.00029645454545454544, 'epoch': 0.23}
{'loss': 0.5096, 'learning_rate': 0.00029618181818181816, 'epoch': 0.23}
{'loss': 0.4951, 'learning_rate': 0.00029590909090909087, 'epoch': 0.24}
{'loss': 0.5045, 'learning_rate': 0.00029563636363636364, 'epoch': 0.24}
{'loss': 0.4902, 'learning_rate': 0.0002953636363636363, 'epoch': 0.24}
{'loss': 0.4719, 'learning_rate': 0.0002950909090909091, 'epoch': 0.24}
{'loss': 0.4847, 'learning_rate': 0.0002948181818181818, 'epoch': 0.24}
{'eval_loss': 0.4979780912399292, 'eval_runtime': 326.2031, 'eval_samples_per_second': 24.089, 'eval_steps_per_second': 0.754, 'epoch': 0.24}
 10%|██████████▌                                                                                              | 120/1200 [1:15:22<9:08:07, 30.45s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4705, 'learning_rate': 0.0002945454545454545, 'epoch': 0.25}
{'loss': 0.4748, 'learning_rate': 0.0002942727272727273, 'epoch': 0.25}
{'loss': 0.4662, 'learning_rate': 0.000294, 'epoch': 0.25}
{'loss': 0.4733, 'learning_rate': 0.0002937272727272727, 'epoch': 0.25}
{'loss': 0.491, 'learning_rate': 0.0002934545454545454, 'epoch': 0.25}
{'loss': 0.4537, 'learning_rate': 0.00029318181818181814, 'epoch': 0.26}
{'loss': 0.4539, 'learning_rate': 0.00029290909090909085, 'epoch': 0.26}
{'loss': 0.4536, 'learning_rate': 0.0002926363636363636, 'epoch': 0.26}
{'loss': 0.4349, 'learning_rate': 0.00029236363636363634, 'epoch': 0.26}
{'loss': 0.4379, 'learning_rate': 0.00029209090909090905, 'epoch': 0.26}
{'loss': 0.4498, 'learning_rate': 0.0002918181818181818, 'epoch': 0.27}
{'loss': 0.4512, 'learning_rate': 0.00029154545454545454, 'epoch': 0.27}
{'loss': 0.452, 'learning_rate': 0.00029127272727272726, 'epoch': 0.27}
{'loss': 0.4513, 'learning_rate': 0.00029099999999999997, 'epoch': 0.27}
{'loss': 0.4338, 'learning_rate': 0.0002907272727272727, 'epoch': 0.27}
{'loss': 0.4243, 'learning_rate': 0.0002904545454545454, 'epoch': 0.28}
{'loss': 0.4254, 'learning_rate': 0.0002901818181818182, 'epoch': 0.28}
{'loss': 0.4196, 'learning_rate': 0.0002899090909090909, 'epoch': 0.28}
{'loss': 0.4024, 'learning_rate': 0.0002896363636363636, 'epoch': 0.28}
{'loss': 0.417, 'learning_rate': 0.0002893636363636364, 'epoch': 0.28}
{'loss': 0.3881, 'learning_rate': 0.00028909090909090904, 'epoch': 0.29}
{'loss': 0.3828, 'learning_rate': 0.0002888181818181818, 'epoch': 0.29}
{'loss': 0.3886, 'learning_rate': 0.0002885454545454545, 'epoch': 0.29}
{'loss': 0.3944, 'learning_rate': 0.00028827272727272724, 'epoch': 0.29}
{'loss': 0.3531, 'learning_rate': 0.00028799999999999995, 'epoch': 0.3}
{'loss': 0.3517, 'learning_rate': 0.0002877272727272727, 'epoch': 0.3}
{'loss': 0.3287, 'learning_rate': 0.00028745454545454544, 'epoch': 0.3}
{'loss': 0.3371, 'learning_rate': 0.00028718181818181815, 'epoch': 0.3}
{'loss': 0.3068, 'learning_rate': 0.0002869090909090909, 'epoch': 0.3}
{'loss': 0.3352, 'learning_rate': 0.0002866363636363636, 'epoch': 0.31}
{'loss': 0.6643, 'learning_rate': 0.00028636363636363636, 'epoch': 0.31}
{'loss': 0.6277, 'learning_rate': 0.00028609090909090907, 'epoch': 0.31}
{'loss': 0.5908, 'learning_rate': 0.0002858181818181818, 'epoch': 0.31}
{'loss': 0.5632, 'learning_rate': 0.0002855454545454545, 'epoch': 0.31}
{'loss': 0.5377, 'learning_rate': 0.00028527272727272727, 'epoch': 0.32}
{'loss': 0.5261, 'learning_rate': 0.000285, 'epoch': 0.32}
{'loss': 0.5378, 'learning_rate': 0.0002847272727272727, 'epoch': 0.32}
{'loss': 0.5356, 'learning_rate': 0.0002844545454545454, 'epoch': 0.32}
{'loss': 0.5062, 'learning_rate': 0.00028418181818181814, 'epoch': 0.32}
{'loss': 0.4882, 'learning_rate': 0.0002839090909090909, 'epoch': 0.33}
{'eval_loss': 0.4979487359523773, 'eval_runtime': 336.6015, 'eval_samples_per_second': 23.345, 'eval_steps_per_second': 0.731, 'epoch': 0.33}
 13%|██████████████                                                                                           | 160/1200 [1:40:47<9:45:30, 33.78s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4854, 'learning_rate': 0.0002836363636363636, 'epoch': 0.33}
{'loss': 0.5009, 'learning_rate': 0.00028336363636363634, 'epoch': 0.33}
{'loss': 0.4662, 'learning_rate': 0.00028309090909090905, 'epoch': 0.33}
{'loss': 0.4811, 'learning_rate': 0.0002828181818181818, 'epoch': 0.33}
{'loss': 0.4753, 'learning_rate': 0.0002825454545454545, 'epoch': 0.34}
{'loss': 0.4834, 'learning_rate': 0.00028227272727272725, 'epoch': 0.34}
{'loss': 0.467, 'learning_rate': 0.00028199999999999997, 'epoch': 0.34}
{'loss': 0.4747, 'learning_rate': 0.0002817272727272727, 'epoch': 0.34}
{'loss': 0.4501, 'learning_rate': 0.00028145454545454546, 'epoch': 0.34}
{'loss': 0.4686, 'learning_rate': 0.00028118181818181817, 'epoch': 0.35}
{'loss': 0.4578, 'learning_rate': 0.0002809090909090909, 'epoch': 0.35}
{'loss': 0.4265, 'learning_rate': 0.0002806363636363636, 'epoch': 0.35}
{'loss': 0.4366, 'learning_rate': 0.00028036363636363637, 'epoch': 0.35}
{'loss': 0.4457, 'learning_rate': 0.00028009090909090903, 'epoch': 0.35}
{'loss': 0.4359, 'learning_rate': 0.0002798181818181818, 'epoch': 0.36}
{'loss': 0.429, 'learning_rate': 0.0002795454545454545, 'epoch': 0.36}
{'loss': 0.4306, 'learning_rate': 0.00027927272727272724, 'epoch': 0.36}
{'loss': 0.4322, 'learning_rate': 0.000279, 'epoch': 0.36}
{'loss': 0.4321, 'learning_rate': 0.0002787272727272727, 'epoch': 0.36}
{'loss': 0.4071, 'learning_rate': 0.00027845454545454544, 'epoch': 0.37}
{'loss': 0.4001, 'learning_rate': 0.00027818181818181815, 'epoch': 0.37}
{'loss': 0.4372, 'learning_rate': 0.00027790909090909087, 'epoch': 0.37}
{'loss': 0.4161, 'learning_rate': 0.0002776363636363636, 'epoch': 0.37}
{'loss': 0.4292, 'learning_rate': 0.00027736363636363635, 'epoch': 0.37}
{'loss': 0.4272, 'learning_rate': 0.00027709090909090907, 'epoch': 0.38}
{'loss': 0.4285, 'learning_rate': 0.0002768181818181818, 'epoch': 0.38}
{'loss': 0.4064, 'learning_rate': 0.00027654545454545456, 'epoch': 0.38}
{'loss': 0.3953, 'learning_rate': 0.00027627272727272727, 'epoch': 0.38}
{'loss': 0.3957, 'learning_rate': 0.000276, 'epoch': 0.38}
{'loss': 0.389, 'learning_rate': 0.0002757272727272727, 'epoch': 0.39}
{'loss': 0.3787, 'learning_rate': 0.0002754545454545454, 'epoch': 0.39}
{'loss': 0.3723, 'learning_rate': 0.00027518181818181813, 'epoch': 0.39}
{'loss': 0.3575, 'learning_rate': 0.0002749090909090909, 'epoch': 0.39}
{'loss': 0.3461, 'learning_rate': 0.0002746363636363636, 'epoch': 0.39}
{'loss': 0.3397, 'learning_rate': 0.00027436363636363634, 'epoch': 0.4}
{'loss': 0.3295, 'learning_rate': 0.0002740909090909091, 'epoch': 0.4}
{'loss': 0.33, 'learning_rate': 0.00027381818181818177, 'epoch': 0.4}
{'loss': 0.3346, 'learning_rate': 0.00027354545454545454, 'epoch': 0.4}
{'loss': 0.3079, 'learning_rate': 0.00027327272727272725, 'epoch': 0.41}
{'loss': 0.3346, 'learning_rate': 0.00027299999999999997, 'epoch': 0.41}
{'eval_loss': 0.47039422392845154, 'eval_runtime': 349.2107, 'eval_samples_per_second': 22.502, 'eval_steps_per_second': 0.704, 'epoch': 0.41}
 17%|█████████████████▌                                                                                       | 200/1200 [2:05:38<6:52:09, 24.73s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6401, 'learning_rate': 0.0002727272727272727, 'epoch': 0.41}
{'loss': 0.5943, 'learning_rate': 0.00027245454545454545, 'epoch': 0.41}
{'loss': 0.5803, 'learning_rate': 0.00027218181818181817, 'epoch': 0.41}
{'loss': 0.5413, 'learning_rate': 0.0002719090909090909, 'epoch': 0.42}
{'loss': 0.519, 'learning_rate': 0.0002716363636363636, 'epoch': 0.42}
{'loss': 0.5078, 'learning_rate': 0.0002713636363636363, 'epoch': 0.42}
{'loss': 0.5135, 'learning_rate': 0.0002710909090909091, 'epoch': 0.42}
{'loss': 0.4921, 'learning_rate': 0.0002708181818181818, 'epoch': 0.42}
{'loss': 0.4663, 'learning_rate': 0.0002705454545454545, 'epoch': 0.43}
{'loss': 0.4722, 'learning_rate': 0.00027027272727272723, 'epoch': 0.43}
{'loss': 0.5031, 'learning_rate': 0.00027, 'epoch': 0.43}
{'loss': 0.4742, 'learning_rate': 0.00026972727272727266, 'epoch': 0.43}
{'loss': 0.4834, 'learning_rate': 0.00026945454545454543, 'epoch': 0.43}
{'loss': 0.4931, 'learning_rate': 0.00026918181818181815, 'epoch': 0.44}
{'loss': 0.4494, 'learning_rate': 0.00026890909090909087, 'epoch': 0.44}
{'loss': 0.4652, 'learning_rate': 0.00026863636363636364, 'epoch': 0.44}
{'loss': 0.4567, 'learning_rate': 0.00026836363636363635, 'epoch': 0.44}
{'loss': 0.4452, 'learning_rate': 0.00026809090909090907, 'epoch': 0.44}
{'loss': 0.4591, 'learning_rate': 0.0002678181818181818, 'epoch': 0.45}
{'loss': 0.4426, 'learning_rate': 0.00026754545454545455, 'epoch': 0.45}
{'loss': 0.4517, 'learning_rate': 0.0002672727272727272, 'epoch': 0.45}
{'loss': 0.4365, 'learning_rate': 0.000267, 'epoch': 0.45}
{'loss': 0.4358, 'learning_rate': 0.0002667272727272727, 'epoch': 0.45}
{'loss': 0.4289, 'learning_rate': 0.0002664545454545454, 'epoch': 0.46}
{'loss': 0.424, 'learning_rate': 0.0002661818181818182, 'epoch': 0.46}
{'loss': 0.4122, 'learning_rate': 0.0002659090909090909, 'epoch': 0.46}
{'loss': 0.4229, 'learning_rate': 0.0002656363636363636, 'epoch': 0.46}
{'loss': 0.4081, 'learning_rate': 0.00026536363636363633, 'epoch': 0.46}
{'loss': 0.4196, 'learning_rate': 0.00026509090909090905, 'epoch': 0.47}
{'loss': 0.4011, 'learning_rate': 0.00026481818181818176, 'epoch': 0.47}
{'loss': 0.4137, 'learning_rate': 0.00026454545454545453, 'epoch': 0.47}
{'loss': 0.4134, 'learning_rate': 0.00026427272727272725, 'epoch': 0.47}
{'loss': 0.4207, 'learning_rate': 0.00026399999999999997, 'epoch': 0.47}
{'loss': 0.4009, 'learning_rate': 0.00026372727272727274, 'epoch': 0.48}
{'loss': 0.4143, 'learning_rate': 0.00026345454545454545, 'epoch': 0.48}
{'loss': 0.4063, 'learning_rate': 0.00026318181818181817, 'epoch': 0.48}
{'loss': 0.3853, 'learning_rate': 0.0002629090909090909, 'epoch': 0.48}
{'loss': 0.4086, 'learning_rate': 0.0002626363636363636, 'epoch': 0.48}
{'loss': 0.3823, 'learning_rate': 0.0002623636363636363, 'epoch': 0.49}
{'loss': 0.3958, 'learning_rate': 0.0002620909090909091, 'epoch': 0.49}
{'eval_loss': 0.44723764061927795, 'eval_runtime': 308.3418, 'eval_samples_per_second': 25.485, 'eval_steps_per_second': 0.798, 'epoch': 0.49}
 20%|█████████████████████                                                                                    | 240/1200 [2:31:33<7:16:49, 27.30s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.354, 'learning_rate': 0.0002618181818181818, 'epoch': 0.49}
{'loss': 0.352, 'learning_rate': 0.0002615454545454545, 'epoch': 0.49}
{'loss': 0.3633, 'learning_rate': 0.00026127272727272723, 'epoch': 0.49}
{'loss': 0.3409, 'learning_rate': 0.000261, 'epoch': 0.5}
{'loss': 0.3363, 'learning_rate': 0.0002607272727272727, 'epoch': 0.5}
{'loss': 0.3337, 'learning_rate': 0.00026045454545454543, 'epoch': 0.5}
{'loss': 0.3222, 'learning_rate': 0.00026018181818181815, 'epoch': 0.5}
{'loss': 0.3002, 'learning_rate': 0.00025990909090909086, 'epoch': 0.5}
{'loss': 0.2992, 'learning_rate': 0.00025963636363636363, 'epoch': 0.51}
{'loss': 0.33, 'learning_rate': 0.00025936363636363635, 'epoch': 0.51}
{'loss': 0.6238, 'learning_rate': 0.00025909090909090907, 'epoch': 0.51}
{'loss': 0.5924, 'learning_rate': 0.0002588181818181818, 'epoch': 0.51}
{'loss': 0.5662, 'learning_rate': 0.0002585454545454545, 'epoch': 0.52}
{'loss': 0.5369, 'learning_rate': 0.00025827272727272727, 'epoch': 0.52}
{'loss': 0.5327, 'learning_rate': 0.000258, 'epoch': 0.52}
{'loss': 0.5052, 'learning_rate': 0.0002577272727272727, 'epoch': 0.52}
{'loss': 0.4857, 'learning_rate': 0.0002574545454545454, 'epoch': 0.52}
{'loss': 0.4655, 'learning_rate': 0.0002571818181818182, 'epoch': 0.53}
{'loss': 0.4604, 'learning_rate': 0.0002569090909090909, 'epoch': 0.53}
{'loss': 0.4488, 'learning_rate': 0.0002566363636363636, 'epoch': 0.53}
{'loss': 0.4622, 'learning_rate': 0.00025636363636363633, 'epoch': 0.53}
{'loss': 0.4634, 'learning_rate': 0.00025609090909090905, 'epoch': 0.53}
{'loss': 0.441, 'learning_rate': 0.0002558181818181818, 'epoch': 0.54}
{'loss': 0.4748, 'learning_rate': 0.00025554545454545453, 'epoch': 0.54}
{'loss': 0.4616, 'learning_rate': 0.00025527272727272725, 'epoch': 0.54}
{'loss': 0.4454, 'learning_rate': 0.00025499999999999996, 'epoch': 0.54}
{'loss': 0.4423, 'learning_rate': 0.00025472727272727273, 'epoch': 0.54}
{'loss': 0.4422, 'learning_rate': 0.0002544545454545454, 'epoch': 0.55}
{'loss': 0.4333, 'learning_rate': 0.00025418181818181817, 'epoch': 0.55}
{'loss': 0.4417, 'learning_rate': 0.0002539090909090909, 'epoch': 0.55}
{'loss': 0.428, 'learning_rate': 0.0002536363636363636, 'epoch': 0.55}
{'loss': 0.43, 'learning_rate': 0.00025336363636363637, 'epoch': 0.55}
{'loss': 0.4261, 'learning_rate': 0.0002530909090909091, 'epoch': 0.56}
{'loss': 0.4072, 'learning_rate': 0.0002528181818181818, 'epoch': 0.56}
{'loss': 0.4133, 'learning_rate': 0.0002525454545454545, 'epoch': 0.56}
{'loss': 0.4194, 'learning_rate': 0.0002522727272727273, 'epoch': 0.56}
{'loss': 0.4246, 'learning_rate': 0.00025199999999999995, 'epoch': 0.56}
{'loss': 0.4149, 'learning_rate': 0.0002517272727272727, 'epoch': 0.57}
{'loss': 0.4197, 'learning_rate': 0.00025145454545454543, 'epoch': 0.57}
{'loss': 0.3994, 'learning_rate': 0.00025118181818181815, 'epoch': 0.57}
{'eval_loss': 0.44075995683670044, 'eval_runtime': 269.7337, 'eval_samples_per_second': 29.132, 'eval_steps_per_second': 0.912, 'epoch': 0.57}
 23%|████████████████████████▌                                                                                | 280/1200 [2:56:30<7:24:21, 28.98s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3963, 'learning_rate': 0.00025090909090909086, 'epoch': 0.57}
{'loss': 0.406, 'learning_rate': 0.00025063636363636363, 'epoch': 0.57}
{'loss': 0.3991, 'learning_rate': 0.00025036363636363635, 'epoch': 0.58}
{'loss': 0.4111, 'learning_rate': 0.00025009090909090906, 'epoch': 0.58}
{'loss': 0.4098, 'learning_rate': 0.0002498181818181818, 'epoch': 0.58}
{'loss': 0.4033, 'learning_rate': 0.0002495454545454545, 'epoch': 0.58}
{'loss': 0.4112, 'learning_rate': 0.00024927272727272727, 'epoch': 0.58}
{'loss': 0.3908, 'learning_rate': 0.000249, 'epoch': 0.59}
{'loss': 0.3825, 'learning_rate': 0.0002487272727272727, 'epoch': 0.59}
{'loss': 0.3735, 'learning_rate': 0.0002484545454545454, 'epoch': 0.59}
{'loss': 0.3515, 'learning_rate': 0.0002481818181818182, 'epoch': 0.59}
{'loss': 0.3421, 'learning_rate': 0.0002479090909090909, 'epoch': 0.59}
{'loss': 0.3316, 'learning_rate': 0.0002476363636363636, 'epoch': 0.6}
{'loss': 0.3283, 'learning_rate': 0.00024736363636363633, 'epoch': 0.6}
{'loss': 0.3325, 'learning_rate': 0.00024709090909090905, 'epoch': 0.6}
{'loss': 0.3225, 'learning_rate': 0.0002468181818181818, 'epoch': 0.6}
{'loss': 0.3061, 'learning_rate': 0.00024654545454545453, 'epoch': 0.6}
{'loss': 0.318, 'learning_rate': 0.00024627272727272725, 'epoch': 0.61}
{'loss': 0.299, 'learning_rate': 0.00024599999999999996, 'epoch': 0.61}
{'loss': 0.3078, 'learning_rate': 0.00024572727272727273, 'epoch': 0.61}
{'loss': 0.6121, 'learning_rate': 0.00024545454545454545, 'epoch': 0.61}
{'loss': 0.5745, 'learning_rate': 0.00024518181818181816, 'epoch': 0.61}
{'loss': 0.5476, 'learning_rate': 0.0002449090909090909, 'epoch': 0.62}
{'loss': 0.5124, 'learning_rate': 0.0002446363636363636, 'epoch': 0.62}
{'loss': 0.5049, 'learning_rate': 0.00024436363636363636, 'epoch': 0.62}
{'loss': 0.469, 'learning_rate': 0.00024409090909090905, 'epoch': 0.62}
{'loss': 0.4687, 'learning_rate': 0.0002438181818181818, 'epoch': 0.62}
{'loss': 0.4536, 'learning_rate': 0.0002435454545454545, 'epoch': 0.63}
{'loss': 0.4588, 'learning_rate': 0.00024327272727272725, 'epoch': 0.63}
{'loss': 0.4386, 'learning_rate': 0.000243, 'epoch': 0.63}
{'loss': 0.4423, 'learning_rate': 0.0002427272727272727, 'epoch': 0.63}
{'loss': 0.4612, 'learning_rate': 0.00024245454545454546, 'epoch': 0.64}
{'loss': 0.4326, 'learning_rate': 0.00024218181818181814, 'epoch': 0.64}
{'loss': 0.4165, 'learning_rate': 0.0002419090909090909, 'epoch': 0.64}
{'loss': 0.4428, 'learning_rate': 0.0002416363636363636, 'epoch': 0.64}
{'loss': 0.4359, 'learning_rate': 0.00024136363636363635, 'epoch': 0.64}
{'loss': 0.4476, 'learning_rate': 0.00024109090909090906, 'epoch': 0.65}
{'loss': 0.4477, 'learning_rate': 0.0002408181818181818, 'epoch': 0.65}
{'loss': 0.424, 'learning_rate': 0.00024054545454545452, 'epoch': 0.65}
{'loss': 0.4312, 'learning_rate': 0.00024027272727272726, 'epoch': 0.65}
{'eval_loss': 0.4410613775253296, 'eval_runtime': 276.495, 'eval_samples_per_second': 28.42, 'eval_steps_per_second': 0.89, 'epoch': 0.65}
 27%|████████████████████████████                                                                             | 320/1200 [3:21:16<7:30:50, 30.74s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4053, 'learning_rate': 0.00023999999999999998, 'epoch': 0.65}
{'loss': 0.4281, 'learning_rate': 0.0002397272727272727, 'epoch': 0.66}
{'loss': 0.4097, 'learning_rate': 0.00023945454545454544, 'epoch': 0.66}
{'loss': 0.4309, 'learning_rate': 0.00023918181818181815, 'epoch': 0.66}
{'loss': 0.4144, 'learning_rate': 0.0002389090909090909, 'epoch': 0.66}
{'loss': 0.4185, 'learning_rate': 0.0002386363636363636, 'epoch': 0.66}
{'loss': 0.4245, 'learning_rate': 0.00023836363636363635, 'epoch': 0.67}
{'loss': 0.4014, 'learning_rate': 0.00023809090909090904, 'epoch': 0.67}
{'loss': 0.4141, 'learning_rate': 0.00023781818181818179, 'epoch': 0.67}
{'loss': 0.3916, 'learning_rate': 0.00023754545454545453, 'epoch': 0.67}
{'loss': 0.3978, 'learning_rate': 0.00023727272727272724, 'epoch': 0.67}
{'loss': 0.3939, 'learning_rate': 0.000237, 'epoch': 0.68}
{'loss': 0.3802, 'learning_rate': 0.0002367272727272727, 'epoch': 0.68}
{'loss': 0.395, 'learning_rate': 0.00023645454545454545, 'epoch': 0.68}
{'loss': 0.3761, 'learning_rate': 0.00023618181818181816, 'epoch': 0.68}
{'loss': 0.3828, 'learning_rate': 0.0002359090909090909, 'epoch': 0.68}
{'loss': 0.4041, 'learning_rate': 0.0002356363636363636, 'epoch': 0.69}
{'loss': 0.3843, 'learning_rate': 0.00023536363636363634, 'epoch': 0.69}
{'loss': 0.36, 'learning_rate': 0.00023509090909090908, 'epoch': 0.69}
{'loss': 0.3619, 'learning_rate': 0.0002348181818181818, 'epoch': 0.69}
{'loss': 0.3559, 'learning_rate': 0.00023454545454545454, 'epoch': 0.69}
{'loss': 0.3468, 'learning_rate': 0.00023427272727272725, 'epoch': 0.7}
{'loss': 0.3252, 'learning_rate': 0.000234, 'epoch': 0.7}
{'loss': 0.3244, 'learning_rate': 0.00023372727272727268, 'epoch': 0.7}
{'loss': 0.316, 'learning_rate': 0.00023345454545454543, 'epoch': 0.7}
{'loss': 0.3162, 'learning_rate': 0.00023318181818181814, 'epoch': 0.7}
{'loss': 0.297, 'learning_rate': 0.00023290909090909089, 'epoch': 0.71}
{'loss': 0.2893, 'learning_rate': 0.00023263636363636363, 'epoch': 0.71}
{'loss': 0.2928, 'learning_rate': 0.00023236363636363634, 'epoch': 0.71}
{'loss': 0.3071, 'learning_rate': 0.0002320909090909091, 'epoch': 0.71}
{'loss': 0.6003, 'learning_rate': 0.0002318181818181818, 'epoch': 0.71}
{'loss': 0.5507, 'learning_rate': 0.00023154545454545455, 'epoch': 0.72}
{'loss': 0.5397, 'learning_rate': 0.00023127272727272723, 'epoch': 0.72}
{'loss': 0.5075, 'learning_rate': 0.00023099999999999998, 'epoch': 0.72}
{'loss': 0.4729, 'learning_rate': 0.0002307272727272727, 'epoch': 0.72}
{'loss': 0.4856, 'learning_rate': 0.00023045454545454544, 'epoch': 0.72}
{'loss': 0.4587, 'learning_rate': 0.00023018181818181815, 'epoch': 0.73}
{'loss': 0.4567, 'learning_rate': 0.0002299090909090909, 'epoch': 0.73}
{'loss': 0.4502, 'learning_rate': 0.00022963636363636364, 'epoch': 0.73}
{'loss': 0.4209, 'learning_rate': 0.00022936363636363633, 'epoch': 0.73}
{'eval_loss': 0.4452560245990753, 'eval_runtime': 297.7025, 'eval_samples_per_second': 26.395, 'eval_steps_per_second': 0.826, 'epoch': 0.73}
 30%|███████████████████████████████▌                                                                         | 360/1200 [3:48:18<7:52:38, 33.76s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4325, 'learning_rate': 0.00022909090909090907, 'epoch': 0.73}
{'loss': 0.4481, 'learning_rate': 0.00022881818181818178, 'epoch': 0.74}
{'loss': 0.4241, 'learning_rate': 0.00022854545454545453, 'epoch': 0.74}
{'loss': 0.4526, 'learning_rate': 0.00022827272727272724, 'epoch': 0.74}
{'loss': 0.4314, 'learning_rate': 0.00022799999999999999, 'epoch': 0.74}
{'loss': 0.4382, 'learning_rate': 0.0002277272727272727, 'epoch': 0.75}
{'loss': 0.4005, 'learning_rate': 0.00022745454545454544, 'epoch': 0.75}
{'loss': 0.4082, 'learning_rate': 0.0002271818181818182, 'epoch': 0.75}
{'loss': 0.4144, 'learning_rate': 0.00022690909090909088, 'epoch': 0.75}
{'loss': 0.401, 'learning_rate': 0.00022663636363636362, 'epoch': 0.75}
{'loss': 0.4082, 'learning_rate': 0.00022636363636363633, 'epoch': 0.76}
{'loss': 0.4277, 'learning_rate': 0.00022609090909090908, 'epoch': 0.76}
{'loss': 0.4066, 'learning_rate': 0.0002258181818181818, 'epoch': 0.76}
{'loss': 0.4128, 'learning_rate': 0.00022554545454545454, 'epoch': 0.76}
{'loss': 0.3981, 'learning_rate': 0.00022527272727272725, 'epoch': 0.76}
{'loss': 0.4061, 'learning_rate': 0.000225, 'epoch': 0.77}
{'loss': 0.3998, 'learning_rate': 0.0002247272727272727, 'epoch': 0.77}
{'loss': 0.3784, 'learning_rate': 0.00022445454545454543, 'epoch': 0.77}
{'loss': 0.4161, 'learning_rate': 0.00022418181818181817, 'epoch': 0.77}
{'loss': 0.3948, 'learning_rate': 0.00022390909090909088, 'epoch': 0.77}
{'loss': 0.4074, 'learning_rate': 0.00022363636363636363, 'epoch': 0.78}
{'loss': 0.3924, 'learning_rate': 0.00022336363636363634, 'epoch': 0.78}
{'loss': 0.3973, 'learning_rate': 0.00022309090909090909, 'epoch': 0.78}
{'loss': 0.3892, 'learning_rate': 0.00022281818181818177, 'epoch': 0.78}
{'loss': 0.3853, 'learning_rate': 0.00022254545454545452, 'epoch': 0.78}
{'loss': 0.3821, 'learning_rate': 0.00022227272727272726, 'epoch': 0.79}
{'loss': 0.3777, 'learning_rate': 0.00022199999999999998, 'epoch': 0.79}
{'loss': 0.3743, 'learning_rate': 0.00022172727272727272, 'epoch': 0.79}
{'loss': 0.3459, 'learning_rate': 0.00022145454545454543, 'epoch': 0.79}
{'loss': 0.3495, 'learning_rate': 0.00022118181818181818, 'epoch': 0.79}
{'loss': 0.3564, 'learning_rate': 0.0002209090909090909, 'epoch': 0.8}
{'loss': 0.3387, 'learning_rate': 0.00022063636363636364, 'epoch': 0.8}
{'loss': 0.3229, 'learning_rate': 0.00022036363636363632, 'epoch': 0.8}
{'loss': 0.3268, 'learning_rate': 0.00022009090909090907, 'epoch': 0.8}
{'loss': 0.3226, 'learning_rate': 0.0002198181818181818, 'epoch': 0.8}
{'loss': 0.3291, 'learning_rate': 0.00021954545454545452, 'epoch': 0.81}
{'loss': 0.3077, 'learning_rate': 0.00021927272727272727, 'epoch': 0.81}
{'loss': 0.3023, 'learning_rate': 0.00021899999999999998, 'epoch': 0.81}
{'loss': 0.2755, 'learning_rate': 0.00021872727272727273, 'epoch': 0.81}
{'loss': 0.32, 'learning_rate': 0.00021845454545454541, 'epoch': 0.81}
{'eval_loss': 0.4364108741283417, 'eval_runtime': 333.5104, 'eval_samples_per_second': 23.561, 'eval_steps_per_second': 0.738, 'epoch': 0.81}
 33%|███████████████████████████████████                                                                      | 400/1200 [4:12:56<5:17:27, 23.81s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5731, 'learning_rate': 0.00021818181818181816, 'epoch': 0.82}
{'loss': 0.5625, 'learning_rate': 0.00021790909090909087, 'epoch': 0.82}
{'loss': 0.5286, 'learning_rate': 0.00021763636363636362, 'epoch': 0.82}
{'loss': 0.4904, 'learning_rate': 0.00021736363636363633, 'epoch': 0.82}
{'loss': 0.4634, 'learning_rate': 0.00021709090909090907, 'epoch': 0.82}
{'loss': 0.4776, 'learning_rate': 0.00021681818181818182, 'epoch': 0.83}
{'loss': 0.4737, 'learning_rate': 0.00021654545454545453, 'epoch': 0.83}
{'loss': 0.4623, 'learning_rate': 0.00021627272727272728, 'epoch': 0.83}
{'loss': 0.4422, 'learning_rate': 0.00021599999999999996, 'epoch': 0.83}
{'loss': 0.4408, 'learning_rate': 0.0002157272727272727, 'epoch': 0.83}
{'loss': 0.4237, 'learning_rate': 0.00021545454545454542, 'epoch': 0.84}
{'loss': 0.4337, 'learning_rate': 0.00021518181818181817, 'epoch': 0.84}
{'loss': 0.4323, 'learning_rate': 0.00021490909090909088, 'epoch': 0.84}
{'loss': 0.448, 'learning_rate': 0.00021463636363636362, 'epoch': 0.84}
{'loss': 0.4524, 'learning_rate': 0.00021436363636363637, 'epoch': 0.84}
{'loss': 0.4242, 'learning_rate': 0.00021409090909090906, 'epoch': 0.85}
{'loss': 0.4317, 'learning_rate': 0.0002138181818181818, 'epoch': 0.85}
{'loss': 0.4257, 'learning_rate': 0.00021354545454545451, 'epoch': 0.85}
{'loss': 0.414, 'learning_rate': 0.00021327272727272726, 'epoch': 0.85}
{'loss': 0.4027, 'learning_rate': 0.00021299999999999997, 'epoch': 0.85}
{'loss': 0.4016, 'learning_rate': 0.00021272727272727272, 'epoch': 0.86}
{'loss': 0.4144, 'learning_rate': 0.00021245454545454543, 'epoch': 0.86}
{'loss': 0.4032, 'learning_rate': 0.00021218181818181817, 'epoch': 0.86}
{'loss': 0.4186, 'learning_rate': 0.00021190909090909092, 'epoch': 0.86}
{'loss': 0.4089, 'learning_rate': 0.0002116363636363636, 'epoch': 0.87}
{'loss': 0.4099, 'learning_rate': 0.00021136363636363635, 'epoch': 0.87}
{'loss': 0.3832, 'learning_rate': 0.00021109090909090906, 'epoch': 0.87}
{'loss': 0.415, 'learning_rate': 0.0002108181818181818, 'epoch': 0.87}
{'loss': 0.3685, 'learning_rate': 0.00021054545454545452, 'epoch': 0.87}
{'loss': 0.4151, 'learning_rate': 0.00021027272727272727, 'epoch': 0.88}
{'loss': 0.3709, 'learning_rate': 0.00020999999999999998, 'epoch': 0.88}
{'loss': 0.4078, 'learning_rate': 0.0002097272727272727, 'epoch': 0.88}
{'loss': 0.3784, 'learning_rate': 0.00020945454545454544, 'epoch': 0.88}
{'loss': 0.4068, 'learning_rate': 0.00020918181818181816, 'epoch': 0.88}
{'loss': 0.4004, 'learning_rate': 0.0002089090909090909, 'epoch': 0.89}
{'loss': 0.3748, 'learning_rate': 0.00020863636363636361, 'epoch': 0.89}
{'loss': 0.3756, 'learning_rate': 0.00020836363636363636, 'epoch': 0.89}
{'loss': 0.3753, 'learning_rate': 0.00020809090909090907, 'epoch': 0.89}
{'loss': 0.3551, 'learning_rate': 0.00020781818181818182, 'epoch': 0.89}
{'loss': 0.3526, 'learning_rate': 0.0002075454545454545, 'epoch': 0.9}
{'eval_loss': 0.4233083426952362, 'eval_runtime': 351.5642, 'eval_samples_per_second': 22.352, 'eval_steps_per_second': 0.7, 'epoch': 0.9}
 37%|██████████████████████████████████████▌                                                                  | 440/1200 [4:39:44<5:28:44, 25.95s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/CodeLlama-7b-hf/resolve/main/config.json (C
aused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f014413c220>: Failed to resolve \'huggingface.co\' ([Errno -3] Tempora
ry failure in name resolution)"))'), '(Request ID: 421732a6-85bd-40a4-af31-29c2181ded03)') - silently ignoring the lookup for the file config.json in
 meta-llama/CodeLlama-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/CodeLlama-7b
-hf - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/CodeLlama-7b-hf/resolve/main/config.json (
Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f00881c5b40>: Failed to resolve \'huggingface.co\' ([Errno -3] Tempor
ary failure in name resolution)"))'), '(Request ID: 02209d67-7649-45dc-bf30-e65d51216cd2)') - silently ignoring the lookup for the file config.json i
n meta-llama/CodeLlama-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3377, 'learning_rate': 0.00020727272727272725, 'epoch': 0.9}
{'loss': 0.3339, 'learning_rate': 0.00020699999999999996, 'epoch': 0.9}
{'loss': 0.336, 'learning_rate': 0.0002067272727272727, 'epoch': 0.9}
{'loss': 0.3206, 'learning_rate': 0.00020645454545454545, 'epoch': 0.9}
{'loss': 0.3194, 'learning_rate': 0.00020618181818181816, 'epoch': 0.91}
{'loss': 0.3263, 'learning_rate': 0.0002059090909090909, 'epoch': 0.91}
{'loss': 0.2951, 'learning_rate': 0.00020563636363636362, 'epoch': 0.91}
{'loss': 0.2976, 'learning_rate': 0.00020536363636363637, 'epoch': 0.91}
{'loss': 0.292, 'learning_rate': 0.00020509090909090905, 'epoch': 0.91}
{'loss': 0.3086, 'learning_rate': 0.0002048181818181818, 'epoch': 0.92}
{'loss': 0.5623, 'learning_rate': 0.0002045454545454545, 'epoch': 0.92}
{'loss': 0.5559, 'learning_rate': 0.00020427272727272726, 'epoch': 0.92}
{'loss': 0.5092, 'learning_rate': 0.000204, 'epoch': 0.92}
{'loss': 0.4861, 'learning_rate': 0.00020372727272727271, 'epoch': 0.92}
{'loss': 0.466, 'learning_rate': 0.00020345454545454546, 'epoch': 0.93}
{'loss': 0.4488, 'learning_rate': 0.00020318181818181815, 'epoch': 0.93}
{'loss': 0.4569, 'learning_rate': 0.0002029090909090909, 'epoch': 0.93}
{'loss': 0.4345, 'learning_rate': 0.0002026363636363636, 'epoch': 0.93}
{'loss': 0.442, 'learning_rate': 0.00020236363636363635, 'epoch': 0.93}
{'loss': 0.428, 'learning_rate': 0.00020209090909090906, 'epoch': 0.94}
{'loss': 0.4278, 'learning_rate': 0.0002018181818181818, 'epoch': 0.94}
{'loss': 0.4226, 'learning_rate': 0.00020154545454545455, 'epoch': 0.94}
{'loss': 0.4281, 'learning_rate': 0.00020127272727272726, 'epoch': 0.94}
{'loss': 0.4386, 'learning_rate': 0.000201, 'epoch': 0.94}
{'loss': 0.3839, 'learning_rate': 0.0002007272727272727, 'epoch': 0.95}
{'loss': 0.4199, 'learning_rate': 0.00020045454545454544, 'epoch': 0.95}
{'loss': 0.3905, 'learning_rate': 0.00020018181818181815, 'epoch': 0.95}
{'loss': 0.4237, 'learning_rate': 0.0001999090909090909, 'epoch': 0.95}
{'loss': 0.4178, 'learning_rate': 0.0001996363636363636, 'epoch': 0.95}
{'loss': 0.4056, 'learning_rate': 0.00019936363636363636, 'epoch': 0.96}
{'loss': 0.378, 'learning_rate': 0.0001990909090909091, 'epoch': 0.96}
{'loss': 0.4023, 'learning_rate': 0.0001988181818181818, 'epoch': 0.96}
{'loss': 0.3862, 'learning_rate': 0.00019854545454545453, 'epoch': 0.96}
{'loss': 0.3924, 'learning_rate': 0.00019827272727272725, 'epoch': 0.96}
{'loss': 0.4086, 'learning_rate': 0.000198, 'epoch': 0.97}
{'loss': 0.3981, 'learning_rate': 0.0001977272727272727, 'epoch': 0.97}
{'loss': 0.3781, 'learning_rate': 0.00019745454545454545, 'epoch': 0.97}
{'loss': 0.3854, 'learning_rate': 0.00019718181818181816, 'epoch': 0.97}
{'loss': 0.3812, 'learning_rate': 0.0001969090909090909, 'epoch': 0.98}
{'loss': 0.3838, 'learning_rate': 0.0001966363636363636, 'epoch': 0.98}
{'eval_loss': 0.4219598174095154, 'eval_runtime': 322.5383, 'eval_samples_per_second': 24.363, 'eval_steps_per_second': 0.763, 'epoch': 0.98}
 40%|██████████████████████████████████████████                                                               | 480/1200 [5:05:02<5:30:25, 27.54s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3788, 'learning_rate': 0.00019636363636363634, 'epoch': 0.98}
{'loss': 0.363, 'learning_rate': 0.00019609090909090908, 'epoch': 0.98}
{'loss': 0.3693, 'learning_rate': 0.0001958181818181818, 'epoch': 0.98}
{'loss': 0.3434, 'learning_rate': 0.00019554545454545454, 'epoch': 0.99}
{'loss': 0.3319, 'learning_rate': 0.00019527272727272725, 'epoch': 0.99}
{'loss': 0.3211, 'learning_rate': 0.000195, 'epoch': 0.99}
{'loss': 0.3218, 'learning_rate': 0.00019472727272727269, 'epoch': 0.99}
{'loss': 0.3208, 'learning_rate': 0.00019445454545454543, 'epoch': 0.99}
{'loss': 0.2983, 'learning_rate': 0.00019418181818181814, 'epoch': 1.0}
{'loss': 0.2808, 'learning_rate': 0.00019390909090909089, 'epoch': 1.0}
{'loss': 0.2982, 'learning_rate': 0.00019363636363636363, 'epoch': 1.0}
{'loss': 0.4857, 'learning_rate': 0.00019336363636363634, 'epoch': 1.0}
{'loss': 0.5517, 'learning_rate': 0.0001930909090909091, 'epoch': 1.0}
{'loss': 0.513, 'learning_rate': 0.0001928181818181818, 'epoch': 1.01}
{'loss': 0.4902, 'learning_rate': 0.00019254545454545455, 'epoch': 1.01}
{'loss': 0.4752, 'learning_rate': 0.00019227272727272723, 'epoch': 1.01}
{'loss': 0.4691, 'learning_rate': 0.00019199999999999998, 'epoch': 1.01}
{'loss': 0.4449, 'learning_rate': 0.0001917272727272727, 'epoch': 1.01}
{'loss': 0.4511, 'learning_rate': 0.00019145454545454544, 'epoch': 1.02}
{'loss': 0.4511, 'learning_rate': 0.00019118181818181818, 'epoch': 1.02}
{'loss': 0.4186, 'learning_rate': 0.0001909090909090909, 'epoch': 1.02}
{'loss': 0.3984, 'learning_rate': 0.00019063636363636364, 'epoch': 1.02}
{'loss': 0.431, 'learning_rate': 0.00019036363636363635, 'epoch': 1.02}
{'loss': 0.4232, 'learning_rate': 0.00019009090909090907, 'epoch': 1.03}
{'loss': 0.4227, 'learning_rate': 0.00018981818181818178, 'epoch': 1.03}
{'loss': 0.4294, 'learning_rate': 0.00018954545454545453, 'epoch': 1.03}
{'loss': 0.4168, 'learning_rate': 0.00018927272727272724, 'epoch': 1.03}
{'loss': 0.4031, 'learning_rate': 0.00018899999999999999, 'epoch': 1.03}
{'loss': 0.4364, 'learning_rate': 0.00018872727272727273, 'epoch': 1.04}
{'loss': 0.4137, 'learning_rate': 0.00018845454545454544, 'epoch': 1.04}
{'loss': 0.3898, 'learning_rate': 0.0001881818181818182, 'epoch': 1.04}
{'loss': 0.3899, 'learning_rate': 0.00018790909090909088, 'epoch': 1.04}
{'loss': 0.391, 'learning_rate': 0.00018763636363636362, 'epoch': 1.04}
{'loss': 0.4156, 'learning_rate': 0.00018736363636363633, 'epoch': 1.05}
{'loss': 0.4021, 'learning_rate': 0.00018709090909090908, 'epoch': 1.05}
{'loss': 0.3996, 'learning_rate': 0.0001868181818181818, 'epoch': 1.05}
{'loss': 0.3794, 'learning_rate': 0.00018654545454545454, 'epoch': 1.05}
{'loss': 0.3766, 'learning_rate': 0.00018627272727272725, 'epoch': 1.05}
{'loss': 0.3875, 'learning_rate': 0.000186, 'epoch': 1.06}
{'loss': 0.357, 'learning_rate': 0.00018572727272727274, 'epoch': 1.06}
{'eval_loss': 0.41894829273223877, 'eval_runtime': 277.7907, 'eval_samples_per_second': 28.287, 'eval_steps_per_second': 0.886, 'epoch': 1.06}
 43%|█████████████████████████████████████████████▌                                                           | 520/1200 [5:30:10<5:23:40, 28.56s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3835, 'learning_rate': 0.00018545454545454543, 'epoch': 1.06}
{'loss': 0.3936, 'learning_rate': 0.00018518181818181817, 'epoch': 1.06}
{'loss': 0.4017, 'learning_rate': 0.00018490909090909088, 'epoch': 1.06}
{'loss': 0.3755, 'learning_rate': 0.00018463636363636363, 'epoch': 1.07}
{'loss': 0.3846, 'learning_rate': 0.00018436363636363634, 'epoch': 1.07}
{'loss': 0.3715, 'learning_rate': 0.00018409090909090909, 'epoch': 1.07}
{'loss': 0.3641, 'learning_rate': 0.00018381818181818177, 'epoch': 1.07}
{'loss': 0.3929, 'learning_rate': 0.00018354545454545452, 'epoch': 1.07}
{'loss': 0.3787, 'learning_rate': 0.00018327272727272726, 'epoch': 1.08}
{'loss': 0.3624, 'learning_rate': 0.00018299999999999998, 'epoch': 1.08}
{'loss': 0.3482, 'learning_rate': 0.00018272727272727272, 'epoch': 1.08}
{'loss': 0.3468, 'learning_rate': 0.00018245454545454543, 'epoch': 1.08}
{'loss': 0.3241, 'learning_rate': 0.00018218181818181818, 'epoch': 1.08}
{'loss': 0.3358, 'learning_rate': 0.0001819090909090909, 'epoch': 1.09}
{'loss': 0.3123, 'learning_rate': 0.00018163636363636364, 'epoch': 1.09}
{'loss': 0.3233, 'learning_rate': 0.00018136363636363632, 'epoch': 1.09}
{'loss': 0.2984, 'learning_rate': 0.00018109090909090907, 'epoch': 1.09}
{'loss': 0.2986, 'learning_rate': 0.0001808181818181818, 'epoch': 1.1}
{'loss': 0.2839, 'learning_rate': 0.00018054545454545453, 'epoch': 1.1}
{'loss': 0.2707, 'learning_rate': 0.00018027272727272727, 'epoch': 1.1}
{'loss': 0.2885, 'learning_rate': 0.00017999999999999998, 'epoch': 1.1}
{'loss': 0.4951, 'learning_rate': 0.00017972727272727273, 'epoch': 1.1}
{'loss': 0.5419, 'learning_rate': 0.00017945454545454542, 'epoch': 1.11}
{'loss': 0.5058, 'learning_rate': 0.00017918181818181816, 'epoch': 1.11}
{'loss': 0.4914, 'learning_rate': 0.00017890909090909087, 'epoch': 1.11}
{'loss': 0.4733, 'learning_rate': 0.00017863636363636362, 'epoch': 1.11}
{'loss': 0.4669, 'learning_rate': 0.00017836363636363636, 'epoch': 1.11}
{'loss': 0.4514, 'learning_rate': 0.00017809090909090908, 'epoch': 1.12}
{'loss': 0.456, 'learning_rate': 0.00017781818181818182, 'epoch': 1.12}
{'loss': 0.4318, 'learning_rate': 0.00017754545454545453, 'epoch': 1.12}
{'loss': 0.4203, 'learning_rate': 0.00017727272727272728, 'epoch': 1.12}
{'loss': 0.4255, 'learning_rate': 0.00017699999999999997, 'epoch': 1.12}
{'loss': 0.421, 'learning_rate': 0.0001767272727272727, 'epoch': 1.13}
{'loss': 0.426, 'learning_rate': 0.00017645454545454542, 'epoch': 1.13}
{'loss': 0.4279, 'learning_rate': 0.00017618181818181817, 'epoch': 1.13}
{'loss': 0.4157, 'learning_rate': 0.00017590909090909088, 'epoch': 1.13}
{'loss': 0.4044, 'learning_rate': 0.00017563636363636363, 'epoch': 1.13}
{'loss': 0.4082, 'learning_rate': 0.00017536363636363637, 'epoch': 1.14}
{'loss': 0.3911, 'learning_rate': 0.00017509090909090908, 'epoch': 1.14}
{'loss': 0.3886, 'learning_rate': 0.0001748181818181818, 'epoch': 1.14}
{'eval_loss': 0.41783884167671204, 'eval_runtime': 389.155, 'eval_samples_per_second': 20.192, 'eval_steps_per_second': 0.632, 'epoch': 1.14}
 47%|█████████████████████████████████████████████████                                                        | 560/1200 [5:57:40<5:35:24, 31.44s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/CodeLlama-7b-hf/resolve/main/config.json (C
aused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f00801c4d60>: Failed to resolve \'huggingface.co\' ([Errno -3] Tempora
ry failure in name resolution)"))'), '(Request ID: 04610870-31d0-458b-8913-9ecc0a90b346)') - silently ignoring the lookup for the file config.json in
 meta-llama/CodeLlama-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/CodeLlama-7b
-hf - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/CodeLlama-7b-hf/resolve/main/config.json (
Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f0049cbda80>: Failed to resolve \'huggingface.co\' ([Errno -3] Tempor
ary failure in name resolution)"))'), '(Request ID: 6f6fae52-a7c0-49c8-97c6-07d99e78d86f)') - silently ignoring the lookup for the file config.json i
n meta-llama/CodeLlama-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4136, 'learning_rate': 0.00017454545454545452, 'epoch': 1.14}
{'loss': 0.3954, 'learning_rate': 0.00017427272727272726, 'epoch': 1.14}
{'loss': 0.3924, 'learning_rate': 0.00017399999999999997, 'epoch': 1.15}
{'loss': 0.3956, 'learning_rate': 0.00017372727272727272, 'epoch': 1.15}
{'loss': 0.4048, 'learning_rate': 0.00017345454545454543, 'epoch': 1.15}
{'loss': 0.3946, 'learning_rate': 0.00017318181818181818, 'epoch': 1.15}
{'loss': 0.3848, 'learning_rate': 0.00017290909090909092, 'epoch': 1.15}
{'loss': 0.3745, 'learning_rate': 0.0001726363636363636, 'epoch': 1.16}
{'loss': 0.3898, 'learning_rate': 0.00017236363636363635, 'epoch': 1.16}
{'loss': 0.3925, 'learning_rate': 0.00017209090909090907, 'epoch': 1.16}
{'loss': 0.3726, 'learning_rate': 0.0001718181818181818, 'epoch': 1.16}
{'loss': 0.3905, 'learning_rate': 0.00017154545454545452, 'epoch': 1.16}
{'loss': 0.3679, 'learning_rate': 0.00017127272727272727, 'epoch': 1.17}
{'loss': 0.3748, 'learning_rate': 0.00017099999999999998, 'epoch': 1.17}
{'loss': 0.3735, 'learning_rate': 0.00017072727272727273, 'epoch': 1.17}
{'loss': 0.3746, 'learning_rate': 0.00017045454545454547, 'epoch': 1.17}
{'loss': 0.3647, 'learning_rate': 0.00017018181818181816, 'epoch': 1.17}
{'loss': 0.3759, 'learning_rate': 0.0001699090909090909, 'epoch': 1.18}
{'loss': 0.3695, 'learning_rate': 0.00016963636363636362, 'epoch': 1.18}
{'loss': 0.3595, 'learning_rate': 0.00016936363636363636, 'epoch': 1.18}
{'loss': 0.3318, 'learning_rate': 0.00016909090909090907, 'epoch': 1.18}
{'loss': 0.3428, 'learning_rate': 0.00016881818181818182, 'epoch': 1.18}
{'loss': 0.3329, 'learning_rate': 0.0001685454545454545, 'epoch': 1.19}
{'loss': 0.3179, 'learning_rate': 0.00016827272727272725, 'epoch': 1.19}
{'loss': 0.3251, 'learning_rate': 0.000168, 'epoch': 1.19}
{'loss': 0.3142, 'learning_rate': 0.0001677272727272727, 'epoch': 1.19}
{'loss': 0.2981, 'learning_rate': 0.00016745454545454545, 'epoch': 1.19}
{'loss': 0.3111, 'learning_rate': 0.00016718181818181816, 'epoch': 1.2}
{'loss': 0.3015, 'learning_rate': 0.0001669090909090909, 'epoch': 1.2}
{'loss': 0.2976, 'learning_rate': 0.00016663636363636362, 'epoch': 1.2}
{'loss': 0.2942, 'learning_rate': 0.00016636363636363637, 'epoch': 1.2}
{'loss': 0.496, 'learning_rate': 0.00016609090909090905, 'epoch': 1.21}
{'loss': 0.5277, 'learning_rate': 0.0001658181818181818, 'epoch': 1.21}
{'loss': 0.5102, 'learning_rate': 0.0001655454545454545, 'epoch': 1.21}
{'loss': 0.4857, 'learning_rate': 0.00016527272727272726, 'epoch': 1.21}
{'loss': 0.4611, 'learning_rate': 0.000165, 'epoch': 1.21}
{'loss': 0.4544, 'learning_rate': 0.00016472727272727271, 'epoch': 1.22}
{'loss': 0.4343, 'learning_rate': 0.00016445454545454546, 'epoch': 1.22}
{'loss': 0.4266, 'learning_rate': 0.00016418181818181815, 'epoch': 1.22}
{'loss': 0.4339, 'learning_rate': 0.0001639090909090909, 'epoch': 1.22}
{'eval_loss': 0.4166487753391266, 'eval_runtime': 482.8632, 'eval_samples_per_second': 16.274, 'eval_steps_per_second': 0.509, 'epoch': 1.22}
 50%|████████████████████████████████████████████████████▌                                                    | 600/1200 [6:31:30<7:18:44, 43.87s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4133, 'learning_rate': 0.0001636363636363636, 'epoch': 1.22}
{'loss': 0.4189, 'learning_rate': 0.00016336363636363635, 'epoch': 1.23}
{'loss': 0.4262, 'learning_rate': 0.00016309090909090906, 'epoch': 1.23}
{'loss': 0.4021, 'learning_rate': 0.0001628181818181818, 'epoch': 1.23}
{'loss': 0.4401, 'learning_rate': 0.00016254545454545455, 'epoch': 1.23}
{'loss': 0.3994, 'learning_rate': 0.00016227272727272726, 'epoch': 1.23}
{'loss': 0.4108, 'learning_rate': 0.000162, 'epoch': 1.24}
{'loss': 0.4196, 'learning_rate': 0.0001617272727272727, 'epoch': 1.24}
{'loss': 0.4177, 'learning_rate': 0.00016145454545454544, 'epoch': 1.24}
{'loss': 0.3814, 'learning_rate': 0.00016118181818181815, 'epoch': 1.24}
{'loss': 0.3805, 'learning_rate': 0.0001609090909090909, 'epoch': 1.24}
{'loss': 0.3748, 'learning_rate': 0.0001606363636363636, 'epoch': 1.25}
{'loss': 0.3809, 'learning_rate': 0.00016036363636363636, 'epoch': 1.25}
{'loss': 0.4044, 'learning_rate': 0.0001600909090909091, 'epoch': 1.25}
{'loss': 0.381, 'learning_rate': 0.0001598181818181818, 'epoch': 1.25}
{'loss': 0.3896, 'learning_rate': 0.00015954545454545453, 'epoch': 1.25}
{'loss': 0.3895, 'learning_rate': 0.00015927272727272725, 'epoch': 1.26}
{'loss': 0.3902, 'learning_rate': 0.000159, 'epoch': 1.26}
{'loss': 0.364, 'learning_rate': 0.0001587272727272727, 'epoch': 1.26}
{'loss': 0.3876, 'learning_rate': 0.00015845454545454545, 'epoch': 1.26}
{'loss': 0.374, 'learning_rate': 0.00015818181818181816, 'epoch': 1.26}
{'loss': 0.3953, 'learning_rate': 0.0001579090909090909, 'epoch': 1.27}
{'loss': 0.3857, 'learning_rate': 0.00015763636363636365, 'epoch': 1.27}
{'loss': 0.369, 'learning_rate': 0.00015736363636363634, 'epoch': 1.27}
{'loss': 0.377, 'learning_rate': 0.00015709090909090908, 'epoch': 1.27}
{'loss': 0.3725, 'learning_rate': 0.0001568181818181818, 'epoch': 1.27}
{'loss': 0.3752, 'learning_rate': 0.00015654545454545454, 'epoch': 1.28}
{'loss': 0.3541, 'learning_rate': 0.00015627272727272725, 'epoch': 1.28}
{'loss': 0.3495, 'learning_rate': 0.000156, 'epoch': 1.28}
{'loss': 0.3644, 'learning_rate': 0.0001557272727272727, 'epoch': 1.28}
{'loss': 0.3677, 'learning_rate': 0.00015545454545454546, 'epoch': 1.28}
{'loss': 0.3388, 'learning_rate': 0.00015518181818181814, 'epoch': 1.29}
{'loss': 0.3267, 'learning_rate': 0.0001549090909090909, 'epoch': 1.29}
{'loss': 0.3373, 'learning_rate': 0.00015463636363636363, 'epoch': 1.29}
{'loss': 0.3076, 'learning_rate': 0.00015436363636363635, 'epoch': 1.29}
{'loss': 0.3127, 'learning_rate': 0.0001540909090909091, 'epoch': 1.29}
{'loss': 0.3034, 'learning_rate': 0.0001538181818181818, 'epoch': 1.3}
{'loss': 0.3023, 'learning_rate': 0.00015354545454545455, 'epoch': 1.3}
{'loss': 0.2915, 'learning_rate': 0.00015327272727272724, 'epoch': 1.3}
{'loss': 0.2791, 'learning_rate': 0.00015299999999999998, 'epoch': 1.3}
{'eval_loss': 0.4151194095611572, 'eval_runtime': 511.2008, 'eval_samples_per_second': 15.372, 'eval_steps_per_second': 0.481, 'epoch': 1.3}
 53%|████████████████████████████████████████████████████████                                                 | 640/1200 [7:04:34<5:29:51, 35.34s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2736, 'learning_rate': 0.0001527272727272727, 'epoch': 1.3}
{'loss': 0.4924, 'learning_rate': 0.00015245454545454544, 'epoch': 1.31}
{'loss': 0.5227, 'learning_rate': 0.00015218181818181818, 'epoch': 1.31}
{'loss': 0.4905, 'learning_rate': 0.0001519090909090909, 'epoch': 1.31}
{'loss': 0.4806, 'learning_rate': 0.00015163636363636364, 'epoch': 1.31}
{'loss': 0.4643, 'learning_rate': 0.00015136363636363635, 'epoch': 1.32}
{'loss': 0.4444, 'learning_rate': 0.0001510909090909091, 'epoch': 1.32}
{'loss': 0.4496, 'learning_rate': 0.00015081818181818179, 'epoch': 1.32}
{'loss': 0.4367, 'learning_rate': 0.00015054545454545453, 'epoch': 1.32}
{'loss': 0.4229, 'learning_rate': 0.00015027272727272724, 'epoch': 1.32}
{'loss': 0.4182, 'learning_rate': 0.00015, 'epoch': 1.33}
{'loss': 0.4165, 'learning_rate': 0.0001497272727272727, 'epoch': 1.33}
{'loss': 0.4203, 'learning_rate': 0.00014945454545454545, 'epoch': 1.33}
{'loss': 0.4154, 'learning_rate': 0.00014918181818181816, 'epoch': 1.33}
{'loss': 0.3883, 'learning_rate': 0.00014890909090909088, 'epoch': 1.33}
{'loss': 0.4183, 'learning_rate': 0.00014863636363636362, 'epoch': 1.34}
{'loss': 0.3997, 'learning_rate': 0.00014836363636363636, 'epoch': 1.34}
{'loss': 0.4221, 'learning_rate': 0.00014809090909090908, 'epoch': 1.34}
{'loss': 0.3944, 'learning_rate': 0.00014781818181818182, 'epoch': 1.34}
{'loss': 0.4075, 'learning_rate': 0.00014754545454545454, 'epoch': 1.34}
{'loss': 0.3745, 'learning_rate': 0.00014727272727272725, 'epoch': 1.35}
{'loss': 0.3876, 'learning_rate': 0.000147, 'epoch': 1.35}
{'loss': 0.3825, 'learning_rate': 0.0001467272727272727, 'epoch': 1.35}
{'loss': 0.3992, 'learning_rate': 0.00014645454545454543, 'epoch': 1.35}
{'loss': 0.4179, 'learning_rate': 0.00014618181818181817, 'epoch': 1.35}
{'loss': 0.3849, 'learning_rate': 0.0001459090909090909, 'epoch': 1.36}
{'loss': 0.3798, 'learning_rate': 0.00014563636363636363, 'epoch': 1.36}
{'loss': 0.3876, 'learning_rate': 0.00014536363636363634, 'epoch': 1.36}
{'loss': 0.3555, 'learning_rate': 0.0001450909090909091, 'epoch': 1.36}
{'loss': 0.369, 'learning_rate': 0.0001448181818181818, 'epoch': 1.36}
{'loss': 0.3956, 'learning_rate': 0.00014454545454545452, 'epoch': 1.37}
{'loss': 0.3718, 'learning_rate': 0.00014427272727272726, 'epoch': 1.37}
{'loss': 0.3826, 'learning_rate': 0.00014399999999999998, 'epoch': 1.37}
{'loss': 0.3733, 'learning_rate': 0.00014372727272727272, 'epoch': 1.37}
{'loss': 0.3675, 'learning_rate': 0.00014345454545454546, 'epoch': 1.37}
{'loss': 0.3807, 'learning_rate': 0.00014318181818181818, 'epoch': 1.38}
{'loss': 0.3755, 'learning_rate': 0.0001429090909090909, 'epoch': 1.38}
{'loss': 0.3609, 'learning_rate': 0.00014263636363636364, 'epoch': 1.38}
{'loss': 0.3575, 'learning_rate': 0.00014236363636363635, 'epoch': 1.38}
{'loss': 0.3455, 'learning_rate': 0.00014209090909090907, 'epoch': 1.38}
{'eval_loss': 0.40908393263816833, 'eval_runtime': 547.5203, 'eval_samples_per_second': 14.352, 'eval_steps_per_second': 0.449, 'epoch': 1.38}
 57%|███████████████████████████████████████████████████████████▌                                             | 680/1200 [7:41:20<5:13:30, 36.17s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3335, 'learning_rate': 0.0001418181818181818, 'epoch': 1.39}
{'loss': 0.3352, 'learning_rate': 0.00014154545454545453, 'epoch': 1.39}
{'loss': 0.3272, 'learning_rate': 0.00014127272727272724, 'epoch': 1.39}
{'loss': 0.3123, 'learning_rate': 0.00014099999999999998, 'epoch': 1.39}
{'loss': 0.3274, 'learning_rate': 0.00014072727272727273, 'epoch': 1.39}
{'loss': 0.3026, 'learning_rate': 0.00014045454545454544, 'epoch': 1.4}
{'loss': 0.3031, 'learning_rate': 0.00014018181818181819, 'epoch': 1.4}
{'loss': 0.3019, 'learning_rate': 0.0001399090909090909, 'epoch': 1.4}
{'loss': 0.2917, 'learning_rate': 0.00013963636363636362, 'epoch': 1.4}
{'loss': 0.2925, 'learning_rate': 0.00013936363636363636, 'epoch': 1.4}
{'loss': 0.2834, 'learning_rate': 0.00013909090909090908, 'epoch': 1.41}
{'loss': 0.4789, 'learning_rate': 0.0001388181818181818, 'epoch': 1.41}
{'loss': 0.5229, 'learning_rate': 0.00013854545454545453, 'epoch': 1.41}
{'loss': 0.4922, 'learning_rate': 0.00013827272727272728, 'epoch': 1.41}
{'loss': 0.4835, 'learning_rate': 0.000138, 'epoch': 1.41}
{'loss': 0.4628, 'learning_rate': 0.0001377272727272727, 'epoch': 1.42}
{'loss': 0.4584, 'learning_rate': 0.00013745454545454545, 'epoch': 1.42}
{'loss': 0.453, 'learning_rate': 0.00013718181818181817, 'epoch': 1.42}
{'loss': 0.4345, 'learning_rate': 0.00013690909090909088, 'epoch': 1.42}
{'loss': 0.4042, 'learning_rate': 0.00013663636363636363, 'epoch': 1.42}
{'loss': 0.4256, 'learning_rate': 0.00013636363636363634, 'epoch': 1.43}
{'loss': 0.4052, 'learning_rate': 0.00013609090909090908, 'epoch': 1.43}
{'loss': 0.4144, 'learning_rate': 0.0001358181818181818, 'epoch': 1.43}
{'loss': 0.4045, 'learning_rate': 0.00013554545454545454, 'epoch': 1.43}
{'loss': 0.4287, 'learning_rate': 0.00013527272727272726, 'epoch': 1.44}
{'loss': 0.4014, 'learning_rate': 0.000135, 'epoch': 1.44}
{'loss': 0.4346, 'learning_rate': 0.00013472727272727272, 'epoch': 1.44}
{'loss': 0.3817, 'learning_rate': 0.00013445454545454543, 'epoch': 1.44}
{'loss': 0.3852, 'learning_rate': 0.00013418181818181818, 'epoch': 1.44}
{'loss': 0.414, 'learning_rate': 0.0001339090909090909, 'epoch': 1.45}
{'loss': 0.384, 'learning_rate': 0.0001336363636363636, 'epoch': 1.45}
{'loss': 0.4207, 'learning_rate': 0.00013336363636363635, 'epoch': 1.45}
{'loss': 0.3712, 'learning_rate': 0.0001330909090909091, 'epoch': 1.45}
{'loss': 0.3841, 'learning_rate': 0.0001328181818181818, 'epoch': 1.45}
{'loss': 0.3805, 'learning_rate': 0.00013254545454545452, 'epoch': 1.46}
{'loss': 0.3865, 'learning_rate': 0.00013227272727272727, 'epoch': 1.46}
{'loss': 0.3683, 'learning_rate': 0.00013199999999999998, 'epoch': 1.46}
{'loss': 0.3689, 'learning_rate': 0.00013172727272727273, 'epoch': 1.46}
{'loss': 0.3802, 'learning_rate': 0.00013145454545454544, 'epoch': 1.46}
{'loss': 0.3604, 'learning_rate': 0.00013118181818181816, 'epoch': 1.47}
{'eval_loss': 0.4084075391292572, 'eval_runtime': 503.8265, 'eval_samples_per_second': 15.597, 'eval_steps_per_second': 0.488, 'epoch': 1.47}
 60%|███████████████████████████████████████████████████████████████                                          | 720/1200 [8:15:13<4:50:48, 36.35s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3822, 'learning_rate': 0.0001309090909090909, 'epoch': 1.47}
{'loss': 0.3606, 'learning_rate': 0.00013063636363636362, 'epoch': 1.47}
{'loss': 0.3659, 'learning_rate': 0.00013036363636363636, 'epoch': 1.47}
{'loss': 0.374, 'learning_rate': 0.00013009090909090907, 'epoch': 1.47}
{'loss': 0.3846, 'learning_rate': 0.00012981818181818182, 'epoch': 1.48}
{'loss': 0.3709, 'learning_rate': 0.00012954545454545453, 'epoch': 1.48}
{'loss': 0.3692, 'learning_rate': 0.00012927272727272725, 'epoch': 1.48}
{'loss': 0.3605, 'learning_rate': 0.000129, 'epoch': 1.48}
{'loss': 0.371, 'learning_rate': 0.0001287272727272727, 'epoch': 1.48}
{'loss': 0.3441, 'learning_rate': 0.00012845454545454545, 'epoch': 1.49}
{'loss': 0.3364, 'learning_rate': 0.00012818181818181817, 'epoch': 1.49}
{'loss': 0.3346, 'learning_rate': 0.0001279090909090909, 'epoch': 1.49}
{'loss': 0.3409, 'learning_rate': 0.00012763636363636362, 'epoch': 1.49}
{'loss': 0.3138, 'learning_rate': 0.00012736363636363637, 'epoch': 1.49}
{'loss': 0.3195, 'learning_rate': 0.00012709090909090908, 'epoch': 1.5}
{'loss': 0.3069, 'learning_rate': 0.0001268181818181818, 'epoch': 1.5}
{'loss': 0.3041, 'learning_rate': 0.00012654545454545454, 'epoch': 1.5}
{'loss': 0.2897, 'learning_rate': 0.00012627272727272726, 'epoch': 1.5}
{'loss': 0.2844, 'learning_rate': 0.00012599999999999997, 'epoch': 1.5}
{'loss': 0.2823, 'learning_rate': 0.00012572727272727272, 'epoch': 1.51}
{'loss': 0.2687, 'learning_rate': 0.00012545454545454543, 'epoch': 1.51}
{'loss': 0.4702, 'learning_rate': 0.00012518181818181817, 'epoch': 1.51}
{'loss': 0.5031, 'learning_rate': 0.0001249090909090909, 'epoch': 1.51}
{'loss': 0.4799, 'learning_rate': 0.00012463636363636363, 'epoch': 1.51}
{'loss': 0.4482, 'learning_rate': 0.00012436363636363635, 'epoch': 1.52}
{'loss': 0.4569, 'learning_rate': 0.0001240909090909091, 'epoch': 1.52}
{'loss': 0.4448, 'learning_rate': 0.0001238181818181818, 'epoch': 1.52}
{'loss': 0.4494, 'learning_rate': 0.00012354545454545452, 'epoch': 1.52}
{'loss': 0.4313, 'learning_rate': 0.00012327272727272727, 'epoch': 1.52}
{'loss': 0.4119, 'learning_rate': 0.00012299999999999998, 'epoch': 1.53}
{'loss': 0.4096, 'learning_rate': 0.00012272727272727272, 'epoch': 1.53}
{'loss': 0.3952, 'learning_rate': 0.00012245454545454544, 'epoch': 1.53}
{'loss': 0.4129, 'learning_rate': 0.00012218181818181818, 'epoch': 1.53}
{'loss': 0.3899, 'learning_rate': 0.0001219090909090909, 'epoch': 1.53}
{'loss': 0.4127, 'learning_rate': 0.00012163636363636363, 'epoch': 1.54}
{'loss': 0.4192, 'learning_rate': 0.00012136363636363636, 'epoch': 1.54}
{'loss': 0.3982, 'learning_rate': 0.00012109090909090907, 'epoch': 1.54}
{'loss': 0.385, 'learning_rate': 0.0001208181818181818, 'epoch': 1.54}
{'loss': 0.3872, 'learning_rate': 0.00012054545454545453, 'epoch': 1.55}
{'loss': 0.3972, 'learning_rate': 0.00012027272727272726, 'epoch': 1.55}
{'eval_loss': 0.40738022327423096, 'eval_runtime': 484.3874, 'eval_samples_per_second': 16.223, 'eval_steps_per_second': 0.508, 'epoch': 1.55}
 63%|██████████████████████████████████████████████████████████████████▌                                      | 760/1200 [8:50:01<4:47:51, 39.25s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3834, 'learning_rate': 0.00011999999999999999, 'epoch': 1.55}
{'loss': 0.3829, 'learning_rate': 0.00011972727272727272, 'epoch': 1.55}
{'loss': 0.3864, 'learning_rate': 0.00011945454545454545, 'epoch': 1.55}
{'loss': 0.3645, 'learning_rate': 0.00011918181818181818, 'epoch': 1.56}
{'loss': 0.3932, 'learning_rate': 0.00011890909090909089, 'epoch': 1.56}
{'loss': 0.392, 'learning_rate': 0.00011863636363636362, 'epoch': 1.56}
{'loss': 0.3846, 'learning_rate': 0.00011836363636363635, 'epoch': 1.56}
{'loss': 0.3912, 'learning_rate': 0.00011809090909090908, 'epoch': 1.56}
{'loss': 0.3835, 'learning_rate': 0.0001178181818181818, 'epoch': 1.57}
{'loss': 0.3732, 'learning_rate': 0.00011754545454545454, 'epoch': 1.57}
{'loss': 0.3817, 'learning_rate': 0.00011727272727272727, 'epoch': 1.57}
{'loss': 0.3722, 'learning_rate': 0.000117, 'epoch': 1.57}
{'loss': 0.3692, 'learning_rate': 0.00011672727272727271, 'epoch': 1.57}
{'loss': 0.3575, 'learning_rate': 0.00011645454545454544, 'epoch': 1.58}
{'loss': 0.3659, 'learning_rate': 0.00011618181818181817, 'epoch': 1.58}
{'loss': 0.3609, 'learning_rate': 0.0001159090909090909, 'epoch': 1.58}
{'loss': 0.3693, 'learning_rate': 0.00011563636363636362, 'epoch': 1.58}
{'loss': 0.3701, 'learning_rate': 0.00011536363636363635, 'epoch': 1.58}
{'loss': 0.3621, 'learning_rate': 0.00011509090909090908, 'epoch': 1.59}
{'loss': 0.3461, 'learning_rate': 0.00011481818181818182, 'epoch': 1.59}
{'loss': 0.3327, 'learning_rate': 0.00011454545454545453, 'epoch': 1.59}
{'loss': 0.3252, 'learning_rate': 0.00011427272727272726, 'epoch': 1.59}
{'loss': 0.3278, 'learning_rate': 0.00011399999999999999, 'epoch': 1.59}
{'loss': 0.3307, 'learning_rate': 0.00011372727272727272, 'epoch': 1.6}
{'loss': 0.302, 'learning_rate': 0.00011345454545454544, 'epoch': 1.6}
{'loss': 0.3171, 'learning_rate': 0.00011318181818181817, 'epoch': 1.6}
{'loss': 0.2992, 'learning_rate': 0.0001129090909090909, 'epoch': 1.6}
{'loss': 0.3034, 'learning_rate': 0.00011263636363636363, 'epoch': 1.6}
{'loss': 0.296, 'learning_rate': 0.00011236363636363635, 'epoch': 1.61}
{'loss': 0.2846, 'learning_rate': 0.00011209090909090908, 'epoch': 1.61}
{'loss': 0.2708, 'learning_rate': 0.00011181818181818181, 'epoch': 1.61}
{'loss': 0.4692, 'learning_rate': 0.00011154545454545454, 'epoch': 1.61}
{'loss': 0.5068, 'learning_rate': 0.00011127272727272726, 'epoch': 1.61}
{'loss': 0.5148, 'learning_rate': 0.00011099999999999999, 'epoch': 1.62}
{'loss': 0.4817, 'learning_rate': 0.00011072727272727272, 'epoch': 1.62}
{'loss': 0.463, 'learning_rate': 0.00011045454545454545, 'epoch': 1.62}
{'loss': 0.4511, 'learning_rate': 0.00011018181818181816, 'epoch': 1.62}
{'loss': 0.4284, 'learning_rate': 0.0001099090909090909, 'epoch': 1.62}
{'loss': 0.4356, 'learning_rate': 0.00010963636363636363, 'epoch': 1.63}
{'loss': 0.4173, 'learning_rate': 0.00010936363636363636, 'epoch': 1.63}
{'eval_loss': 0.40682679414749146, 'eval_runtime': 485.7228, 'eval_samples_per_second': 16.178, 'eval_steps_per_second': 0.506, 'epoch': 1.63}
 67%|██████████████████████████████████████████████████████████████████████                                   | 800/1200 [9:23:30<4:09:08, 37.37s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4295, 'learning_rate': 0.00010909090909090908, 'epoch': 1.63}
{'loss': 0.4099, 'learning_rate': 0.00010881818181818181, 'epoch': 1.63}
{'loss': 0.4039, 'learning_rate': 0.00010854545454545454, 'epoch': 1.63}
{'loss': 0.4108, 'learning_rate': 0.00010827272727272727, 'epoch': 1.64}
{'loss': 0.4022, 'learning_rate': 0.00010799999999999998, 'epoch': 1.64}
{'loss': 0.4188, 'learning_rate': 0.00010772727272727271, 'epoch': 1.64}
{'loss': 0.4096, 'learning_rate': 0.00010745454545454544, 'epoch': 1.64}
{'loss': 0.4115, 'learning_rate': 0.00010718181818181818, 'epoch': 1.64}
{'loss': 0.3779, 'learning_rate': 0.0001069090909090909, 'epoch': 1.65}
{'loss': 0.3956, 'learning_rate': 0.00010663636363636363, 'epoch': 1.65}
{'loss': 0.3813, 'learning_rate': 0.00010636363636363636, 'epoch': 1.65}
{'loss': 0.3692, 'learning_rate': 0.00010609090909090909, 'epoch': 1.65}
{'loss': 0.3826, 'learning_rate': 0.0001058181818181818, 'epoch': 1.65}
{'loss': 0.3875, 'learning_rate': 0.00010554545454545453, 'epoch': 1.66}
{'loss': 0.3696, 'learning_rate': 0.00010527272727272726, 'epoch': 1.66}
{'loss': 0.3818, 'learning_rate': 0.00010499999999999999, 'epoch': 1.66}
{'loss': 0.3784, 'learning_rate': 0.00010472727272727272, 'epoch': 1.66}
{'loss': 0.3782, 'learning_rate': 0.00010445454545454545, 'epoch': 1.67}
{'loss': 0.3729, 'learning_rate': 0.00010418181818181818, 'epoch': 1.67}
{'loss': 0.3597, 'learning_rate': 0.00010390909090909091, 'epoch': 1.67}
{'loss': 0.3543, 'learning_rate': 0.00010363636363636362, 'epoch': 1.67}
{'loss': 0.3633, 'learning_rate': 0.00010336363636363635, 'epoch': 1.67}
{'loss': 0.3676, 'learning_rate': 0.00010309090909090908, 'epoch': 1.68}
{'loss': 0.3586, 'learning_rate': 0.00010281818181818181, 'epoch': 1.68}
{'loss': 0.3707, 'learning_rate': 0.00010254545454545453, 'epoch': 1.68}
{'loss': 0.3736, 'learning_rate': 0.00010227272727272726, 'epoch': 1.68}
{'loss': 0.3567, 'learning_rate': 0.000102, 'epoch': 1.68}
{'loss': 0.3506, 'learning_rate': 0.00010172727272727273, 'epoch': 1.69}
{'loss': 0.3618, 'learning_rate': 0.00010145454545454544, 'epoch': 1.69}
{'loss': 0.3518, 'learning_rate': 0.00010118181818181817, 'epoch': 1.69}
{'loss': 0.3496, 'learning_rate': 0.0001009090909090909, 'epoch': 1.69}
{'loss': 0.3347, 'learning_rate': 0.00010063636363636363, 'epoch': 1.69}
{'loss': 0.325, 'learning_rate': 0.00010036363636363635, 'epoch': 1.7}
{'loss': 0.3229, 'learning_rate': 0.00010009090909090908, 'epoch': 1.7}
{'loss': 0.3287, 'learning_rate': 9.98181818181818e-05, 'epoch': 1.7}
{'loss': 0.3096, 'learning_rate': 9.954545454545455e-05, 'epoch': 1.7}
{'loss': 0.2982, 'learning_rate': 9.927272727272726e-05, 'epoch': 1.7}
{'loss': 0.306, 'learning_rate': 9.9e-05, 'epoch': 1.71}
{'loss': 0.2924, 'learning_rate': 9.872727272727272e-05, 'epoch': 1.71}
{'loss': 0.2897, 'learning_rate': 9.845454545454545e-05, 'epoch': 1.71}
{'eval_loss': 0.40561699867248535, 'eval_runtime': 513.1777, 'eval_samples_per_second': 15.312, 'eval_steps_per_second': 0.479, 'epoch': 1.71}
 70%|█████████████████████████████████████████████████████████████████████████▌                               | 840/1200 [9:57:08<3:18:23, 33.07s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2944, 'learning_rate': 9.818181818181817e-05, 'epoch': 1.71}
{'loss': 0.4849, 'learning_rate': 9.79090909090909e-05, 'epoch': 1.71}
{'loss': 0.4881, 'learning_rate': 9.763636363636363e-05, 'epoch': 1.72}
{'loss': 0.4774, 'learning_rate': 9.736363636363634e-05, 'epoch': 1.72}
{'loss': 0.4637, 'learning_rate': 9.709090909090907e-05, 'epoch': 1.72}
{'loss': 0.428, 'learning_rate': 9.681818181818181e-05, 'epoch': 1.72}
{'loss': 0.4586, 'learning_rate': 9.654545454545454e-05, 'epoch': 1.72}
{'loss': 0.4268, 'learning_rate': 9.627272727272727e-05, 'epoch': 1.73}
{'loss': 0.4077, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.73}
{'loss': 0.4032, 'learning_rate': 9.572727272727272e-05, 'epoch': 1.73}
{'loss': 0.4397, 'learning_rate': 9.545454545454545e-05, 'epoch': 1.73}
{'loss': 0.4007, 'learning_rate': 9.518181818181818e-05, 'epoch': 1.73}
{'loss': 0.4049, 'learning_rate': 9.490909090909089e-05, 'epoch': 1.74}
{'loss': 0.4048, 'learning_rate': 9.463636363636362e-05, 'epoch': 1.74}
{'loss': 0.4155, 'learning_rate': 9.436363636363636e-05, 'epoch': 1.74}
{'loss': 0.4099, 'learning_rate': 9.40909090909091e-05, 'epoch': 1.74}
{'loss': 0.4065, 'learning_rate': 9.381818181818181e-05, 'epoch': 1.74}
{'loss': 0.3796, 'learning_rate': 9.354545454545454e-05, 'epoch': 1.75}
{'loss': 0.4044, 'learning_rate': 9.327272727272727e-05, 'epoch': 1.75}
{'loss': 0.3879, 'learning_rate': 9.3e-05, 'epoch': 1.75}
{'loss': 0.3955, 'learning_rate': 9.272727272727271e-05, 'epoch': 1.75}
{'loss': 0.3719, 'learning_rate': 9.245454545454544e-05, 'epoch': 1.75}
{'loss': 0.3788, 'learning_rate': 9.218181818181817e-05, 'epoch': 1.76}
{'loss': 0.3849, 'learning_rate': 9.190909090909089e-05, 'epoch': 1.76}
{'loss': 0.3801, 'learning_rate': 9.163636363636363e-05, 'epoch': 1.76}
{'loss': 0.3753, 'learning_rate': 9.136363636363636e-05, 'epoch': 1.76}
{'loss': 0.3675, 'learning_rate': 9.109090909090909e-05, 'epoch': 1.76}
{'loss': 0.391, 'learning_rate': 9.081818181818182e-05, 'epoch': 1.77}
{'loss': 0.3756, 'learning_rate': 9.054545454545453e-05, 'epoch': 1.77}
{'loss': 0.3711, 'learning_rate': 9.027272727272726e-05, 'epoch': 1.77}
{'loss': 0.3854, 'learning_rate': 8.999999999999999e-05, 'epoch': 1.77}
{'loss': 0.3667, 'learning_rate': 8.972727272727271e-05, 'epoch': 1.78}
{'loss': 0.3704, 'learning_rate': 8.945454545454544e-05, 'epoch': 1.78}
{'loss': 0.3762, 'learning_rate': 8.918181818181818e-05, 'epoch': 1.78}
{'loss': 0.3596, 'learning_rate': 8.890909090909091e-05, 'epoch': 1.78}
{'loss': 0.3752, 'learning_rate': 8.863636363636364e-05, 'epoch': 1.78}
{'loss': 0.3743, 'learning_rate': 8.836363636363635e-05, 'epoch': 1.79}
{'loss': 0.3553, 'learning_rate': 8.809090909090908e-05, 'epoch': 1.79}
{'loss': 0.3453, 'learning_rate': 8.781818181818181e-05, 'epoch': 1.79}
{'loss': 0.3498, 'learning_rate': 8.754545454545454e-05, 'epoch': 1.79}
{'eval_loss': 0.40140777826309204, 'eval_runtime': 509.7405, 'eval_samples_per_second': 15.416, 'eval_steps_per_second': 0.483, 'epoch': 1.79}
 73%|████████████████████████████████████████████████████████████████████████████▎                           | 880/1200 [10:32:28<2:51:06, 32.08s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3519, 'learning_rate': 8.727272727272726e-05, 'epoch': 1.79}
{'loss': 0.3279, 'learning_rate': 8.699999999999999e-05, 'epoch': 1.8}
{'loss': 0.3111, 'learning_rate': 8.672727272727272e-05, 'epoch': 1.8}
{'loss': 0.3243, 'learning_rate': 8.645454545454546e-05, 'epoch': 1.8}
{'loss': 0.3169, 'learning_rate': 8.618181818181817e-05, 'epoch': 1.8}
{'loss': 0.3016, 'learning_rate': 8.59090909090909e-05, 'epoch': 1.8}
{'loss': 0.3031, 'learning_rate': 8.563636363636363e-05, 'epoch': 1.81}
{'loss': 0.2994, 'learning_rate': 8.536363636363636e-05, 'epoch': 1.81}
{'loss': 0.2901, 'learning_rate': 8.509090909090908e-05, 'epoch': 1.81}
{'loss': 0.2805, 'learning_rate': 8.481818181818181e-05, 'epoch': 1.81}
{'loss': 0.2833, 'learning_rate': 8.454545454545454e-05, 'epoch': 1.81}
{'loss': 0.4493, 'learning_rate': 8.427272727272725e-05, 'epoch': 1.82}
{'loss': 0.513, 'learning_rate': 8.4e-05, 'epoch': 1.82}
{'loss': 0.4853, 'learning_rate': 8.372727272727272e-05, 'epoch': 1.82}
{'loss': 0.4551, 'learning_rate': 8.345454545454545e-05, 'epoch': 1.82}
{'loss': 0.4337, 'learning_rate': 8.318181818181818e-05, 'epoch': 1.82}
{'loss': 0.4316, 'learning_rate': 8.29090909090909e-05, 'epoch': 1.83}
{'loss': 0.4409, 'learning_rate': 8.263636363636363e-05, 'epoch': 1.83}
{'loss': 0.4356, 'learning_rate': 8.236363636363636e-05, 'epoch': 1.83}
{'loss': 0.412, 'learning_rate': 8.209090909090907e-05, 'epoch': 1.83}
{'loss': 0.3975, 'learning_rate': 8.18181818181818e-05, 'epoch': 1.83}
{'loss': 0.4236, 'learning_rate': 8.154545454545453e-05, 'epoch': 1.84}
{'loss': 0.4026, 'learning_rate': 8.127272727272727e-05, 'epoch': 1.84}
{'loss': 0.4031, 'learning_rate': 8.1e-05, 'epoch': 1.84}
{'loss': 0.393, 'learning_rate': 8.072727272727272e-05, 'epoch': 1.84}
{'loss': 0.3929, 'learning_rate': 8.045454545454545e-05, 'epoch': 1.84}
{'loss': 0.3996, 'learning_rate': 8.018181818181818e-05, 'epoch': 1.85}
{'loss': 0.3891, 'learning_rate': 7.99090909090909e-05, 'epoch': 1.85}
{'loss': 0.3958, 'learning_rate': 7.963636363636362e-05, 'epoch': 1.85}
{'loss': 0.4062, 'learning_rate': 7.936363636363635e-05, 'epoch': 1.85}
{'loss': 0.3899, 'learning_rate': 7.909090909090908e-05, 'epoch': 1.85}
{'loss': 0.3784, 'learning_rate': 7.881818181818182e-05, 'epoch': 1.86}
{'loss': 0.4008, 'learning_rate': 7.854545454545454e-05, 'epoch': 1.86}
{'loss': 0.3539, 'learning_rate': 7.827272727272727e-05, 'epoch': 1.86}
{'loss': 0.3865, 'learning_rate': 7.8e-05, 'epoch': 1.86}
{'loss': 0.3668, 'learning_rate': 7.772727272727273e-05, 'epoch': 1.86}
{'loss': 0.3872, 'learning_rate': 7.745454545454544e-05, 'epoch': 1.87}
{'loss': 0.3902, 'learning_rate': 7.718181818181817e-05, 'epoch': 1.87}
{'loss': 0.3712, 'learning_rate': 7.69090909090909e-05, 'epoch': 1.87}
{'loss': 0.398, 'learning_rate': 7.663636363636362e-05, 'epoch': 1.87}
{'eval_loss': 0.40053462982177734, 'eval_runtime': 477.1079, 'eval_samples_per_second': 16.47, 'eval_steps_per_second': 0.516, 'epoch': 1.87}
 77%|███████████████████████████████████████████████████████████████████████████████▋                        | 920/1200 [11:06:40<2:47:19, 35.85s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.383, 'learning_rate': 7.636363636363635e-05, 'epoch': 1.87}
{'loss': 0.3827, 'learning_rate': 7.609090909090909e-05, 'epoch': 1.88}
{'loss': 0.3869, 'learning_rate': 7.581818181818182e-05, 'epoch': 1.88}
{'loss': 0.3691, 'learning_rate': 7.554545454545455e-05, 'epoch': 1.88}
{'loss': 0.3737, 'learning_rate': 7.527272727272726e-05, 'epoch': 1.88}
{'loss': 0.3605, 'learning_rate': 7.5e-05, 'epoch': 1.88}
{'loss': 0.3618, 'learning_rate': 7.472727272727272e-05, 'epoch': 1.89}
{'loss': 0.3579, 'learning_rate': 7.445454545454544e-05, 'epoch': 1.89}
{'loss': 0.3571, 'learning_rate': 7.418181818181818e-05, 'epoch': 1.89}
{'loss': 0.3539, 'learning_rate': 7.390909090909091e-05, 'epoch': 1.89}
{'loss': 0.3314, 'learning_rate': 7.363636363636363e-05, 'epoch': 1.9}
{'loss': 0.3439, 'learning_rate': 7.336363636363636e-05, 'epoch': 1.9}
{'loss': 0.3132, 'learning_rate': 7.309090909090908e-05, 'epoch': 1.9}
{'loss': 0.3042, 'learning_rate': 7.281818181818181e-05, 'epoch': 1.9}
{'loss': 0.3058, 'learning_rate': 7.254545454545454e-05, 'epoch': 1.9}
{'loss': 0.3033, 'learning_rate': 7.227272727272726e-05, 'epoch': 1.91}
{'loss': 0.3014, 'learning_rate': 7.199999999999999e-05, 'epoch': 1.91}
{'loss': 0.2928, 'learning_rate': 7.172727272727273e-05, 'epoch': 1.91}
{'loss': 0.2843, 'learning_rate': 7.145454545454545e-05, 'epoch': 1.91}
{'loss': 0.2745, 'learning_rate': 7.118181818181818e-05, 'epoch': 1.91}
{'loss': 0.2872, 'learning_rate': 7.09090909090909e-05, 'epoch': 1.92}
{'loss': 0.4383, 'learning_rate': 7.063636363636362e-05, 'epoch': 1.92}
{'loss': 0.5062, 'learning_rate': 7.036363636363636e-05, 'epoch': 1.92}
{'loss': 0.4766, 'learning_rate': 7.009090909090909e-05, 'epoch': 1.92}
{'loss': 0.4491, 'learning_rate': 6.981818181818181e-05, 'epoch': 1.92}
{'loss': 0.4186, 'learning_rate': 6.954545454545454e-05, 'epoch': 1.93}
{'loss': 0.4284, 'learning_rate': 6.927272727272727e-05, 'epoch': 1.93}
{'loss': 0.4096, 'learning_rate': 6.9e-05, 'epoch': 1.93}
{'loss': 0.4066, 'learning_rate': 6.872727272727273e-05, 'epoch': 1.93}
{'loss': 0.4098, 'learning_rate': 6.845454545454544e-05, 'epoch': 1.93}
{'loss': 0.4087, 'learning_rate': 6.818181818181817e-05, 'epoch': 1.94}
{'loss': 0.3992, 'learning_rate': 6.79090909090909e-05, 'epoch': 1.94}
{'loss': 0.4049, 'learning_rate': 6.763636363636363e-05, 'epoch': 1.94}
{'loss': 0.4104, 'learning_rate': 6.736363636363636e-05, 'epoch': 1.94}
{'loss': 0.3587, 'learning_rate': 6.709090909090909e-05, 'epoch': 1.94}
{'loss': 0.3993, 'learning_rate': 6.68181818181818e-05, 'epoch': 1.95}
{'loss': 0.3843, 'learning_rate': 6.654545454545455e-05, 'epoch': 1.95}
{'loss': 0.3785, 'learning_rate': 6.627272727272726e-05, 'epoch': 1.95}
{'loss': 0.3936, 'learning_rate': 6.599999999999999e-05, 'epoch': 1.95}
{'loss': 0.388, 'learning_rate': 6.572727272727272e-05, 'epoch': 1.95}
{'eval_loss': 0.3999573886394501, 'eval_runtime': 502.202, 'eval_samples_per_second': 15.647, 'eval_steps_per_second': 0.49, 'epoch': 1.95}
 80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 960/1200 [11:40:36<2:27:27, 36.86s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3828, 'learning_rate': 6.545454545454545e-05, 'epoch': 1.96}
{'loss': 0.3776, 'learning_rate': 6.518181818181818e-05, 'epoch': 1.96}
{'loss': 0.3813, 'learning_rate': 6.490909090909091e-05, 'epoch': 1.96}
{'loss': 0.3616, 'learning_rate': 6.463636363636362e-05, 'epoch': 1.96}
{'loss': 0.347, 'learning_rate': 6.436363636363635e-05, 'epoch': 1.96}
{'loss': 0.3554, 'learning_rate': 6.409090909090908e-05, 'epoch': 1.97}
{'loss': 0.3699, 'learning_rate': 6.381818181818181e-05, 'epoch': 1.97}
{'loss': 0.3733, 'learning_rate': 6.354545454545454e-05, 'epoch': 1.97}
{'loss': 0.3712, 'learning_rate': 6.327272727272727e-05, 'epoch': 1.97}
{'loss': 0.3648, 'learning_rate': 6.299999999999999e-05, 'epoch': 1.97}
{'loss': 0.36, 'learning_rate': 6.272727272727272e-05, 'epoch': 1.98}
{'loss': 0.3499, 'learning_rate': 6.245454545454544e-05, 'epoch': 1.98}
{'loss': 0.3346, 'learning_rate': 6.218181818181817e-05, 'epoch': 1.98}
{'loss': 0.3373, 'learning_rate': 6.19090909090909e-05, 'epoch': 1.98}
{'loss': 0.3147, 'learning_rate': 6.163636363636363e-05, 'epoch': 1.98}
{'loss': 0.3136, 'learning_rate': 6.136363636363636e-05, 'epoch': 1.99}
{'loss': 0.3238, 'learning_rate': 6.109090909090909e-05, 'epoch': 1.99}
{'loss': 0.301, 'learning_rate': 6.0818181818181814e-05, 'epoch': 1.99}
{'loss': 0.3021, 'learning_rate': 6.0545454545454536e-05, 'epoch': 1.99}
{'loss': 0.2803, 'learning_rate': 6.0272727272727265e-05, 'epoch': 1.99}
{'loss': 0.2876, 'learning_rate': 5.9999999999999995e-05, 'epoch': 2.0}
{'loss': 0.273, 'learning_rate': 5.9727272727272724e-05, 'epoch': 2.0}
{'loss': 0.405, 'learning_rate': 5.9454545454545447e-05, 'epoch': 2.0}
{'loss': 0.4864, 'learning_rate': 5.9181818181818176e-05, 'epoch': 2.0}
{'loss': 0.4696, 'learning_rate': 5.89090909090909e-05, 'epoch': 2.01}
{'loss': 0.4417, 'learning_rate': 5.8636363636363634e-05, 'epoch': 2.01}
{'loss': 0.4422, 'learning_rate': 5.836363636363636e-05, 'epoch': 2.01}
{'loss': 0.4377, 'learning_rate': 5.8090909090909086e-05, 'epoch': 2.01}
{'loss': 0.4267, 'learning_rate': 5.781818181818181e-05, 'epoch': 2.01}
{'loss': 0.4321, 'learning_rate': 5.754545454545454e-05, 'epoch': 2.02}
{'loss': 0.3973, 'learning_rate': 5.727272727272727e-05, 'epoch': 2.02}
{'loss': 0.4026, 'learning_rate': 5.6999999999999996e-05, 'epoch': 2.02}
{'loss': 0.398, 'learning_rate': 5.672727272727272e-05, 'epoch': 2.02}
{'loss': 0.4047, 'learning_rate': 5.645454545454545e-05, 'epoch': 2.02}
{'loss': 0.417, 'learning_rate': 5.618181818181818e-05, 'epoch': 2.03}
{'loss': 0.3835, 'learning_rate': 5.590909090909091e-05, 'epoch': 2.03}
{'loss': 0.4029, 'learning_rate': 5.563636363636363e-05, 'epoch': 2.03}
{'loss': 0.3897, 'learning_rate': 5.536363636363636e-05, 'epoch': 2.03}
{'loss': 0.3741, 'learning_rate': 5.509090909090908e-05, 'epoch': 2.03}
{'loss': 0.3946, 'learning_rate': 5.481818181818182e-05, 'epoch': 2.04}
{'eval_loss': 0.3991994857788086, 'eval_runtime': 516.6032, 'eval_samples_per_second': 15.211, 'eval_steps_per_second': 0.476, 'epoch': 2.04}
 83%|█████████████████████████████████████████████████████████████████████████████████████▊                 | 1000/1200 [12:15:41<2:23:48, 43.14s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.38, 'learning_rate': 5.454545454545454e-05, 'epoch': 2.04}
{'loss': 0.3938, 'learning_rate': 5.427272727272727e-05, 'epoch': 2.04}
{'loss': 0.385, 'learning_rate': 5.399999999999999e-05, 'epoch': 2.04}
{'loss': 0.3699, 'learning_rate': 5.372727272727272e-05, 'epoch': 2.04}
{'loss': 0.3597, 'learning_rate': 5.345454545454545e-05, 'epoch': 2.05}
{'loss': 0.3903, 'learning_rate': 5.318181818181818e-05, 'epoch': 2.05}
{'loss': 0.3807, 'learning_rate': 5.29090909090909e-05, 'epoch': 2.05}
{'loss': 0.3796, 'learning_rate': 5.263636363636363e-05, 'epoch': 2.05}
{'loss': 0.3684, 'learning_rate': 5.236363636363636e-05, 'epoch': 2.05}
{'loss': 0.3526, 'learning_rate': 5.209090909090909e-05, 'epoch': 2.06}
{'loss': 0.3628, 'learning_rate': 5.181818181818181e-05, 'epoch': 2.06}
{'loss': 0.3533, 'learning_rate': 5.154545454545454e-05, 'epoch': 2.06}
{'loss': 0.381, 'learning_rate': 5.1272727272727264e-05, 'epoch': 2.06}
{'loss': 0.3704, 'learning_rate': 5.1e-05, 'epoch': 2.06}
{'loss': 0.3607, 'learning_rate': 5.072727272727272e-05, 'epoch': 2.07}
{'loss': 0.3484, 'learning_rate': 5.045454545454545e-05, 'epoch': 2.07}
{'loss': 0.3703, 'learning_rate': 5.0181818181818174e-05, 'epoch': 2.07}
{'loss': 0.3508, 'learning_rate': 4.99090909090909e-05, 'epoch': 2.07}
{'loss': 0.3576, 'learning_rate': 4.963636363636363e-05, 'epoch': 2.07}
{'loss': 0.3648, 'learning_rate': 4.936363636363636e-05, 'epoch': 2.08}
{'loss': 0.3544, 'learning_rate': 4.9090909090909084e-05, 'epoch': 2.08}
{'loss': 0.3459, 'learning_rate': 4.8818181818181813e-05, 'epoch': 2.08}
{'loss': 0.3197, 'learning_rate': 4.8545454545454536e-05, 'epoch': 2.08}
{'loss': 0.3302, 'learning_rate': 4.827272727272727e-05, 'epoch': 2.08}
{'loss': 0.3242, 'learning_rate': 4.7999999999999994e-05, 'epoch': 2.09}
{'loss': 0.3047, 'learning_rate': 4.7727272727272724e-05, 'epoch': 2.09}
{'loss': 0.3135, 'learning_rate': 4.7454545454545446e-05, 'epoch': 2.09}
{'loss': 0.3017, 'learning_rate': 4.718181818181818e-05, 'epoch': 2.09}
{'loss': 0.3026, 'learning_rate': 4.6909090909090905e-05, 'epoch': 2.09}
{'loss': 0.2961, 'learning_rate': 4.6636363636363634e-05, 'epoch': 2.1}
{'loss': 0.2788, 'learning_rate': 4.6363636363636356e-05, 'epoch': 2.1}
{'loss': 0.2741, 'learning_rate': 4.6090909090909086e-05, 'epoch': 2.1}
{'loss': 0.3882, 'learning_rate': 4.5818181818181815e-05, 'epoch': 2.1}
{'loss': 0.4851, 'learning_rate': 4.5545454545454544e-05, 'epoch': 2.1}
{'loss': 0.4737, 'learning_rate': 4.527272727272727e-05, 'epoch': 2.11}
{'loss': 0.4701, 'learning_rate': 4.4999999999999996e-05, 'epoch': 2.11}
{'loss': 0.4347, 'learning_rate': 4.472727272727272e-05, 'epoch': 2.11}
{'loss': 0.4362, 'learning_rate': 4.4454545454545455e-05, 'epoch': 2.11}
{'loss': 0.4078, 'learning_rate': 4.418181818181818e-05, 'epoch': 2.12}
{'loss': 0.418, 'learning_rate': 4.3909090909090906e-05, 'epoch': 2.12}
{'eval_loss': 0.39851298928260803, 'eval_runtime': 500.7856, 'eval_samples_per_second': 15.691, 'eval_steps_per_second': 0.491, 'epoch': 2.12}
 87%|█████████████████████████████████████████████████████████████████████████████████████████▎             | 1040/1200 [12:49:53<1:55:08, 43.18s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4027, 'learning_rate': 4.363636363636363e-05, 'epoch': 2.12}
{'loss': 0.4146, 'learning_rate': 4.336363636363636e-05, 'epoch': 2.12}
{'loss': 0.4083, 'learning_rate': 4.309090909090909e-05, 'epoch': 2.12}
{'loss': 0.4053, 'learning_rate': 4.281818181818182e-05, 'epoch': 2.13}
{'loss': 0.3951, 'learning_rate': 4.254545454545454e-05, 'epoch': 2.13}
{'loss': 0.3848, 'learning_rate': 4.227272727272727e-05, 'epoch': 2.13}
{'loss': 0.3825, 'learning_rate': 4.2e-05, 'epoch': 2.13}
{'loss': 0.3965, 'learning_rate': 4.172727272727273e-05, 'epoch': 2.13}
{'loss': 0.3787, 'learning_rate': 4.145454545454545e-05, 'epoch': 2.14}
{'loss': 0.3638, 'learning_rate': 4.118181818181818e-05, 'epoch': 2.14}
{'loss': 0.362, 'learning_rate': 4.09090909090909e-05, 'epoch': 2.14}
{'loss': 0.3984, 'learning_rate': 4.063636363636364e-05, 'epoch': 2.14}
{'loss': 0.36, 'learning_rate': 4.036363636363636e-05, 'epoch': 2.14}
{'loss': 0.363, 'learning_rate': 4.009090909090909e-05, 'epoch': 2.15}
{'loss': 0.3775, 'learning_rate': 3.981818181818181e-05, 'epoch': 2.15}
{'loss': 0.3732, 'learning_rate': 3.954545454545454e-05, 'epoch': 2.15}
{'loss': 0.3691, 'learning_rate': 3.927272727272727e-05, 'epoch': 2.15}
{'loss': 0.3712, 'learning_rate': 3.9e-05, 'epoch': 2.15}
{'loss': 0.3616, 'learning_rate': 3.872727272727272e-05, 'epoch': 2.16}
{'loss': 0.3687, 'learning_rate': 3.845454545454545e-05, 'epoch': 2.16}
{'loss': 0.3564, 'learning_rate': 3.8181818181818174e-05, 'epoch': 2.16}
{'loss': 0.3599, 'learning_rate': 3.790909090909091e-05, 'epoch': 2.16}
{'loss': 0.3641, 'learning_rate': 3.763636363636363e-05, 'epoch': 2.16}
{'loss': 0.3531, 'learning_rate': 3.736363636363636e-05, 'epoch': 2.17}
{'loss': 0.3825, 'learning_rate': 3.709090909090909e-05, 'epoch': 2.17}
{'loss': 0.3697, 'learning_rate': 3.681818181818181e-05, 'epoch': 2.17}
{'loss': 0.3529, 'learning_rate': 3.654545454545454e-05, 'epoch': 2.17}
{'loss': 0.3577, 'learning_rate': 3.627272727272727e-05, 'epoch': 2.17}
{'loss': 0.3652, 'learning_rate': 3.5999999999999994e-05, 'epoch': 2.18}
{'loss': 0.3376, 'learning_rate': 3.5727272727272723e-05, 'epoch': 2.18}
{'loss': 0.3292, 'learning_rate': 3.545454545454545e-05, 'epoch': 2.18}
{'loss': 0.3273, 'learning_rate': 3.518181818181818e-05, 'epoch': 2.18}
{'loss': 0.3323, 'learning_rate': 3.4909090909090904e-05, 'epoch': 2.18}
{'loss': 0.3083, 'learning_rate': 3.4636363636363634e-05, 'epoch': 2.19}
{'loss': 0.3352, 'learning_rate': 3.436363636363636e-05, 'epoch': 2.19}
{'loss': 0.309, 'learning_rate': 3.4090909090909085e-05, 'epoch': 2.19}
{'loss': 0.3056, 'learning_rate': 3.3818181818181815e-05, 'epoch': 2.19}
{'loss': 0.2884, 'learning_rate': 3.3545454545454544e-05, 'epoch': 2.19}
{'loss': 0.2987, 'learning_rate': 3.327272727272727e-05, 'epoch': 2.2}
{'loss': 0.2929, 'learning_rate': 3.2999999999999996e-05, 'epoch': 2.2}
{'eval_loss': 0.3979171812534332, 'eval_runtime': 495.2316, 'eval_samples_per_second': 15.867, 'eval_steps_per_second': 0.497, 'epoch': 2.2}
 90%|████████████████████████████████████████████████████████████████████████████████████████████▋          | 1080/1200 [13:23:14<1:05:05, 32.54s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2704, 'learning_rate': 3.2727272727272725e-05, 'epoch': 2.2}
{'loss': 0.2775, 'learning_rate': 3.2454545454545454e-05, 'epoch': 2.2}
{'loss': 0.3887, 'learning_rate': 3.218181818181818e-05, 'epoch': 2.2}
{'loss': 0.5108, 'learning_rate': 3.1909090909090906e-05, 'epoch': 2.21}
{'loss': 0.4817, 'learning_rate': 3.1636363636363635e-05, 'epoch': 2.21}
{'loss': 0.4441, 'learning_rate': 3.136363636363636e-05, 'epoch': 2.21}
{'loss': 0.4432, 'learning_rate': 3.109090909090909e-05, 'epoch': 2.21}
{'loss': 0.4291, 'learning_rate': 3.0818181818181816e-05, 'epoch': 2.21}
{'loss': 0.4177, 'learning_rate': 3.0545454545454546e-05, 'epoch': 2.22}
{'loss': 0.4117, 'learning_rate': 3.0272727272727268e-05, 'epoch': 2.22}
{'loss': 0.4069, 'learning_rate': 2.9999999999999997e-05, 'epoch': 2.22}
{'loss': 0.3946, 'learning_rate': 2.9727272727272723e-05, 'epoch': 2.22}
{'loss': 0.399, 'learning_rate': 2.945454545454545e-05, 'epoch': 2.22}
{'loss': 0.4049, 'learning_rate': 2.918181818181818e-05, 'epoch': 2.23}
{'loss': 0.4061, 'learning_rate': 2.8909090909090904e-05, 'epoch': 2.23}
{'loss': 0.3998, 'learning_rate': 2.8636363636363634e-05, 'epoch': 2.23}
{'loss': 0.3864, 'learning_rate': 2.836363636363636e-05, 'epoch': 2.23}
{'loss': 0.3908, 'learning_rate': 2.809090909090909e-05, 'epoch': 2.24}
{'loss': 0.3895, 'learning_rate': 2.7818181818181815e-05, 'epoch': 2.24}
{'loss': 0.3804, 'learning_rate': 2.754545454545454e-05, 'epoch': 2.24}
{'loss': 0.3968, 'learning_rate': 2.727272727272727e-05, 'epoch': 2.24}
{'loss': 0.3593, 'learning_rate': 2.6999999999999996e-05, 'epoch': 2.24}
{'loss': 0.3897, 'learning_rate': 2.6727272727272725e-05, 'epoch': 2.25}
{'loss': 0.368, 'learning_rate': 2.645454545454545e-05, 'epoch': 2.25}
{'loss': 0.3745, 'learning_rate': 2.618181818181818e-05, 'epoch': 2.25}
{'loss': 0.3756, 'learning_rate': 2.5909090909090906e-05, 'epoch': 2.25}
{'loss': 0.3756, 'learning_rate': 2.5636363636363632e-05, 'epoch': 2.25}
{'loss': 0.3731, 'learning_rate': 2.536363636363636e-05, 'epoch': 2.26}
{'loss': 0.3608, 'learning_rate': 2.5090909090909087e-05, 'epoch': 2.26}
{'loss': 0.3648, 'learning_rate': 2.4818181818181816e-05, 'epoch': 2.26}
{'loss': 0.3497, 'learning_rate': 2.4545454545454542e-05, 'epoch': 2.26}
{'loss': 0.357, 'learning_rate': 2.4272727272727268e-05, 'epoch': 2.26}
{'loss': 0.3653, 'learning_rate': 2.3999999999999997e-05, 'epoch': 2.27}
{'loss': 0.3535, 'learning_rate': 2.3727272727272723e-05, 'epoch': 2.27}
{'loss': 0.3608, 'learning_rate': 2.3454545454545452e-05, 'epoch': 2.27}
{'loss': 0.3594, 'learning_rate': 2.3181818181818178e-05, 'epoch': 2.27}
{'loss': 0.3575, 'learning_rate': 2.2909090909090908e-05, 'epoch': 2.27}
{'loss': 0.3565, 'learning_rate': 2.2636363636363633e-05, 'epoch': 2.28}
{'loss': 0.346, 'learning_rate': 2.236363636363636e-05, 'epoch': 2.28}
{'loss': 0.3448, 'learning_rate': 2.209090909090909e-05, 'epoch': 2.28}
{'eval_loss': 0.3963205814361572, 'eval_runtime': 344.9619, 'eval_samples_per_second': 22.779, 'eval_steps_per_second': 0.713, 'epoch': 2.28}
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████       | 1120/1200 [13:53:37<37:16, 27.96s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3423, 'learning_rate': 2.1818181818181814e-05, 'epoch': 2.28}
{'loss': 0.321, 'learning_rate': 2.1545454545454544e-05, 'epoch': 2.28}
{'loss': 0.3177, 'learning_rate': 2.127272727272727e-05, 'epoch': 2.29}
{'loss': 0.3113, 'learning_rate': 2.1e-05, 'epoch': 2.29}
{'loss': 0.3064, 'learning_rate': 2.0727272727272725e-05, 'epoch': 2.29}
{'loss': 0.3036, 'learning_rate': 2.045454545454545e-05, 'epoch': 2.29}
{'loss': 0.3041, 'learning_rate': 2.018181818181818e-05, 'epoch': 2.29}
{'loss': 0.2903, 'learning_rate': 1.9909090909090906e-05, 'epoch': 2.3}
{'loss': 0.2992, 'learning_rate': 1.9636363636363635e-05, 'epoch': 2.3}
{'loss': 0.2825, 'learning_rate': 1.936363636363636e-05, 'epoch': 2.3}
{'loss': 0.278, 'learning_rate': 1.9090909090909087e-05, 'epoch': 2.3}
{'loss': 0.2697, 'learning_rate': 1.8818181818181816e-05, 'epoch': 2.3}
{'loss': 0.391, 'learning_rate': 1.8545454545454545e-05, 'epoch': 2.31}
{'loss': 0.4894, 'learning_rate': 1.827272727272727e-05, 'epoch': 2.31}
{'loss': 0.4619, 'learning_rate': 1.7999999999999997e-05, 'epoch': 2.31}
{'loss': 0.4394, 'learning_rate': 1.7727272727272726e-05, 'epoch': 2.31}
{'loss': 0.4263, 'learning_rate': 1.7454545454545452e-05, 'epoch': 2.31}
{'loss': 0.4158, 'learning_rate': 1.718181818181818e-05, 'epoch': 2.32}
{'loss': 0.4173, 'learning_rate': 1.6909090909090907e-05, 'epoch': 2.32}
{'loss': 0.4013, 'learning_rate': 1.6636363636363637e-05, 'epoch': 2.32}
{'loss': 0.4195, 'learning_rate': 1.6363636363636363e-05, 'epoch': 2.32}
{'loss': 0.3977, 'learning_rate': 1.609090909090909e-05, 'epoch': 2.32}
{'loss': 0.3889, 'learning_rate': 1.5818181818181818e-05, 'epoch': 2.33}
{'loss': 0.3847, 'learning_rate': 1.5545454545454544e-05, 'epoch': 2.33}
{'loss': 0.3974, 'learning_rate': 1.5272727272727273e-05, 'epoch': 2.33}
{'loss': 0.3998, 'learning_rate': 1.4999999999999999e-05, 'epoch': 2.33}
{'loss': 0.4081, 'learning_rate': 1.4727272727272725e-05, 'epoch': 2.33}
{'loss': 0.3987, 'learning_rate': 1.4454545454545452e-05, 'epoch': 2.34}
{'loss': 0.3852, 'learning_rate': 1.418181818181818e-05, 'epoch': 2.34}
{'loss': 0.3787, 'learning_rate': 1.3909090909090907e-05, 'epoch': 2.34}
{'loss': 0.3605, 'learning_rate': 1.3636363636363635e-05, 'epoch': 2.34}
{'loss': 0.3826, 'learning_rate': 1.3363636363636362e-05, 'epoch': 2.35}
{'loss': 0.3621, 'learning_rate': 1.309090909090909e-05, 'epoch': 2.35}
{'loss': 0.367, 'learning_rate': 1.2818181818181816e-05, 'epoch': 2.35}
{'loss': 0.3887, 'learning_rate': 1.2545454545454543e-05, 'epoch': 2.35}
{'loss': 0.3931, 'learning_rate': 1.2272727272727271e-05, 'epoch': 2.35}
{'loss': 0.3621, 'learning_rate': 1.1999999999999999e-05, 'epoch': 2.36}
{'loss': 0.3728, 'learning_rate': 1.1727272727272726e-05, 'epoch': 2.36}
{'loss': 0.3644, 'learning_rate': 1.1454545454545454e-05, 'epoch': 2.36}
{'loss': 0.3607, 'learning_rate': 1.118181818181818e-05, 'epoch': 2.36}
{'eval_loss': 0.396053671836853, 'eval_runtime': 337.4346, 'eval_samples_per_second': 23.287, 'eval_steps_per_second': 0.729, 'epoch': 2.36}
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 1160/1200 [14:20:22<20:23, 30.59s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.35, 'learning_rate': 1.0909090909090907e-05, 'epoch': 2.36}
{'loss': 0.3795, 'learning_rate': 1.0636363636363635e-05, 'epoch': 2.37}
{'loss': 0.3653, 'learning_rate': 1.0363636363636362e-05, 'epoch': 2.37}
{'loss': 0.3586, 'learning_rate': 1.009090909090909e-05, 'epoch': 2.37}
{'loss': 0.3647, 'learning_rate': 9.818181818181818e-06, 'epoch': 2.37}
{'loss': 0.3635, 'learning_rate': 9.545454545454543e-06, 'epoch': 2.37}
{'loss': 0.364, 'learning_rate': 9.272727272727273e-06, 'epoch': 2.38}
{'loss': 0.3539, 'learning_rate': 8.999999999999999e-06, 'epoch': 2.38}
{'loss': 0.3406, 'learning_rate': 8.727272727272726e-06, 'epoch': 2.38}
{'loss': 0.3444, 'learning_rate': 8.454545454545454e-06, 'epoch': 2.38}
{'loss': 0.3385, 'learning_rate': 8.181818181818181e-06, 'epoch': 2.38}
{'loss': 0.3167, 'learning_rate': 7.909090909090909e-06, 'epoch': 2.39}
{'loss': 0.345, 'learning_rate': 7.636363636363636e-06, 'epoch': 2.39}
{'loss': 0.3089, 'learning_rate': 7.363636363636362e-06, 'epoch': 2.39}
{'loss': 0.3031, 'learning_rate': 7.09090909090909e-06, 'epoch': 2.39}
{'loss': 0.3051, 'learning_rate': 6.8181818181818174e-06, 'epoch': 2.39}
{'loss': 0.2993, 'learning_rate': 6.545454545454545e-06, 'epoch': 2.4}
{'loss': 0.2992, 'learning_rate': 6.272727272727272e-06, 'epoch': 2.4}
{'loss': 0.2984, 'learning_rate': 5.999999999999999e-06, 'epoch': 2.4}
{'loss': 0.2845, 'learning_rate': 5.727272727272727e-06, 'epoch': 2.4}
{'loss': 0.2855, 'learning_rate': 5.454545454545454e-06, 'epoch': 2.4}
{'loss': 0.2789, 'learning_rate': 5.181818181818181e-06, 'epoch': 2.41}
{'loss': 0.3954, 'learning_rate': 4.909090909090909e-06, 'epoch': 2.41}
{'loss': 0.4851, 'learning_rate': 4.636363636363636e-06, 'epoch': 2.41}
{'loss': 0.4657, 'learning_rate': 4.363636363636363e-06, 'epoch': 2.41}
{'loss': 0.4324, 'learning_rate': 4.090909090909091e-06, 'epoch': 2.41}
{'loss': 0.4205, 'learning_rate': 3.818181818181818e-06, 'epoch': 2.42}
{'loss': 0.4245, 'learning_rate': 3.545454545454545e-06, 'epoch': 2.42}
{'loss': 0.406, 'learning_rate': 3.2727272727272725e-06, 'epoch': 2.42}
{'loss': 0.4292, 'learning_rate': 2.9999999999999997e-06, 'epoch': 2.42}
{'loss': 0.3982, 'learning_rate': 2.727272727272727e-06, 'epoch': 2.42}
{'loss': 0.3915, 'learning_rate': 2.4545454545454544e-06, 'epoch': 2.43}
{'loss': 0.388, 'learning_rate': 2.1818181818181815e-06, 'epoch': 2.43}
{'loss': 0.392, 'learning_rate': 1.909090909090909e-06, 'epoch': 2.43}
{'loss': 0.3839, 'learning_rate': 1.6363636363636363e-06, 'epoch': 2.43}
{'loss': 0.3872, 'learning_rate': 1.3636363636363634e-06, 'epoch': 2.43}
{'loss': 0.3901, 'learning_rate': 1.0909090909090908e-06, 'epoch': 2.44}
{'loss': 0.3938, 'learning_rate': 8.181818181818181e-07, 'epoch': 2.44}
{'loss': 0.373, 'learning_rate': 5.454545454545454e-07, 'epoch': 2.44}
{'loss': 0.388, 'learning_rate': 2.727272727272727e-07, 'epoch': 2.44}
{'eval_loss': 0.3956533968448639, 'eval_runtime': 369.2303, 'eval_samples_per_second': 21.282, 'eval_steps_per_second': 0.666, 'epoch': 2.44}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [14:47:31<00:00, 31.91s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'train_runtime': 53260.5587, 'train_samples_per_second': 2.884, 'train_steps_per_second': 0.023, 'train_loss': 0.45437952178219954, 'epoch': 2.44}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1200/1200 [14:47:40<00:00, 44.38s/it]

real    888m37.163s
user    885m13.346s
sys     1m24.195s
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 53}
