aftab@ubuntu:~/workspace/Llama-experiments/src$ source train.sh
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported versi
on!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be rem
oved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-08-26 18:35:17:     Running model for finetuning.
2024-08-26 18:35:17:     Loaded saved finetuning dataset (train:
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 62861
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/train
2024-08-26 18:35:17:     Loaded saved finetuning dataset (eval):
  Dataset({
    features: ['question', 'answer', 'context'],
    num_rows: 7858
})
  Dataset path: /home/aftab/workspace/Llama-experiments/src/datasets/sql-create-context/clean/70k/val
2024-08-26 18:35:17:     Printing 1 samples from the dataset:
2024-08-26 18:35:17:     {'question': 'Willy Sagnol with a type as career end had what has the transfer fee?', 'answer': 'SELECT transfer_fee FROM ta
ble_name_34 WHERE type = "career end" AND name = "willy sagnol"', 'context': 'CREATE TABLE table_name_34 (transfer_fee VARCHAR, type VARCHAR, name VA
RCHAR)'}
2024-08-26 18:35:17:     {'question': 'What was the final score of the game with 7523 in attendance?', 'answer': 'SELECT final_score FROM table_25331
766_3 WHERE attendance = 7523', 'context': 'CREATE TABLE table_25331766_3 (final_score VARCHAR, attendance VARCHAR)'}
2024-08-26 18:35:17:
2024-08-26 18:35:17:     Printing 1 samples from the dataset:
2024-08-26 18:35:17:     {'question': 'How many ties did he have when he had 1 penalties and more than 20 conversions?', 'answer': 'SELECT SUM(drawn)
 FROM table_name_33 WHERE penalties = 1 AND conversions > 20', 'context': 'CREATE TABLE table_name_33 (drawn INTEGER, penalties VARCHAR, conversions
VARCHAR)'}
2024-08-26 18:35:17:     {'question': 'What place goes with the score of 70-66-65=201?', 'answer': 'SELECT place FROM table_name_90 WHERE score = 70
- 66 - 65 = 201', 'context': 'CREATE TABLE table_name_90 (place VARCHAR, score VARCHAR)'}
2024-08-26 18:35:17:
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.77s/it]
2024-08-26 18:35:27:     compiling the model
2024-08-26 18:35:27:     Saving output model(s) of training in
{'output_dir': 'Llama-2-7b-hf-text-to-sql-train-localData_lora_qbits-8'}
  0%|                                                                                                                        | 0/400 [00:00<?, ?it/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to enc
ode the text followed by a call to the `pad` method to get a padded encoding.
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8307, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.0}
{'loss': 2.0363, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.0}
{'loss': 2.103, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.01}
{'loss': 2.1259, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1774, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.01}
{'loss': 2.1511, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.01}
{'loss': 2.1792, 'learning_rate': 2.1e-05, 'epoch': 0.01}
{'loss': 2.1787, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1562, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}
{'loss': 2.17, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.02}
{'loss': 2.1454, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}
{'loss': 2.1353, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.02}
{'loss': 2.1125, 'learning_rate': 3.9e-05, 'epoch': 0.03}
{'loss': 2.1074, 'learning_rate': 4.2e-05, 'epoch': 0.03}
{'loss': 2.0854, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.03}
{'loss': 2.0202, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.03}
{'loss': 2.017, 'learning_rate': 5.1e-05, 'epoch': 0.03}
{'loss': 2.0125, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.04}
{'loss': 1.9801, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.04}
{'loss': 1.9447, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.04}
{'eval_loss': 1.8949497938156128, 'eval_runtime': 511.4748, 'eval_samples_per_second': 15.363, 'eval_steps_per_second': 0.481, 'epoch': 0.04}
  5%|█████▍                                                                                                       | 20/400 [22:52<3:50:09, 36.34s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9255, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.04}
{'loss': 1.883, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.04}
{'loss': 1.8245, 'learning_rate': 6.9e-05, 'epoch': 0.05}
{'loss': 1.7541, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.05}
{'loss': 1.6459, 'learning_rate': 7.5e-05, 'epoch': 0.05}
{'loss': 1.6258, 'learning_rate': 7.8e-05, 'epoch': 0.05}
{'loss': 1.5384, 'learning_rate': 8.1e-05, 'epoch': 0.05}
{'loss': 1.4631, 'learning_rate': 8.4e-05, 'epoch': 0.06}
{'loss': 1.3806, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.06}
{'loss': 1.3171, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.06}
{'loss': 1.2093, 'learning_rate': 9.3e-05, 'epoch': 0.06}
{'loss': 1.1301, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.07}
{'loss': 1.0596, 'learning_rate': 9.9e-05, 'epoch': 0.07}
{'loss': 0.9081, 'learning_rate': 0.000102, 'epoch': 0.07}
{'loss': 0.8859, 'learning_rate': 0.00010499999999999999, 'epoch': 0.07}
{'loss': 0.8761, 'learning_rate': 0.00010799999999999998, 'epoch': 0.07}
{'loss': 0.8152, 'learning_rate': 0.00011099999999999999, 'epoch': 0.08}
{'loss': 0.7823, 'learning_rate': 0.00011399999999999999, 'epoch': 0.08}
{'loss': 0.7579, 'learning_rate': 0.000117, 'epoch': 0.08}
{'loss': 0.7187, 'learning_rate': 0.00011999999999999999, 'epoch': 0.08}
{'eval_loss': 0.8187939524650574, 'eval_runtime': 666.7636, 'eval_samples_per_second': 11.785, 'eval_steps_per_second': 0.369, 'epoch': 0.08}
 10%|██████████▉                                                                                                  | 40/400 [46:11<3:22:49, 33.80s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6909, 'learning_rate': 0.00012299999999999998, 'epoch': 0.08}
{'loss': 0.6714, 'learning_rate': 0.00012599999999999997, 'epoch': 0.09}
{'loss': 0.5585, 'learning_rate': 0.000129, 'epoch': 0.09}
{'loss': 0.6141, 'learning_rate': 0.00013199999999999998, 'epoch': 0.09}
{'loss': 0.5967, 'learning_rate': 0.000135, 'epoch': 0.09}
{'loss': 0.556, 'learning_rate': 0.000138, 'epoch': 0.09}
{'loss': 0.5416, 'learning_rate': 0.00014099999999999998, 'epoch': 0.1}
{'loss': 0.5054, 'learning_rate': 0.00014399999999999998, 'epoch': 0.1}
{'loss': 0.4533, 'learning_rate': 0.000147, 'epoch': 0.1}
{'loss': 0.5218, 'learning_rate': 0.00015, 'epoch': 0.1}
{'loss': 0.9167, 'learning_rate': 0.00015299999999999998, 'epoch': 0.1}
{'loss': 0.8677, 'learning_rate': 0.000156, 'epoch': 0.11}
{'loss': 0.8353, 'learning_rate': 0.000159, 'epoch': 0.11}
{'loss': 0.7691, 'learning_rate': 0.000162, 'epoch': 0.11}
{'loss': 0.7689, 'learning_rate': 0.000165, 'epoch': 0.11}
{'loss': 0.7703, 'learning_rate': 0.000168, 'epoch': 0.11}
{'loss': 0.725, 'learning_rate': 0.00017099999999999998, 'epoch': 0.12}
{'loss': 0.7446, 'learning_rate': 0.00017399999999999997, 'epoch': 0.12}
{'loss': 0.7366, 'learning_rate': 0.00017699999999999997, 'epoch': 0.12}
{'loss': 0.6871, 'learning_rate': 0.00017999999999999998, 'epoch': 0.12}
{'eval_loss': 0.6265365481376648, 'eval_runtime': 667.657, 'eval_samples_per_second': 11.77, 'eval_steps_per_second': 0.368, 'epoch': 0.12}
 15%|████████████████                                                                                           | 60/400 [1:13:14<4:38:56, 49.22s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6583, 'learning_rate': 0.00018299999999999998, 'epoch': 0.12}
{'loss': 0.6909, 'learning_rate': 0.000186, 'epoch': 0.13}
{'loss': 0.6785, 'learning_rate': 0.00018899999999999999, 'epoch': 0.13}
{'loss': 0.6537, 'learning_rate': 0.00019199999999999998, 'epoch': 0.13}
{'loss': 0.6493, 'learning_rate': 0.000195, 'epoch': 0.13}
{'loss': 0.6413, 'learning_rate': 0.000198, 'epoch': 0.13}
{'loss': 0.5827, 'learning_rate': 0.000201, 'epoch': 0.14}
{'loss': 0.6405, 'learning_rate': 0.000204, 'epoch': 0.14}
{'loss': 0.6201, 'learning_rate': 0.00020699999999999996, 'epoch': 0.14}
{'loss': 0.6092, 'learning_rate': 0.00020999999999999998, 'epoch': 0.14}
{'loss': 0.5907, 'learning_rate': 0.00021299999999999997, 'epoch': 0.14}
{'loss': 0.5923, 'learning_rate': 0.00021599999999999996, 'epoch': 0.15}
{'loss': 0.5795, 'learning_rate': 0.00021899999999999998, 'epoch': 0.15}
{'loss': 0.5451, 'learning_rate': 0.00022199999999999998, 'epoch': 0.15}
{'loss': 0.5291, 'learning_rate': 0.000225, 'epoch': 0.15}
{'loss': 0.485, 'learning_rate': 0.00022799999999999999, 'epoch': 0.15}
{'loss': 0.4883, 'learning_rate': 0.00023099999999999998, 'epoch': 0.16}
{'loss': 0.4826, 'learning_rate': 0.000234, 'epoch': 0.16}
{'loss': 0.4735, 'learning_rate': 0.000237, 'epoch': 0.16}
{'loss': 0.4581, 'learning_rate': 0.00023999999999999998, 'epoch': 0.16}
{'eval_loss': 0.5117664933204651, 'eval_runtime': 665.067, 'eval_samples_per_second': 11.815, 'eval_steps_per_second': 0.37, 'epoch': 0.16}
 20%|█████████████████████▍                                                                                     | 80/400 [1:39:20<3:42:51, 41.79s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4602, 'learning_rate': 0.000243, 'epoch': 0.16}
{'loss': 0.4575, 'learning_rate': 0.00024599999999999996, 'epoch': 0.17}
{'loss': 0.4352, 'learning_rate': 0.000249, 'epoch': 0.17}
{'loss': 0.4305, 'learning_rate': 0.00025199999999999995, 'epoch': 0.17}
{'loss': 0.4062, 'learning_rate': 0.00025499999999999996, 'epoch': 0.17}
{'loss': 0.3982, 'learning_rate': 0.000258, 'epoch': 0.18}
{'loss': 0.4218, 'learning_rate': 0.000261, 'epoch': 0.18}
{'loss': 0.4033, 'learning_rate': 0.00026399999999999997, 'epoch': 0.18}
{'loss': 0.3811, 'learning_rate': 0.000267, 'epoch': 0.18}
{'loss': 0.3947, 'learning_rate': 0.00027, 'epoch': 0.18}
{'loss': 0.3731, 'learning_rate': 0.00027299999999999997, 'epoch': 0.19}
{'loss': 0.3616, 'learning_rate': 0.000276, 'epoch': 0.19}
{'loss': 0.3664, 'learning_rate': 0.000279, 'epoch': 0.19}
{'loss': 0.3329, 'learning_rate': 0.00028199999999999997, 'epoch': 0.19}
{'loss': 0.3313, 'learning_rate': 0.000285, 'epoch': 0.19}
{'loss': 0.3388, 'learning_rate': 0.00028799999999999995, 'epoch': 0.2}
{'loss': 0.3245, 'learning_rate': 0.00029099999999999997, 'epoch': 0.2}
{'loss': 0.3241, 'learning_rate': 0.000294, 'epoch': 0.2}
{'loss': 0.301, 'learning_rate': 0.00029699999999999996, 'epoch': 0.2}
{'loss': 0.3663, 'learning_rate': 0.0003, 'epoch': 0.2}
{'eval_loss': 0.5117504000663757, 'eval_runtime': 658.4112, 'eval_samples_per_second': 11.935, 'eval_steps_per_second': 0.374, 'epoch': 0.2}
 25%|██████████████████████████▌                                                                               | 100/400 [2:04:07<3:09:26, 37.89s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7346, 'learning_rate': 0.000299, 'epoch': 0.21}
{'loss': 0.6924, 'learning_rate': 0.000298, 'epoch': 0.21}
{'loss': 0.6555, 'learning_rate': 0.00029699999999999996, 'epoch': 0.21}
{'loss': 0.6341, 'learning_rate': 0.000296, 'epoch': 0.21}
{'loss': 0.6004, 'learning_rate': 0.00029499999999999996, 'epoch': 0.21}
{'loss': 0.6096, 'learning_rate': 0.000294, 'epoch': 0.22}
{'loss': 0.607, 'learning_rate': 0.00029299999999999997, 'epoch': 0.22}
{'loss': 0.581, 'learning_rate': 0.000292, 'epoch': 0.22}
{'loss': 0.5634, 'learning_rate': 0.00029099999999999997, 'epoch': 0.22}
{'loss': 0.5606, 'learning_rate': 0.00029, 'epoch': 0.22}
{'loss': 0.547, 'learning_rate': 0.000289, 'epoch': 0.23}
{'loss': 0.5353, 'learning_rate': 0.00028799999999999995, 'epoch': 0.23}
{'loss': 0.5305, 'learning_rate': 0.000287, 'epoch': 0.23}
{'loss': 0.523, 'learning_rate': 0.00028599999999999996, 'epoch': 0.23}
{'loss': 0.5153, 'learning_rate': 0.000285, 'epoch': 0.23}
{'loss': 0.5339, 'learning_rate': 0.00028399999999999996, 'epoch': 0.24}
{'loss': 0.5063, 'learning_rate': 0.000283, 'epoch': 0.24}
{'loss': 0.5301, 'learning_rate': 0.00028199999999999997, 'epoch': 0.24}
{'loss': 0.4778, 'learning_rate': 0.00028099999999999995, 'epoch': 0.24}
{'loss': 0.4979, 'learning_rate': 0.00028, 'epoch': 0.24}
{'eval_loss': 0.473286509513855, 'eval_runtime': 646.7399, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.38, 'epoch': 0.24}
 30%|███████████████████████████████▊                                                                          | 120/400 [2:31:27<3:10:08, 40.74s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4766, 'learning_rate': 0.000279, 'epoch': 0.25}
{'loss': 0.4712, 'learning_rate': 0.000278, 'epoch': 0.25}
{'loss': 0.4717, 'learning_rate': 0.00027699999999999996, 'epoch': 0.25}
{'loss': 0.4623, 'learning_rate': 0.000276, 'epoch': 0.25}
{'loss': 0.4655, 'learning_rate': 0.00027499999999999996, 'epoch': 0.25}
{'loss': 0.4605, 'learning_rate': 0.000274, 'epoch': 0.26}
{'loss': 0.44, 'learning_rate': 0.00027299999999999997, 'epoch': 0.26}
{'loss': 0.4547, 'learning_rate': 0.00027199999999999994, 'epoch': 0.26}
{'loss': 0.44, 'learning_rate': 0.000271, 'epoch': 0.26}
{'loss': 0.4094, 'learning_rate': 0.00027, 'epoch': 0.26}
{'loss': 0.4195, 'learning_rate': 0.000269, 'epoch': 0.27}
{'loss': 0.3988, 'learning_rate': 0.00026799999999999995, 'epoch': 0.27}
{'loss': 0.4231, 'learning_rate': 0.000267, 'epoch': 0.27}
{'loss': 0.3929, 'learning_rate': 0.000266, 'epoch': 0.27}
{'loss': 0.3848, 'learning_rate': 0.000265, 'epoch': 0.27}
{'loss': 0.3938, 'learning_rate': 0.00026399999999999997, 'epoch': 0.28}
{'loss': 0.3692, 'learning_rate': 0.000263, 'epoch': 0.28}
{'loss': 0.3644, 'learning_rate': 0.00026199999999999997, 'epoch': 0.28}
{'loss': 0.3716, 'learning_rate': 0.000261, 'epoch': 0.28}
{'loss': 0.3722, 'learning_rate': 0.00026, 'epoch': 0.28}
{'eval_loss': 0.4651689827442169, 'eval_runtime': 680.227, 'eval_samples_per_second': 11.552, 'eval_steps_per_second': 0.362, 'epoch': 0.28}
 35%|█████████████████████████████████████                                                                     | 140/400 [2:57:32<3:02:47, 42.18s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3746, 'learning_rate': 0.00025899999999999995, 'epoch': 0.29}
{'loss': 0.354, 'learning_rate': 0.000258, 'epoch': 0.29}
{'loss': 0.3411, 'learning_rate': 0.00025699999999999996, 'epoch': 0.29}
{'loss': 0.329, 'learning_rate': 0.000256, 'epoch': 0.29}
{'loss': 0.3275, 'learning_rate': 0.00025499999999999996, 'epoch': 0.3}
{'loss': 0.3278, 'learning_rate': 0.000254, 'epoch': 0.3}
{'loss': 0.3089, 'learning_rate': 0.00025299999999999997, 'epoch': 0.3}
{'loss': 0.2902, 'learning_rate': 0.00025199999999999995, 'epoch': 0.3}
{'loss': 0.2983, 'learning_rate': 0.000251, 'epoch': 0.3}
{'loss': 0.3651, 'learning_rate': 0.00025, 'epoch': 0.31}
{'loss': 0.6827, 'learning_rate': 0.000249, 'epoch': 0.31}
{'loss': 0.6468, 'learning_rate': 0.00024799999999999996, 'epoch': 0.31}
{'loss': 0.6192, 'learning_rate': 0.000247, 'epoch': 0.31}
{'loss': 0.6175, 'learning_rate': 0.00024599999999999996, 'epoch': 0.31}
{'loss': 0.5818, 'learning_rate': 0.000245, 'epoch': 0.32}
{'loss': 0.5621, 'learning_rate': 0.000244, 'epoch': 0.32}
{'loss': 0.5568, 'learning_rate': 0.000243, 'epoch': 0.32}
{'loss': 0.543, 'learning_rate': 0.00024199999999999997, 'epoch': 0.32}
{'loss': 0.5271, 'learning_rate': 0.00024099999999999998, 'epoch': 0.32}
{'loss': 0.5156, 'learning_rate': 0.00023999999999999998, 'epoch': 0.33}
{'eval_loss': 0.4693317711353302, 'eval_runtime': 662.5265, 'eval_samples_per_second': 11.861, 'eval_steps_per_second': 0.371, 'epoch': 0.33}
 40%|██████████████████████████████████████████▍                                                               | 160/400 [3:23:46<3:06:45, 46.69s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5221, 'learning_rate': 0.00023899999999999998, 'epoch': 0.33}
{'loss': 0.5065, 'learning_rate': 0.00023799999999999998, 'epoch': 0.33}
{'loss': 0.4976, 'learning_rate': 0.000237, 'epoch': 0.33}
{'loss': 0.5076, 'learning_rate': 0.00023599999999999996, 'epoch': 0.33}
{'loss': 0.5067, 'learning_rate': 0.00023499999999999997, 'epoch': 0.34}
{'loss': 0.4836, 'learning_rate': 0.000234, 'epoch': 0.34}
{'loss': 0.4838, 'learning_rate': 0.00023299999999999997, 'epoch': 0.34}
{'loss': 0.4843, 'learning_rate': 0.00023199999999999997, 'epoch': 0.34}
{'loss': 0.4753, 'learning_rate': 0.00023099999999999998, 'epoch': 0.34}
{'loss': 0.4673, 'learning_rate': 0.00023, 'epoch': 0.35}
{'loss': 0.4709, 'learning_rate': 0.00022899999999999998, 'epoch': 0.35}
{'loss': 0.4578, 'learning_rate': 0.00022799999999999999, 'epoch': 0.35}
{'loss': 0.4535, 'learning_rate': 0.000227, 'epoch': 0.35}
{'loss': 0.4289, 'learning_rate': 0.00022599999999999996, 'epoch': 0.35}
{'loss': 0.4438, 'learning_rate': 0.000225, 'epoch': 0.36}
{'loss': 0.4471, 'learning_rate': 0.000224, 'epoch': 0.36}
{'loss': 0.4202, 'learning_rate': 0.00022299999999999997, 'epoch': 0.36}
{'loss': 0.4229, 'learning_rate': 0.00022199999999999998, 'epoch': 0.36}
{'loss': 0.4254, 'learning_rate': 0.00022099999999999998, 'epoch': 0.36}
{'loss': 0.4244, 'learning_rate': 0.00021999999999999995, 'epoch': 0.37}
{'eval_loss': 0.4464828073978424, 'eval_runtime': 514.0882, 'eval_samples_per_second': 15.285, 'eval_steps_per_second': 0.479, 'epoch': 0.37}
 45%|███████████████████████████████████████████████▋                                                          | 180/400 [3:45:36<1:52:34, 30.70s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3912, 'learning_rate': 0.00021899999999999998, 'epoch': 0.37}
{'loss': 0.4001, 'learning_rate': 0.00021799999999999999, 'epoch': 0.37}
{'loss': 0.3888, 'learning_rate': 0.000217, 'epoch': 0.37}
{'loss': 0.3781, 'learning_rate': 0.00021599999999999996, 'epoch': 0.37}
{'loss': 0.3861, 'learning_rate': 0.000215, 'epoch': 0.38}
{'loss': 0.3714, 'learning_rate': 0.000214, 'epoch': 0.38}
{'loss': 0.3627, 'learning_rate': 0.00021299999999999997, 'epoch': 0.38}
{'loss': 0.3459, 'learning_rate': 0.00021199999999999998, 'epoch': 0.38}
{'loss': 0.3521, 'learning_rate': 0.00021099999999999998, 'epoch': 0.38}
{'loss': 0.3428, 'learning_rate': 0.00020999999999999998, 'epoch': 0.39}
{'loss': 0.3361, 'learning_rate': 0.00020899999999999998, 'epoch': 0.39}
{'loss': 0.325, 'learning_rate': 0.000208, 'epoch': 0.39}
{'loss': 0.3296, 'learning_rate': 0.00020699999999999996, 'epoch': 0.39}
{'loss': 0.3216, 'learning_rate': 0.00020599999999999997, 'epoch': 0.39}
{'loss': 0.3212, 'learning_rate': 0.000205, 'epoch': 0.4}
{'loss': 0.3196, 'learning_rate': 0.000204, 'epoch': 0.4}
{'loss': 0.3158, 'learning_rate': 0.00020299999999999997, 'epoch': 0.4}
{'loss': 0.3056, 'learning_rate': 0.00020199999999999998, 'epoch': 0.4}
{'loss': 0.2906, 'learning_rate': 0.000201, 'epoch': 0.41}
{'loss': 0.3682, 'learning_rate': 0.00019999999999999998, 'epoch': 0.41}
{'eval_loss': 0.4590398669242859, 'eval_runtime': 500.3601, 'eval_samples_per_second': 15.705, 'eval_steps_per_second': 0.492, 'epoch': 0.41}
 50%|█████████████████████████████████████████████████████                                                     | 200/400 [4:04:57<1:45:38, 31.69s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6442, 'learning_rate': 0.00019899999999999999, 'epoch': 0.41}
{'loss': 0.6307, 'learning_rate': 0.000198, 'epoch': 0.41}
{'loss': 0.586, 'learning_rate': 0.00019699999999999996, 'epoch': 0.41}
{'loss': 0.5806, 'learning_rate': 0.00019599999999999997, 'epoch': 0.42}
{'loss': 0.5429, 'learning_rate': 0.000195, 'epoch': 0.42}
{'loss': 0.5501, 'learning_rate': 0.00019399999999999997, 'epoch': 0.42}
{'loss': 0.5567, 'learning_rate': 0.00019299999999999997, 'epoch': 0.42}
{'loss': 0.5336, 'learning_rate': 0.00019199999999999998, 'epoch': 0.42}
{'loss': 0.5002, 'learning_rate': 0.000191, 'epoch': 0.43}
{'loss': 0.5192, 'learning_rate': 0.00018999999999999998, 'epoch': 0.43}
{'loss': 0.5159, 'learning_rate': 0.00018899999999999999, 'epoch': 0.43}
{'loss': 0.4982, 'learning_rate': 0.000188, 'epoch': 0.43}
{'loss': 0.4879, 'learning_rate': 0.00018699999999999996, 'epoch': 0.43}
{'loss': 0.4948, 'learning_rate': 0.000186, 'epoch': 0.44}
{'loss': 0.4699, 'learning_rate': 0.000185, 'epoch': 0.44}
{'loss': 0.4788, 'learning_rate': 0.00018399999999999997, 'epoch': 0.44}
{'loss': 0.4587, 'learning_rate': 0.00018299999999999998, 'epoch': 0.44}
{'loss': 0.453, 'learning_rate': 0.00018199999999999998, 'epoch': 0.44}
{'loss': 0.4598, 'learning_rate': 0.000181, 'epoch': 0.45}
{'loss': 0.4634, 'learning_rate': 0.00017999999999999998, 'epoch': 0.45}
{'eval_loss': 0.44239357113838196, 'eval_runtime': 487.1112, 'eval_samples_per_second': 16.132, 'eval_steps_per_second': 0.505, 'epoch': 0.45}
 55%|██████████████████████████████████████████████████████████▎                                               | 220/400 [4:26:00<1:32:43, 30.91s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4429, 'learning_rate': 0.000179, 'epoch': 0.45}
{'loss': 0.4475, 'learning_rate': 0.000178, 'epoch': 0.45}
{'loss': 0.4285, 'learning_rate': 0.00017699999999999997, 'epoch': 0.45}
{'loss': 0.4432, 'learning_rate': 0.000176, 'epoch': 0.46}
{'loss': 0.4222, 'learning_rate': 0.000175, 'epoch': 0.46}
{'loss': 0.4432, 'learning_rate': 0.00017399999999999997, 'epoch': 0.46}
{'loss': 0.4032, 'learning_rate': 0.00017299999999999998, 'epoch': 0.46}
{'loss': 0.4153, 'learning_rate': 0.000172, 'epoch': 0.46}
{'loss': 0.4146, 'learning_rate': 0.00017099999999999998, 'epoch': 0.47}
{'loss': 0.3914, 'learning_rate': 0.00016999999999999999, 'epoch': 0.47}
{'loss': 0.386, 'learning_rate': 0.000169, 'epoch': 0.47}
{'loss': 0.3845, 'learning_rate': 0.000168, 'epoch': 0.47}
{'loss': 0.3978, 'learning_rate': 0.00016699999999999997, 'epoch': 0.47}
{'loss': 0.3706, 'learning_rate': 0.000166, 'epoch': 0.48}
{'loss': 0.3654, 'learning_rate': 0.000165, 'epoch': 0.48}
{'loss': 0.376, 'learning_rate': 0.00016399999999999997, 'epoch': 0.48}
{'loss': 0.3509, 'learning_rate': 0.00016299999999999998, 'epoch': 0.48}
{'loss': 0.3528, 'learning_rate': 0.000162, 'epoch': 0.48}
{'loss': 0.3424, 'learning_rate': 0.00016099999999999998, 'epoch': 0.49}
{'loss': 0.3533, 'learning_rate': 0.00015999999999999999, 'epoch': 0.49}
{'eval_loss': 0.43734994530677795, 'eval_runtime': 512.1549, 'eval_samples_per_second': 15.343, 'eval_steps_per_second': 0.48, 'epoch': 0.49}
 60%|███████████████████████████████████████████████████████████████▌                                          | 240/400 [4:46:34<1:30:40, 34.01s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3314, 'learning_rate': 0.000159, 'epoch': 0.49}
{'loss': 0.3316, 'learning_rate': 0.00015799999999999996, 'epoch': 0.49}
{'loss': 0.3141, 'learning_rate': 0.000157, 'epoch': 0.49}
{'loss': 0.3288, 'learning_rate': 0.000156, 'epoch': 0.5}
{'loss': 0.3058, 'learning_rate': 0.000155, 'epoch': 0.5}
{'loss': 0.301, 'learning_rate': 0.00015399999999999998, 'epoch': 0.5}
{'loss': 0.3014, 'learning_rate': 0.00015299999999999998, 'epoch': 0.5}
{'loss': 0.2897, 'learning_rate': 0.000152, 'epoch': 0.5}
{'loss': 0.3014, 'learning_rate': 0.00015099999999999998, 'epoch': 0.51}
{'loss': 0.3454, 'learning_rate': 0.00015, 'epoch': 0.51}
{'loss': 0.644, 'learning_rate': 0.000149, 'epoch': 0.51}
{'loss': 0.6048, 'learning_rate': 0.000148, 'epoch': 0.51}
{'loss': 0.5966, 'learning_rate': 0.000147, 'epoch': 0.52}
{'loss': 0.5567, 'learning_rate': 0.000146, 'epoch': 0.52}
{'loss': 0.5298, 'learning_rate': 0.000145, 'epoch': 0.52}
{'loss': 0.5375, 'learning_rate': 0.00014399999999999998, 'epoch': 0.52}
{'loss': 0.5159, 'learning_rate': 0.00014299999999999998, 'epoch': 0.52}
{'loss': 0.5288, 'learning_rate': 0.00014199999999999998, 'epoch': 0.53}
{'loss': 0.5165, 'learning_rate': 0.00014099999999999998, 'epoch': 0.53}
{'loss': 0.5179, 'learning_rate': 0.00014, 'epoch': 0.53}
{'eval_loss': 0.439098060131073, 'eval_runtime': 491.2317, 'eval_samples_per_second': 15.997, 'eval_steps_per_second': 0.501, 'epoch': 0.53}
 65%|████████████████████████████████████████████████████████████████████▉                                     | 260/400 [5:06:20<1:15:52, 32.52s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4913, 'learning_rate': 0.000139, 'epoch': 0.53}
{'loss': 0.5008, 'learning_rate': 0.000138, 'epoch': 0.53}
{'loss': 0.4862, 'learning_rate': 0.000137, 'epoch': 0.54}
{'loss': 0.4648, 'learning_rate': 0.00013599999999999997, 'epoch': 0.54}
{'loss': 0.4669, 'learning_rate': 0.000135, 'epoch': 0.54}
{'loss': 0.4816, 'learning_rate': 0.00013399999999999998, 'epoch': 0.54}
{'loss': 0.4582, 'learning_rate': 0.000133, 'epoch': 0.54}
{'loss': 0.447, 'learning_rate': 0.00013199999999999998, 'epoch': 0.55}
{'loss': 0.4382, 'learning_rate': 0.00013099999999999999, 'epoch': 0.55}
{'loss': 0.4632, 'learning_rate': 0.00013, 'epoch': 0.55}
{'loss': 0.4548, 'learning_rate': 0.000129, 'epoch': 0.55}
{'loss': 0.4424, 'learning_rate': 0.000128, 'epoch': 0.55}
{'loss': 0.4328, 'learning_rate': 0.000127, 'epoch': 0.56}
{'loss': 0.4157, 'learning_rate': 0.00012599999999999997, 'epoch': 0.56}
{'loss': 0.433, 'learning_rate': 0.000125, 'epoch': 0.56}
{'loss': 0.4198, 'learning_rate': 0.00012399999999999998, 'epoch': 0.56}
{'loss': 0.4076, 'learning_rate': 0.00012299999999999998, 'epoch': 0.56}
{'loss': 0.4153, 'learning_rate': 0.000122, 'epoch': 0.57}
{'loss': 0.4026, 'learning_rate': 0.00012099999999999999, 'epoch': 0.57}
{'loss': 0.3932, 'learning_rate': 0.00011999999999999999, 'epoch': 0.57}
{'eval_loss': 0.42896339297294617, 'eval_runtime': 529.5359, 'eval_samples_per_second': 14.839, 'eval_steps_per_second': 0.465, 'epoch': 0.57}
 70%|██████████████████████████████████████████████████████████████████████████▏                               | 280/400 [5:26:11<1:01:56, 30.97s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3932, 'learning_rate': 0.00011899999999999999, 'epoch': 0.57}
{'loss': 0.4065, 'learning_rate': 0.00011799999999999998, 'epoch': 0.57}
{'loss': 0.3809, 'learning_rate': 0.000117, 'epoch': 0.58}
{'loss': 0.3743, 'learning_rate': 0.00011599999999999999, 'epoch': 0.58}
{'loss': 0.3739, 'learning_rate': 0.000115, 'epoch': 0.58}
{'loss': 0.3566, 'learning_rate': 0.00011399999999999999, 'epoch': 0.58}
{'loss': 0.3602, 'learning_rate': 0.00011299999999999998, 'epoch': 0.58}
{'loss': 0.3417, 'learning_rate': 0.000112, 'epoch': 0.59}
{'loss': 0.3496, 'learning_rate': 0.00011099999999999999, 'epoch': 0.59}
{'loss': 0.32, 'learning_rate': 0.00010999999999999998, 'epoch': 0.59}
{'loss': 0.3165, 'learning_rate': 0.00010899999999999999, 'epoch': 0.59}
{'loss': 0.3238, 'learning_rate': 0.00010799999999999998, 'epoch': 0.59}
{'loss': 0.32, 'learning_rate': 0.000107, 'epoch': 0.6}
{'loss': 0.3176, 'learning_rate': 0.00010599999999999999, 'epoch': 0.6}
{'loss': 0.3023, 'learning_rate': 0.00010499999999999999, 'epoch': 0.6}
{'loss': 0.3001, 'learning_rate': 0.000104, 'epoch': 0.6}
{'loss': 0.3026, 'learning_rate': 0.00010299999999999998, 'epoch': 0.6}
{'loss': 0.297, 'learning_rate': 0.000102, 'epoch': 0.61}
{'loss': 0.2894, 'learning_rate': 0.00010099999999999999, 'epoch': 0.61}
{'loss': 0.3341, 'learning_rate': 9.999999999999999e-05, 'epoch': 0.61}
{'eval_loss': 0.4372474253177643, 'eval_runtime': 490.0569, 'eval_samples_per_second': 16.035, 'eval_steps_per_second': 0.502, 'epoch': 0.61}
 75%|█████████████████████████████████████████████████████████████████████████████████                           | 300/400 [5:45:07<52:14, 31.34s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6266, 'learning_rate': 9.9e-05, 'epoch': 0.61}
{'loss': 0.5931, 'learning_rate': 9.799999999999998e-05, 'epoch': 0.61}
{'loss': 0.5915, 'learning_rate': 9.699999999999999e-05, 'epoch': 0.62}
{'loss': 0.5239, 'learning_rate': 9.599999999999999e-05, 'epoch': 0.62}
{'loss': 0.5424, 'learning_rate': 9.499999999999999e-05, 'epoch': 0.62}
{'loss': 0.5297, 'learning_rate': 9.4e-05, 'epoch': 0.62}
{'loss': 0.5303, 'learning_rate': 9.3e-05, 'epoch': 0.62}
{'loss': 0.4958, 'learning_rate': 9.199999999999999e-05, 'epoch': 0.63}
{'loss': 0.5025, 'learning_rate': 9.099999999999999e-05, 'epoch': 0.63}
{'loss': 0.4996, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.63}
{'loss': 0.4849, 'learning_rate': 8.9e-05, 'epoch': 0.63}
{'loss': 0.4741, 'learning_rate': 8.8e-05, 'epoch': 0.64}
{'loss': 0.4722, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.64}
{'loss': 0.4683, 'learning_rate': 8.6e-05, 'epoch': 0.64}
{'loss': 0.4759, 'learning_rate': 8.499999999999999e-05, 'epoch': 0.64}
{'loss': 0.4573, 'learning_rate': 8.4e-05, 'epoch': 0.64}
{'loss': 0.4595, 'learning_rate': 8.3e-05, 'epoch': 0.65}
{'loss': 0.455, 'learning_rate': 8.199999999999999e-05, 'epoch': 0.65}
{'loss': 0.4595, 'learning_rate': 8.1e-05, 'epoch': 0.65}
{'loss': 0.4373, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.65}
{'eval_loss': 0.4280502200126648, 'eval_runtime': 475.3158, 'eval_samples_per_second': 16.532, 'eval_steps_per_second': 0.518, 'epoch': 0.65}
 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 320/400 [6:05:55<42:43, 32.04s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4318, 'learning_rate': 7.899999999999998e-05, 'epoch': 0.65}
{'loss': 0.4484, 'learning_rate': 7.8e-05, 'epoch': 0.66}
{'loss': 0.4275, 'learning_rate': 7.699999999999999e-05, 'epoch': 0.66}
{'loss': 0.4249, 'learning_rate': 7.6e-05, 'epoch': 0.66}
{'loss': 0.4084, 'learning_rate': 7.5e-05, 'epoch': 0.66}
{'loss': 0.4072, 'learning_rate': 7.4e-05, 'epoch': 0.66}
{'loss': 0.4005, 'learning_rate': 7.3e-05, 'epoch': 0.67}
{'loss': 0.4081, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.67}
{'loss': 0.3979, 'learning_rate': 7.099999999999999e-05, 'epoch': 0.67}
{'loss': 0.3756, 'learning_rate': 7e-05, 'epoch': 0.67}
{'loss': 0.3892, 'learning_rate': 6.9e-05, 'epoch': 0.67}
{'loss': 0.3956, 'learning_rate': 6.799999999999999e-05, 'epoch': 0.68}
{'loss': 0.385, 'learning_rate': 6.699999999999999e-05, 'epoch': 0.68}
{'loss': 0.3596, 'learning_rate': 6.599999999999999e-05, 'epoch': 0.68}
{'loss': 0.3584, 'learning_rate': 6.5e-05, 'epoch': 0.68}
{'loss': 0.3431, 'learning_rate': 6.4e-05, 'epoch': 0.68}
{'loss': 0.3453, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.69}
{'loss': 0.3387, 'learning_rate': 6.199999999999999e-05, 'epoch': 0.69}
{'loss': 0.3387, 'learning_rate': 6.1e-05, 'epoch': 0.69}
{'loss': 0.3234, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.69}
{'eval_loss': 0.4236106276512146, 'eval_runtime': 479.8915, 'eval_samples_per_second': 16.375, 'eval_steps_per_second': 0.513, 'epoch': 0.69}
 85%|███████████████████████████████████████████████████████████████████████████████████████████▊                | 340/400 [6:25:44<37:06, 37.10s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3299, 'learning_rate': 5.899999999999999e-05, 'epoch': 0.69}
{'loss': 0.316, 'learning_rate': 5.7999999999999994e-05, 'epoch': 0.7}
{'loss': 0.3186, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.7}
{'loss': 0.3133, 'learning_rate': 5.6e-05, 'epoch': 0.7}
{'loss': 0.3116, 'learning_rate': 5.499999999999999e-05, 'epoch': 0.7}
{'loss': 0.2948, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.7}
{'loss': 0.2854, 'learning_rate': 5.2999999999999994e-05, 'epoch': 0.71}
{'loss': 0.2895, 'learning_rate': 5.2e-05, 'epoch': 0.71}
{'loss': 0.2828, 'learning_rate': 5.1e-05, 'epoch': 0.71}
{'loss': 0.3323, 'learning_rate': 4.9999999999999996e-05, 'epoch': 0.71}
{'loss': 0.5944, 'learning_rate': 4.899999999999999e-05, 'epoch': 0.71}
{'loss': 0.5903, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.72}
{'loss': 0.5457, 'learning_rate': 4.7e-05, 'epoch': 0.72}
{'loss': 0.5304, 'learning_rate': 4.599999999999999e-05, 'epoch': 0.72}
{'loss': 0.5271, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.72}
{'loss': 0.5114, 'learning_rate': 4.4e-05, 'epoch': 0.72}
{'loss': 0.5089, 'learning_rate': 4.3e-05, 'epoch': 0.73}
{'loss': 0.4998, 'learning_rate': 4.2e-05, 'epoch': 0.73}
{'loss': 0.4973, 'learning_rate': 4.0999999999999994e-05, 'epoch': 0.73}
{'loss': 0.4769, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.73}
{'eval_loss': 0.4235776960849762, 'eval_runtime': 470.7799, 'eval_samples_per_second': 16.691, 'eval_steps_per_second': 0.523, 'epoch': 0.73}
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▏          | 360/400 [6:45:52<25:37, 38.44s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4629, 'learning_rate': 3.9e-05, 'epoch': 0.73}
{'loss': 0.4772, 'learning_rate': 3.8e-05, 'epoch': 0.74}
{'loss': 0.4576, 'learning_rate': 3.7e-05, 'epoch': 0.74}
{'loss': 0.4735, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.74}
{'loss': 0.4671, 'learning_rate': 3.5e-05, 'epoch': 0.74}
{'loss': 0.4673, 'learning_rate': 3.399999999999999e-05, 'epoch': 0.75}
{'loss': 0.4629, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.75}
{'loss': 0.443, 'learning_rate': 3.2e-05, 'epoch': 0.75}
{'loss': 0.4422, 'learning_rate': 3.0999999999999995e-05, 'epoch': 0.75}
{'loss': 0.4174, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.75}
{'loss': 0.4382, 'learning_rate': 2.8999999999999997e-05, 'epoch': 0.76}
{'loss': 0.4445, 'learning_rate': 2.8e-05, 'epoch': 0.76}
{'loss': 0.4361, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.76}
{'loss': 0.4132, 'learning_rate': 2.6e-05, 'epoch': 0.76}
{'loss': 0.4111, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.76}
{'loss': 0.413, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.77}
{'loss': 0.3903, 'learning_rate': 2.2999999999999997e-05, 'epoch': 0.77}
{'loss': 0.3988, 'learning_rate': 2.2e-05, 'epoch': 0.77}
{'loss': 0.3905, 'learning_rate': 2.1e-05, 'epoch': 0.77}
{'loss': 0.3818, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.77}
{'eval_loss': 0.42043009400367737, 'eval_runtime': 465.5582, 'eval_samples_per_second': 16.879, 'eval_steps_per_second': 0.528, 'epoch': 0.77}
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 380/400 [7:04:28<10:27, 31.40s/it/
home/aftab/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be remo
ved in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.fl
oat32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3645, 'learning_rate': 1.9e-05, 'epoch': 0.78}
{'loss': 0.3715, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.78}
{'loss': 0.3627, 'learning_rate': 1.6999999999999996e-05, 'epoch': 0.78}
{'loss': 0.3495, 'learning_rate': 1.6e-05, 'epoch': 0.78}
{'loss': 0.3463, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.78}
{'loss': 0.341, 'learning_rate': 1.4e-05, 'epoch': 0.79}
{'loss': 0.3597, 'learning_rate': 1.3e-05, 'epoch': 0.79}
{'loss': 0.3313, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.79}
{'loss': 0.3457, 'learning_rate': 1.1e-05, 'epoch': 0.79}
{'loss': 0.3352, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.79}
{'loss': 0.3229, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3243, 'learning_rate': 8e-06, 'epoch': 0.8}
{'loss': 0.3227, 'learning_rate': 7e-06, 'epoch': 0.8}
{'loss': 0.3275, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.8}
{'loss': 0.3092, 'learning_rate': 4.9999999999999996e-06, 'epoch': 0.8}
{'loss': 0.3135, 'learning_rate': 4e-06, 'epoch': 0.81}
{'loss': 0.2992, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.81}
{'loss': 0.2923, 'learning_rate': 2e-06, 'epoch': 0.81}
{'loss': 0.2965, 'learning_rate': 1e-06, 'epoch': 0.81}
{'loss': 0.3566, 'learning_rate': 0.0, 'epoch': 0.81}
{'eval_loss': 0.4195553958415985, 'eval_runtime': 400.917, 'eval_samples_per_second': 19.6, 'eval_steps_per_second': 0.614, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [7:20:06<00:00, 25.12s/it/
home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetry
Error('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Cau
sed by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f42dc3dc8b0>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporary
 failure in name resolution)"))'), '(Request ID: 3af3bf8a-d1b1-4653-bdf7-a0cafd05404f)') - silently ignoring the lookup for the file config.json in m
eta-llama/Llama-2-7b-hf.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-h
f - will assume that the vocabulary was not modified.
  warnings.warn(
/home/aftab/.local/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetr
yError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /meta-llama/Llama-2-7b-hf/resolve/main/config.json (Ca
used by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x7f41b84b7250>: Failed to resolve \'huggingface.co\' ([Errno -3] Temporar
y failure in name resolution)"))'), '(Request ID: e397f309-44fa-4170-af4c-fb1eff6da24c)') - silently ignoring the lookup for the file config.json in
meta-llama/Llama-2-7b-hf.
  warnings.warn(
{'train_runtime': 26416.3601, 'train_samples_per_second': 1.938, 'train_steps_per_second': 0.015, 'train_loss': 0.5738081806898117, 'epoch': 0.81}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [7:20:16<00:00, 66.04s/it]

real    440m46.264s
user    438m34.331s
sys     0m42.650s
aftab@ubuntu:~/workspace/Llama-experiments/src$
{'tmux_session_id': 49}
